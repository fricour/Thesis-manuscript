# General discussion and perspectives

## UVP6 classification model: From start to finish

The development of the UVP6 classification model started without a data set of labeled UVP6 images. Therefore, a significant part of my work was to test and prepare a fully functional data preparation and model training pipeline for the upcoming UVP6 data set using UVP5 images. In the following sections, I will describe a brief history of what has been done prior to the acquisition and human labeling of UVP6 images and discuss about the use and perspectives on the embedded classification.

### Trust index

<!-- https://datascience.stackexchange.com/questions/61761/how-does-xgboost-use-softmax-as-an-objective-function -->

In multiclass classification task, the goal is to predict the label (i.e. class) of a given input among a defined number of labels. In @sec-uvp6, I explained that XGBoost minimizes a regularized objective function $L$ (see @eq-objective-function-xgboost) during model training. In practice, the XGBoost library proposes two choices of objective functions for multiclass classification problems: `multi:softmax` and `multi:softprob` (https://xgboost.readthedocs.io/en/latest/parameter.html, last accessed: 03/03/2023). Both functions are based on the softmax normalization which normalizes the classification score of each class using an exponential function (@eq-softmax). `multi:softmax` returns the class of highest probability whereas `multi:softprob` returns a class probability array, which allows the computation of custom objective functions.

$$
\sigma(x_{i}) = \frac{e^{x_{i}}}{\sum_{j=1}^{N}e^{x_{j}}}, i = 1,...,N
$$ {#eq-softmax}

where $x_{i}$ and $N$ are, respectively, the classification score of the $i^{th}$ class and the number of classes.

::: {.blackbox data-latex=""}
::: {.flushleft data-latex=""}
**Information Box 4.** **Cross entropy loss**
:::

Cross entropy loss is a loss function often used in multiclass classification problems [@Geron2019-is].

$$
J({\Theta}) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{(i)}log(\hat{p}_{k}^{(i)})
$$

where $J$ is the loss (i.e. cost) function, $\Theta$ a parameter matrix, $m$ the number of training data (i.e. instances), $K$ the number of classes, $y_{k}^{(i)}$ the true probability that the $i^{th}$ instance belongs to class $k$ (0 or 1) and $\hat{p}_{k}^{(i)}$ the predicted probability that the instance belongs to class $k$. In practice, if one sets the objective function to `multi:softmax`, it means that the objective of the model is to predict the highest probability for the true class while minimizing the cross entropy loss.
:::

By definition, `multi:softmax` amplifies the difference between classification scores. Here is a simple illustration of this effect. Let A, B and C, three classes of zooplankton whose classification scores are $(1,7,8)$. Using @eq-softmax, the output probabilities are $(0.0006, 0.26, 0.73)$. Therefore, a unit difference in the classification scores of class B and class C results in a probability three times higher for class C than class B.

This led us to define what we called a trust index, that is a metric whose score indicates the degree of confidence in the classification score. In other words, we looked for the best error estimator (@fig-trust-index). To this end, we trained a first classification model with classes from the UVP5 data set using a `multi:softprob` objective function. Then, we used the model to make a prediction on a test set that returned a matrix $(M \times N)$ of $M$ images and $N$ probabilities ($N$ classes in the model). Afterwards, we computed four metrics on each row of the probability matrix: the maximum probability ($max$, equivalent to `multi:softmax`), the softmax of the probability array ($softmax$, equivalent to a double `multi:softmax`), the difference between the highest and the second highest probability ($diff_{max}$) and the difference between the highest and second highest probability of the softmax array ($diff_{softmax}$). We compared those metrics for each class of the test set in @fig-trust-index which clearly shows that all metrics were very similar and that they captured the errors at the same speed. This result is threefold: 1) in our case, there is no need to use a trust index in addition to the probabilities given by `multi:softprob`, 2) some classes kept accumulating errors in the prediction with increasing rank (e.g. *Copepoda*, darksphere and fiber\<detritus) thus an increasing rank is not always a trustworthy indicator of increasing confidence, 3) it partly explains why the quantification did not improve the overall classification by either using a fixed probability threshold (i.e. TC) or using the probability matrix (i.e. SPAC).

```{r, trust index, dpi = 300, fig.height = 12, fig.width= 10}
#| label: fig-trust-index
#| fig-cap: Percentage of error captured as a function of rank for four metrics (max, softmax, diff$_{max}$ and diff$_{softmax}$) on six UVP5 classes (4 biological, 2 non-biological). The number of objects predicted in each class is provided in the title of each subplot.

library(gridExtra)
library(grid)
library(jpeg)

path <- paste0(getwd(), '/image/uvp6/trust_index')

grid.arrange(
  rasterGrob(as.raster(readJPEG(paste0(path, "/Aulacanthidae.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/Aulosphaeridae.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/Copepoda.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/darksphere.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/detritus.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/fiber_detritus.jpeg")))),
  ncol = 2
)
```

### Technical constraints on the classification model

Because the UVP6-LP is designed to work on AUVs, several constraints were imposed on the model to fulfill technical and energy requirements. The UVP6 data treatment at sea shown in @fig-uvp-pipeline briefly describes the steps from the image acquisition to the prediction. The two most energy consuming steps are the image segmentation (median execution time of 650 ms) and the loading of the classification model whose execution time was capped at 700 ms. This limitation imposed a direct constraint on the prediction quality of the model because it implied a limitation on the choice of the model hyperparameters (learning rate, tree depth) and the number of boosting rounds.

The second critical aspect of the model was its limitation at 40 classes. In the UVP5 SD data set, the most detailed (human) classification at a taxonomic level was roughly 150 classes (referred to as $level_{0}$). It was therefore necessary to merge classes based on morphological and ecological attributes to decrease that number in order to get to $level_{1}$ that had 38 classes (reminder, it is currently limited at 40). From $level_{1}$, we thus experimented a lot of different prediction models with a hyperparameters grid search in order to find both the right set of hyperparameters and the right number of classes. Still, some $level_{1}$ classes had very low precision and recall scores because they were confused with some specific taxonomic groups hence we merged them accordingly with those groups. This trial-error approach was necessary to create the final aggregation level ($level_{2}$) used for the final UVP5 model. The same work had to be done later with the UVP6 data because the taxonomic groups were not identical.

The last technical limitation was the number of features (see @tbl-features). Before I started my PhD, 55 features (basically geometry, gray levels and statistical moments) were defined and were compliant with the energy budget imposed by the float (the UVP6 is adapted for other AUVs but it was especially designed for profiling floats). When XGBoost results are compared with a state-of-the-art CNN, the main difference does not likely come from the classifiers themselves but most likely from the selected features. We could have done some feature engineering, that is either doing feature selection (selecting the most useful features) either doing feature combination (combining existing features to create new ones) or creating new features by collecting new data [@Geron2019-is]. Therefore, texture features could have been added to our global 55 features. The main issue with the latter (e.g. histogram of oriented gradients for detecting edges, gray-level co-occurrence matrix for homogeneity and contrast) is that their processing time is proportional to the size of the image hence it was difficult to evaluate them in the energy budget.

### Detritus, this ubiquitous mess

@tbl-stats-uvp6 shows that the median number of objects detected in 3.5 m$^{3}$ is roughly 300. In addition, it is usual to have less than 20% of living objects hence the vast majority of images are labeled as detritus. The latter are ubiquitous in the ocean and display a wide variety of shapes, sizes and textures (@fig-detritus). In @sec-improve-classif, I showed that using several classes of detritus did not improve the classification model and that it actually made it worse because the different detritus classes showed a high level of confusion between each other.

```{r, mean and median abundances in a typical vertical profile}
#| label: tbl-stats-uvp6
#| tbl-cap: Mean and median concentrations and number of segmented objects whose size is above 0.6 mm (referred to as "vignettes" in french) expected from a standard vertical profile taken by a BGC-Argo float equipped with a UVP6 in different parts of the water column.

library(gt)
library(gtsummary)
library(gtExtras)
library(tibble)
library(tidyverse)
library(latex2exp)

stats_uvp6 <- tibble(mean_conc = c(119, 457, 110, 76)/1000, 
                     median_conc = c(85, 332, 59, 38)/1000, 
                     mean_nb =  c(426,146,143,137),
                     median_nb = c(305,106,77,75),
                     layer = c('0-2000', '0-100', '100-500', '500-2000'),
                     water_volume = c(3.57, 0.32, 1.3, 1.95)*1000)

stats_uvp6 |> gt() |> cols_label(mean_conc = 'Mean',
                                 median_conc = 'Median',
                                 mean_nb = 'Mean',
                                 median_nb = 'Median',
                                 layer = 'Depth range (m)',
                                 water_volume = 'Sampled volume (L)') |>
  tab_spanner(label = 'Concentration (#/L)',
              columns = c(mean_conc, median_conc)) |>
   tab_spanner(label = 'Number',
              columns = c(mean_nb, median_nb))
```

![Examples of UVP objects labeled as detritus. The scale bar is the same for all (2 mm).](image/detritus/moz_detritus.jpg){#fig-detritus}

![Examples of UVP objects labeled as living material. The scale bar is the same for all (2 mm). Note that I have hidden three detritus from @fig-detritus to highlight the difficulty of classifying plankton already at the human level.](image/detritus/uvp_user_meeting/living_except3.jpg){#fig-living}

We assumed that detritus form a continuum in the 55-dimensional feature space. Therefore, we tried several methods to break this continuum while also trying to cope with the data set class imbalance. For this, we tried under-sampling the detritus to decrease their number and non random under-sampling by keeping only detritus close to K-means clustered centroids but it did not improve the classification model. Using a clustering method to split the detritus into several classes (e.g. fluffy gray or dark compact) to improve the classification of biological objects was based on the strong belief that it would improve the classification. However, my work and the work of another PhD student that tried using several detritus classes using a CNN (not published yet) showed that it did not improve the classification model either. Nonetheless, it is true that other clustering methods could be further explored. Clustering algorithms such as DBSCAN [@Schubert2017-xr] and HDBSCAN [@Campello2013-cb] are density-based in contrast to K-Means with is a partitioning clustering algorithm that needs a predefined number of clusters in input. Other clustering methods could obviously be explored [see @Geron2019-is, pp. 259-260] based on their computational complexity and the data set characteristics (number of instances and features).

```{=latex}
\newpage
```
### The future of the embedded classification

#### UVPec

[UVPec](https://github.com/ecotaxa/uvpec/) (https://github.com/ecotaxa/uvpec/) - short name for Underwater Vision Profiler Embedded Classifier - is a toolbox to train classification models for UVP6 images and evaluate their performance. This python package has been designed to be as user-friendly as possible in order to be accessible to the largest number of users. Basically, the user only needs to create folders (40 maximum because the maximum number of classes is fixed at 40) of labeled UVP6 images and specify some hyperparameters (e.g. learning rate) to train a classification model. The evaluation of the created model provides a confusion matrix and a classification report to help the user evaluate the quality of the model as well as the cross entropy curve to have information on the optimal boosting round number (before the model overfits).

Overall, the goal of this package is to propose a simple and fast way to create new classification models for UVP6 applications on platforms that need to send data in near-real time or that cannot be easily retrieved.

#### More regions to be explored

The Labrador Sea was the first of 9 key oceanic regions studied in the frame of the REFINE (Robots Explore plankton-driven Fluxes in the marine twIlight zoNE) project (see https://erc-refine.eu). The latter aims to study regions with distinct biogeochemical conditions and responses to climate (@fig-refine).

```{r, map refine}
#| label: fig-refine
#| fig-cap: Oceanic regions (Baffin Bay (BB), Labrador Sea (LS), Greenland Sea (GS), Guinea Dome (GD), Arabian Sea (AS), Tropical Indian Ocean (TIO), South Pacific subtropical Gyre (SPG), West Kerguelen Plateau (WKP), East Kerguelen Plateau (EKP)) investigated in the frame of REFINE.

library(tmap)
library(tibble)
data("World")
library(sf)
library(ggspatial)
library(rnaturalearth)
library(rnaturalearthdata)
library(ggplot2)
library(RColorBrewer)

regions <- tibble(Region = c('BB', 'GS', 'LS', 'GD', 'AS', 'TIO', 'SPG', 'WKP', 'EKP'),
                  long_name = c('Baffin Bay', 'Greenland Sea', 'Labrador Sea', 'Guinea Dome',
                                'Arabian Sea', 'Tropical Indian Ocean', 'South Pacific subtropical gyre','West Kerguelen Plateau', 'East Kerguelen Plateau'),
                  lat = c(74, 77, 62, 9, 19, -7.7, -32, -49.6, -50),
                  lon = c(-67.4, -3, -55, -21, 64, 54, -130, 63.6, 75.6))


points_sf <- st_as_sf(regions, coords = c("lon", "lat"), crs = "+proj=longlat")

# # map
tm_shape(World) +
  tm_polygons() +
  tm_shape(points_sf) +
  tm_bubbles(size = .3, col = 'Region', alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.position = c("left", "center"), legend.frame = T)

# # load world data
# world <- ne_countries(scale = "medium", returnclass = "sf")
# 
# ggplot(data = world) +
#     geom_sf() +
#     xlab("Longitude") + ylab("Latitude") + theme_bw() +
#     geom_point(data = regions, aes(x = lon, y = lat, colour = Region), size = 4) +
#     scale_colour_manual(values = brewer.pal(9, "Set3")) +
#   geom_point(data = regions, aes(x = lon, y = lat), colour = 'black', size = 4, shape = 1)

```

@fig-kerguelen present some preliminary results from the UVP6 embedded classification that highlights the difference in *Calanoida*, *Chaetognatha*, detritus, puff and tuff concentrations between February and March 2023 in the western and eastern part of the Kerguelen Plateau. For these floats, the configuration was different from the ones deployed in the Labrador Sea with a higher profiling time resolution after deployment.

```{r, kerguelen, fig.width=8, fig.height=10}
#| label: fig-kerguelen
#| fig-cap: $\textit{Calanoida}$, $\textit{Chaetognatha}$, detritus, puff and tuff (i.e. $\textit{Trichodesmium}$) concentrations in February-March 2023 on the Kerguelen Plateau.

library(vroom)
library(tidyverse)
library(patchwork)
library(lubridate)
library(viridis)

# load data
calanoids_east <- vroom('DATA/ARGO/1902593_class_number_4_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Calanoida - East') 

calanoids_west <- vroom('DATA/ARGO/4903657_class_number_4_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Calanoida - West') 

chaeto_east <- vroom('DATA/ARGO/1902593_class_number_5_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Chaetognatha - East')

chaeto_west <- vroom('DATA/ARGO/4903657_class_number_5_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Chaetognatha - West')

detritus_east <- vroom('DATA/ARGO/1902593_class_number_13_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'detritus - East') 

detritus_west <- vroom('DATA/ARGO/4903657_class_number_13_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'detritus - West') 

puff_east <- vroom('DATA/ARGO/1902593_class_number_16_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'puff - East') 

puff_west <- vroom('DATA/ARGO/4903657_class_number_16_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'puff - West') 

tuff_east <- vroom('DATA/ARGO/1902593_class_number_19_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'tuff - East') 

tuff_west <- vroom('DATA/ARGO/4903657_class_number_19_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'tuff - West') 

# tuff_west |> filter(cycle == 9) |> ggplot(aes(y = -depth, x = conc_object))


taxo_kerguelen <- rbind(calanoids_east, calanoids_west, chaeto_east, chaeto_west,
                        detritus_east, detritus_west,
                         puff_east, puff_west, tuff_east, tuff_west)

ggplot(taxo_kerguelen) + geom_point(aes(x = conc_object, y = depth, colour = date)) + scale_colour_viridis_c(trans = "date",labels = c("2023-02-11","2023-02-18","2023-02-25","2023-03-04","2023-03-11", "2023-03-18"), breaks = c(as.Date("2023-02-11"), as.Date("2023-02-18"), as.Date("2023-02-25"), as.Date("2023-03-04"), as.Date("2023-03-11"), as.Date("2023-03-18"))) + scale_y_reverse() + theme_bw() +
  labs(x = 'Concentration (#/L)', colour = "Date", y = 'Depth (m)') +
  facet_wrap(~group, scale = "free_x", nrow = 5)
```

```{=latex}
\newpage
```
The observed concentrations of the 4 biological classes in @fig-kerguelen needs to be interpreted with the UVP6 classification report (@tbl-classification-report-uvp6-xgboost-cnn). *Calanoida* have a precision and recall of, respectively, 66% and 69% thus we potentially miss 31% of them (i.e. 31% of *Calanoida* are non detected because they are classified in another class) but 34% of images classified as *Calanoida* are false positives and should be placed in another class. In that case, the observed concentrations can therefore be assumed as the real concentration. *Chaetognatha* have a precision and recall of, respectively, 80% and 62% therefore when a *Chaetognatha* is detected, the model is confident (20% of false positives) but it misses 38% of them so we might have more *Chaetognatha* than measured by the UVP6. However, One must also look at the concentration values because it is likely that the 2 points measured on the eastern part of the Kerguelen Plateau are misclassified objects. We could therefore conclude to the absence of *Chaetognatha* on the eastern part while their presence is likely on the western part because, on the one hand, this class has a high precision and, on the other hand, they are observed on successive short-lapsed vertical profiles and between 0-500 m where they likely feed on *Calanoida*.

Puff and tuff are both *Trichodesmium* which are usually observed in tropical and subtropical regions [@Westberry2006-bt] hence their presence on the Kerguelen Plateau is dubious. Tuff have a low precision and recall (55% and 63%) in contrast to puff (73% and 78%). On the eastern part, both classes have low concentrations and their position in the water column should be questioned for photosynthetic organisms. On the western part, they also both have low concentrations (0.25 #/L for puff is likely false) even though detritus concentrations are 5-10 times higher than on the eastern part.

The first results of the embedded classification are promising. However, they need to be interpreted with the UVP6 classification report and contextualized (e.g. species geographical and temporal distribution, correlation with the presence of other species, position in the water column).

#### New algorithms

Unless we find a cheap and efficient (low power) way to send UVP6 images via Iridium communication, we are stuck with the issue of embedded classification. In @sec-uvp6, I showed that CNNs usually outperform features-based classifiers. However, the field of computer vision is constantly evolving with the arrival of new technologies that could help improve the automatic classification of zooplankton images. Transformers [@Vaswani2017-il], also referred to as Foundation Models in @Bommasani2021-kb are the latest cutting-edge tools in the field of computer vision and natural language processing [e.g. @Liu2021-jq; @Li2022-qt]. The structure of transformers relies on attention mechanisms able to draw global dependencies between input and output [@Vaswani2017-il]. In other words, it is able to learn dependencies between distant positions in a network architecture in constrast to CNNs. As an example, @Li2022-qt used a vision transformer model combining a CNN and a vision transformer to identify apple diseases. In their model, the CNN is used to extract the global features of the input image while the transformer extracts local features of the disease region to help the CNN see better. How does it help the CNN? In the case of apple images with different backgrounds (different weather conditions, brightness changes, other objects in the field of view), the CNN could be "distracted" hence the transformer is able to focus the attention of the CNN on the region of interest (the disease region) and not on irrelevant features such as the weather. The use of transformers for automatic zooplankton classification has just begun [@Kyathanahally2022-ma] but it should certainly be explored further in the near future.

#### UVP6m

There is a class called "other\<living" in the UVP6 classification model. I did not mention earlier that we were forced to place copepods (except *Calanoida*) in that class. Indeed, *Calanoida* are a "large" species of copepods compared to other copepods found in the Mediterranean Sea that are smaller. The classification score of the latter was low (low precision and low recall) hence we put them in the "other\<living" class, which is a shame.

The good news is that the next generation of UVPs is already in the making with the UVP6m (m for micro) which will be able to look at smaller objects (particle counter between 10 and 200 $\mu$m and plankton/particle imaging \> 100 $\mu$m). The UVP6m will be more compact than the UVP6 but the principles will be the same and both instruments will share 5 size classes for intercalibration.

```{=latex}
\newpage
```
## Deriving carbon fluxes from particle size distribution

Before working on the CONVERSE framework, I was trying to find a global relationship (I also tried to find a regional one, without more success) to derive carbon fluxes from UVP5 PSDs. The idea was to reproduce the method of @Guidi2008-vp on the newly quality controlled UVP5 PSD data set of @Kiko2022-sk. For this, I needed to build a global data set of POC measurements derived from sediment traps and $^{234}Th$ (see @tbl-poc-from-sediment-traps, @tbl-poc-from-thorium and [here](https://fricour.shinyapps.io/carbon_fluxes_app/) for a more interactive view). Here below, I briefly review three methods we explored to find a POC-PSD relationship and what could be further investigated.

```{=latex}
%\includepdf[pages=1]{"/home/flo/dox/THESIS/bg-18-755-2021"}
```
```{=html}
<!--#https://github.com/quarto-dev/quarto-cli/discussions/1042-->
<style type="text/css">
caption, .table-caption {
  text-align: left;
}
</style>
```
```{r, poc database from traps}
#| label: tbl-poc-from-sediment-traps
#| tbl-cap: Compilation of POC flux measurements from sediment traps. Data taken from public databases listed on PANGAEA or extracted from articles (with the help of Frédéric Le Moigne and Meike Vogt). The number of measurements could be different from the original publication after data cleaning and potential removal of duplicates.
#| tbl-cap-location: bottom

library(dplyr)
library(knitr)
library(kableExtra)
library(purrr)

# read data
trap <- vroom::vroom('DATA/Trap_Thorium/TRAP_DATABASE.csv')
#thorium <- vroom::vroom('DATA/Trap_Thorium/THORIUM_DATABASE.csv')

# clean a bit
trap[trap$doi == 'https://doi.org/10.1007/s00227-018-3376-1',]$doi <- '10.1007/s00227-018-3376-1'
trap[trap$doi == 'https://doi.org/10.1016/j.dsr.2016.08.015',]$doi <- '10.1016/j.dsr.2016.08.015'
trap[trap$doi == 'Geochimica et Cosmochimica Acta, Vol. 66, No. 3, pp. 457–465, 2002',]$doi <- '10.1016/S0016-7037(01)00787-6'

# some stats
trap_resume <- trap |> group_by(doi) |> dplyr::summarise(data_points = n(), max_date = lubridate::year(max(start_date)), min_date = lubridate::year(min(start_date))) |> arrange(desc(data_points)) |>
  mutate(period_covered = if_else(max_date > min_date, paste0(min_date,'-',max_date), paste0(max_date)))

# BASED ON : https://stackoverflow.com/questions/48506712/kable-kableextra-cells-with-hyperlinks
# make url links
make_link <- function(doi){
  link <- paste0('https://doi.org/',doi)
  return(link)
}
url <- map_chr(trap_resume$doi, make_link)

# # put url in dataframe
# trap_resume <- trap_resume |> mutate(doi_url = cell_spec(doi, 'html', link = url)) |> select(doi_url, data_points) |>
#   kable('html', escape = FALSE, col.names = c('Sources of data', 'Number of measurements'), align = 'l') #|> kable_styling(bootstrap_options = c("hover", "condensed"))

trap_resume <- trap_resume |> mutate(doi_url = cell_spec(doi, link = url)) |> select(doi_url, period_covered, data_points) |>
  kable(escape = FALSE, col.names = c('Source of data', 'Period covered', 'Number of observations'), align = 'l') 

# make final table
trap_resume
```

```{r, poc database from thorium}
#| label: tbl-poc-from-thorium
#| tbl-cap: Compilation of POC flux measurements derived from Thorium 234. Data taken from public databases listed on PANGAEA or extracted from articles (with the help of Frédéric Le Moigne). The number of measurements could be different from the original publication after data cleaning and potential removal of duplicates.
#| tbl-cap-location: bottom

# read data
thorium <- vroom::vroom('DATA/Trap_Thorium/THORIUM_DATABASE.csv')

# some stats
thorium_resume <- thorium |> group_by(doi) |> summarise(data_points = n(), max_date = lubridate::year(max(start_date)), min_date = lubridate::year(min(start_date))) |> arrange(desc(data_points)) |>
  mutate(period_covered = if_else(max_date > min_date, paste0(min_date,'-',max_date), paste0(max_date)))
thorium_resume$doi <- c('10.1594/PANGAEA.809717', '10.1029/2018GB00615', '10.1525/elementa.2020.030')

# make url links
url <- c('https://doi.org/10.1594/PANGAEA.809717', 'https://doi.org/10.1029/2018GB006158', 'https://doi.org/10.1525/elementa.2020.030')

# put url in dataframe
thorium_resume <- thorium_resume |> mutate(doi_url = cell_spec(doi, link = url)) |> select(doi_url, period_covered, data_points) |>
  kable(escape = FALSE, col.names = c('Source of data', 'Period covered', 'Number of observations'), align = 'l') #|> kable_styling(bootstrap_options = c("hover", "condensed"))

# make final table
thorium_resume
```

```{=latex}
\newpage
```
### The straightforward approach

This approach consisted in simply applying the method of @Guidi2008-vp (see @eq-carbon-flux-finite). Therefore, we looked for match-ups (i.e. spatially and temporally close observations) between UVP5 and POC observations within a 0.2° latitude and longitude window, a 10 m vertical depth range and a 5-day window. @Guidi2008-vp found 118 match-ups (with UVP2/3/4 data) whereas we only found 108 match-ups with UVP5 data, located on the western and eastern parts of the Kerguelen Plateau, in the Guinea Dome and in the California Current (panel D in @fig-uvp-trap-mapping). Consequently, we decided not to go further because the number of match-ups was too low to improve the minimization (see @eq-cost-function).

```{r, uvp traps th234 map}
#| label: fig-uvp-trap-mapping
#| fig-cap: Spatial distribution of (A) sediment traps, (B) $^{234}Th$, (C) UVP5 and (D) their spatio-temporal match-ups.

d <- vroom::vroom('DATA/CFLUX/uvp_trap.csv')
guidi_matchups <- vroom::vroom('DATA/CFLUX/guidi_matchups.csv')

d_sf <- st_as_sf(d, coords = c("lon", "lat"), crs = "+proj=longlat")

d_uvp <- d_sf |> filter(source == 'UVP5')
d_trap <- d_sf |> filter(source == 'Trap')
d_th <- d_sf |> filter(source == 'Thorium')
d_matchups <- st_as_sf(guidi_matchups, coords = c("lon", "lat"), crs = "+proj=longlat")

# map
# tm_shape(World) +
#   tm_polygons() +
#   tm_shape(d_sf) +
#   tm_dots(size = .2, col = "black", alpha = 1, border.col = 'black', border.alpha = 1) +
#   tm_layout(legend.show = F) +
#   tm_facets(by = 'source')

# up left
ul <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_trap) +
  tm_bubbles(size = .05, col = "#1B9E77", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'A', main.title.position = 'center', main.title.size = 1)

ur <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_th) +
  tm_bubbles(size = .05, col = "#D95F02", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'B', main.title.position = 'center', main.title.size = 1)

bl <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_uvp) +
  tm_bubbles(size = .05, col = "#7570B3", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'C', main.title.position = 'center', main.title.size = 1)

br <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_matchups) +
  tm_bubbles(size = .05, col = "#E7298A", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'D', main.title.position = 'center', main.title.size = 1)

tmap_arrange(ul, ur, bl, br, ncol = 2, nrow = 2)

# https://www.datanovia.com/en/blog/the-a-z-of-rcolorbrewer-palette/
# display.brewer.all()
# brewer.pal(n = 8, name = "Dark2")


```

### The environmental match-ups approach

The spatio-temporal approach lacks match-ups between UVP5 and sediment traps and $^{234}Th$ data. This is not really surprising when we look at their spatial distribution (@fig-uvp-trap-mapping) and especially their temporal distribution (@fig-sediment-trap-thorium-yearly-distribution) because the UVP5 was firstly used in 2008 while the 'historical' data reached their observation peak before 2000. Therefore, we thought of using environmental data to increase the number of match-ups.

The idea was based on the assumption that carbon fluxes could be similar in distinct regions of the ocean if those regions had similar environmental conditions thus removing spatial (horizontal) and temporal constraints on match-ups. For this, we tried different combinations of environmental variables such as surface temperature, salinity, nutrients, Chla, $b_{bp}$ or water column properties such as integrated Chla content, oxygen minimum and MLD from monthly climatologies. Then, we used those environmental variables to build a 3D environmental space using a 3D principal component analysis (PCA) decomposition and we projected UVP5 data and trap/$^{234}Th$ in that space based on the environmental characteristics from the respective coordinates and month of each UVP5 and trap/$^{234}Th$ profiles.

Once all data were projected in the environmental space, we needed a new match-up procedure. Because the PCA projection conserves the euclidean distance, we firstly computed the pairwise distance between each UVP5 profile and all trap/$^{234}Th$ profiles. Then, we looked at the minimal euclidean distance for each UVP5 profile and we plotted the minimal distance distribution (\~8000 points in the distribution). From this distribution, we computed a radius of interest (ROI) corresponding to a threshold based on the 95$^{th}$ percentile (considered as a rather objective decision criterion). For each UVP5 profile in the environmental space, we kept the closest trap/$^{234}Th$ profiles within that radius (@fig-match-ups). Finally, we still needed to find the PSD-POC match-ups on the vertical axis (depth). For this, we used a depth range proportional to the depth of the trap or $^{234}Th$ measurement following Martin's curve [@Martin1987-um] because the downward carbon flux is less attenuated with depth. Then, we averaged PSDs in that depth range (weighted by the water volume for each depth bin) to obtain a table with weighted averaged PSDs in front of POC fluxes to proceed with the minimization procedure of @Guidi2008-vp.

![Examples of trap/$^{234}Th$ match-ups (blue dots) in the geographical space that were within the ROI in the environmental space for 4 UVP5 profiles (red star).](DATA/CFLUX/moz2.jpeg){#fig-match-ups}

![Residual error $\Delta F$ (for the carbon flux) as a function of $A$ and $D$ (i.e. $B = 2D - 1$). White crosses correspond to the best ($A$, $D$) values computed during the jackknife error analysis (i.e. 1000 random subsamples of 30% of the match-ups data set).](DATA/CFLUX/jacknife.jpeg){#fig-solution-space width="550"}

This method increased the number of match-ups to \~7800 however the minimization did not find a non ambiguous minimum in the solution space (@fig-solution-space). Instead of having a global minimum, @fig-solution-space shows what looks like a 2D solution plane. In addition, I think the choice of $A$ and $D$ (or $B$, $B = 2D - 1$ ) is not constrained enough. In Figure 5 of @Guidi2008-vp, the light gray color area (low residual error) encompasses a wide region of $A$ and $B$ values where their jackknife procedure found local minima with ($A$, $B$) pairs such as (50, 2) or (150, 5). A is the single-particle carbon or mass flux for particles of size d$_{0}$ (i.e. a reference size bin) however it is mostly the values of $B$ that should really be questioned since it is related to the fractal dimension $D$ of particles and aggregates. Fractal dimensions should not exceed 3 (physical limit) but they have been estimated down to 1.1-1.3 [@Alldredge1988-lx]. In addition, it is possible to obtain coherent POC fluxes by tuning the $A$ value for a fixed $B$ therefore I am not sure that $A$ and $B$ are truly independent.

Several other reasons could explain the 'failure' of this method. Firstly, the match-up procedure relies on a very strong initial assumption, especially when we deal with PSD measurements (i.e. direct measurements) and monthly climatologies. Second, the choice of environmental variables we used was limited and most likely insufficient to represent the dynamics of the system at surface responsible for the downward POC flux. Thirdly, the UVP5 data set of @Kiko2022-sk is composed of UVP5 SD and HD data hence the choice of size classes is also an issue if the minimization is done on a wide size spectrum due to instrumental differences. Fourthly, the model itself as well as the minimization procedure could be revised. In their paper, @Bisson2022-bt clearly enonciates the limitations of using the UVP to assess POC fluxes and I agree (see also @sec-BGC-sensors). All things considered, I think the search for a global relationship to derive POC fluxes from UVP data might try to generalize a relationship that cannot be generalized. Some studies did try to apply the ($A$, $B$) coefficients of @Guidi2008-vp but the results were orders of magnitude different from local ($A$, $B$) estimations [@Iversen2010-an; @Fender2019-ou] hence the question of its global applicability.

### The machine learning approach

So far, we have seen that the match-up quest was not enough in the geographical space and not working in the environmental space but what about a method that does not need any match-ups? With XGBoost, it is possible to create a regression model to predict carbon fluxes using environmental variables and geographical variables (depth) as predictors (i.e. explanatory variables) and trap/$^{234}Th$ POC fluxes as target variables. Once the model is trained, it can be applied to the entire data set of PSDs (\> 1.5 million) to derive POC fluxes. We tried to build such a model with a few predictors (i.e. depth, surface temperature, Chla, $b_{bp}$ and the oxygen minimum in the water column). Our model had a $R^{2}$ of 0.47 with a percentage of explained variance dominated by depth (54%), surface Chla (16%), surface temperature (13%) and the oxygen minimum (10%). The minimization of @Guidi2008-vp applied on all true PSDs and predicted POC fluxes led to the same kind of solution space as shown in @fig-solution-space.

Instead of directly predicting carbon fluxes using environmental variables, we could focus on predicting PSD slopes or PSDs. In a recent paper, @Clements2022-oq used physical and biological variables (see their Table 1) to predict global maps of PSD slope and biovolume at the base of the EZ using UVP5 data and a random forest algorithm. @Clements2022-oq highlighted the importance of Chla (i.e. proxy of biomass), oxygen (i.e. proxy of respiration) and MLD as predictors in their random forest model. They also emphasized the importance of including phytoplankton functional groups and abundance as key controlling factors on both the PSD slope and the biovolume in future models due to their impact on the downward POC flux (i.e. aggregation/disaggregation processes). The study of @Denvil-Sommer2022-oi used both random forest and XGBoost models to predict small and large POC concentrations using geographical (depth, latitude and longitude), physical (incident light, MLD and temperature), chemical (phosphate and nitrate) and biological variables (Chla and 12 Plankton Functional Types (PFTs) such as diatoms or microzooplankton). @Denvil-Sommer2022-oi showed that for small POC, the most important predictors are the depth level, temperature, microzooplankton and phosphate while the large POC is mostly driven by depth, temperature, MLD, microzooplankton, phaeocystis (another PFT), phosphate and Chla averaged over the MLD. Overall, the results of these studies suggest that it is possible to link the surface environmental and ecosystem structure to the POC distribution in the ocean interior.

### Improving the PSD-POC relationship

I used three methods to derive POC fluxes from UVP5 PSDs. The main issues I encountered were related to the number and accuracy of match-ups as well as the choice of environmental variables to link the surface ocean to the POC distribution below. With the deployment of new BGC-Argo floats equipped with both the UVP6 and the OST as well as the core biogeochemical variables (e.g. Chla, $b_{bp}$, incident light) of the BGC-Argo program, we will hopefully be able to better constraint the PSD-POC relationship however it will probably need a few more years to acquire enough profiles in different oceanic regions to better understand the key drivers of the downward POC flux.

```{=latex}
\newpage
```
## Continuous vertical sequestration

Using the CONVERSE approach, we have estimated that the global carbon sequestration ($\geq$ 100 years) flux by the BCP is 0.9-2.6 Pg C y$^{-1}$. This is 2-3 and 3-6 times larger than POC fluxes at, respectively, 1,000 and 2,000 m. CONVERSE sequestration flux estimates are also up to 4 times larger than those from studies assuming that sequestration occurs below a fixed reference depth [@Honjo2008-kn; @Guidi2015-sj; @Henson2012-ck]. Here below, I highlight some key points and ways to improve our estimations.

### Marine carbon dioxide removal strategies

The goal of marine carbon dioxide removal (mCDR) strategies [@Boyd2023-xf] is the removal of excess CO$_{2}$ from the atmosphere and the sequestration of carbon away from the atmosphere for a given time period [@National_Academies_of_Sciences2022-ah]. Several mCDR strategies have been proposed such as increasing alkalinity to enhance the chemical uptake of atmospheric CO$_{2}$ [@Hartmann2023-vk], increasing the photosynthetic uptake of atmospheric CO$_{2}$ with iron and macronutrients fertilization [@Blain2007-ws] or directly injecting CO$_{2}$ into the deep ocean [@Siegel2021-oz].

We have seen with CONVERSE that carbon sequestration occurs on the whole water column and that using a fixed reference depth would likely miss the bulk of sequestration. Therefore, the evaluation of future mCDR strategies using biologically-driven carbon sequestration fluxes should be estimated over the entire water column.

### Particle injection pumps

Our study on the sequestration flux by the BCP also shows that the contribution of the vertical migration and mixing pumps represent at least 21-34% of the sequestration flux by the BCP. However, those two pumps could probably be better estimated with a multi-instruments approach [@Claustre2021-tx].

#### Physically mediated particle injection pump

With CONVERSE, we estimate that the mixing pump contributes to 11-23% of the sequestration flux by the BCP. However, we were not able to distinct the contribution of the three identified pathways composing the mixing pump due to lacking information in the literature. This could be improved by using the observational framework developed in @Claustre2021-tx by combining satellite, ship-based, mooring and AUV data. BGC-Argo floats have shown that they are adapted to study the detrainment of mixed-layer particles [@DallOlmo2014-ot; @DallOlmo2016-ex] and so have gliders for the eddy subduction pump [@Stukel2017-lc; @Omand2015-mf]. Nonetheless, the study of the large-scale subduction pump will be more challenging because it will require a larger number of measurements made at the scale of ocean basins and over annual cycles [@Claustre2021-tx], hence the need for a multi-instruments approach to better understand it.

#### Biologically mediated particle injection pump

With CONVERSE, we estimate that the vertical migration pump contributes to 9-12% (diel) and 0.1-0.4% (seasonal). The estimation of the latter was limited to the northern North Atlantic thus it is clearly underestimated. However, the estimation of vertically migrating (either daily or seasonally) zooplankton and other animals (e.g. fish, jellyfish) is expected to improve with robotic measurements at high temporal resolution and over the entire annual cycle [@Claustre2021-tx]. The combination of acoustic sensors [@Reiss2021-rk; @De_Robertis2021-nw], optical sensors [@Picheral2021-sx] and vertical profiles of biogeochemical variables (e.g. Chla, $b_{bp}$, CDOM) have already shown promising results to detect the migration of mesopelagic organisms in the water column [@Haentjens2020-dc].

```{=latex}
\newpage
```
## Contributions of this work

The embedded classification for AUVs will contribute to a better comprehension and quantification of the vertical migration pump. So far, BGC-Argo floats equipped with the UVP6 and the classification model have been deployed in some oceanic regions (e.g. Labrador Sea, Kerguelen Plateau) and have just started to acquire data. In addition to the particle (\> 0.6 mm) identification, the UVP6 also sends their mean area and gray levels which will hopefully contribute to better characterize their morphological traits and therefore better predict their sinking rates (cfr. size versus excess density debate). The latter are important to estimate the downward carbon flux and the sequestration time.

The BCP can no longer be seen as the sole gravitational settling of particles. Physically and biologically mediated particle injection pumps need to be taken into consideration which is highlighted by our work that estimates their contribution to 21-34% of the total BCP sequestration flux. Our work also shows that the common assumption of sequestration below a fixed reference depth (typically 1,000 or 2,000 m) misses the bulk of sequestration flux. Therefore, the evaluation of mCDR efficiencies using biologically-driven carbon sequestration fluxes should be estimated over the entire water column.

This work has shown the first comparison of BGC-Argo floats equipped with both the UVP6 and an OST to better quantify carbon fluxes from particle counts and beam attenuation. If the prediction of carbon fluxes at global scale was not a success, this work has highlighted current difficulties and ways to move forward using machine learning approaches. The deployment of BGC-Argo floats in the frame of REFINE with the combination of UVP6 measurements (particle counts and identification), OST beam attenuation fluxes as well as bio-optical variables (e.g. Chla, $b_{bp}$, incident light) will likely improve our understanding of processes that affect the attenuation of the downward carbon flux at local scale.
