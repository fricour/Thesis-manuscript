# General discussion and perspectives

## UVP6 classification model: From start to finish

The development of the UVP6 classification model started without a data set of labeled UVP6 images. Therefore, a significant part of my work was to test and prepare a fully functional data preparation and model training pipeline for the upcoming UVP6 data set using UVP5 images. In the following sections, I will describe a brief historic of what has been done prior to the acquisition and human labeling of UVP6 images.

### Trust index

<!-- https://datascience.stackexchange.com/questions/61761/how-does-xgboost-use-softmax-as-an-objective-function -->

In multiclass classification task, the goal is to predict the label (i.e. class) of a given input among a defined number of labels. In @sec-uvp6, I explained that XGBoost minimizes a regularized objective function $L$ (see @eq-objective-function-xgboost) during model training. In practice, the XGBoost library proposes two choices of objective functions for multiclass classification problems: `multi:softmax` and `multi:softprob` (https://xgboost.readthedocs.io/en/latest/parameter.html, last accessed: 03/03/2023). Both functions are based on the softmax normalization which normalizes the classification score of each class using an exponential function (@eq-softmax). `multi:softmax` returns the class of highest probability whereas `multi:softprob` outputs a class probability array, which allows the computation of custom objective functions.

$$
\sigma(x_{i}) = \frac{e^{x_{i}}}{\sum_{j=1}^{N}e^{x_{j}}}, i = 1,...,N
$$ {#eq-softmax}

where $x_{i}$ and $N$ are, respectively, the classification score of the $i^{th}$ class and the number of classes.

::: {.blackbox data-latex=""}
::: {.flushleft data-latex=""}
**Information Box 4.** **Cross entropy loss**
:::

Cross entropy loss is a loss function often used in multiclass classification problems [@Geron2019-is].

$$
J({\Theta}) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{(i)}log(\hat{p}_{k}^{(i)})
$$

where $J$ is the loss (i.e. cost) function, $\Theta$ a parameter matrix, $m$ the number of training data (i.e. instances), $K$ the number of classes, $y_{k}^{(i)}$ the true probability that the $i^{th}$ instance belongs to class $k$ (0 or 1) and $\hat{p}_{k}^{(i)}$ the predicted probability that the instance belongs to class $k$. However, the XGBoost library can be confusing in comparison with the paper of @Chen2016-xt and especially @eq-objective-function-xgboost. In practice, if one sets the objective function to `multi:softmax`, it means that the objective of the model is to predict the highest probability for the true class while minimizing a loss function specified by the user (in our case, cross entropy).
:::

By definition, `multi:softmax` amplifies the difference between classification scores. Here is a simple illustration of this effect. Let A, B and C, three classes of zooplankton whose classification scores are $(1,7,8)$. Using @eq-softmax, the output probabilities are $(0.0006, 0.26, 0.73)$. Therefore, a unit difference in the classification scores of class B and class C results in a probability three times higher for class C than class B.

This lead us to define what we called a trust index, that is a metric whose score indicates the degree of confidence in the classification score. In other words, we looked for the best error estimator (@fig-trust-index). To this end, we trained a first classification model with classes from the UVP5 data set using a `multi:softprob` objective function. Then, we used the model to make a prediction on a test set that returned a matrix $(M \times N)$ of $M$ images and $N$ classes. Afterwards, we computed four metrics on each row of the probability matrix: the maximum probability ($max$, equivalent to `multi:softmax`), the softmax of the probability array ($softmax$, equivalent to a double `multi:softmax`), the difference between the highest and the second highest probability ($diff_{max}$) and the difference between the highest and second highest probability of the softmax array ($diff_{softmax}$). We compared those metrics for each class of the test set in @fig-trust-index which clearly shows that all metrics were very similar and that they captured the errors at the same speed. Thus, it meant that we could keep using XGBoost as we initially did, that is using `multi:softprob` as the objective function. @fig-trust-index also shows that some classes kept accumulating errors in the prediction with increasing rank (e.g. *Copepoda*, darksphere and fiber\<detritus). In their case, an increasing rank is not a trustworthy indicator of increasing confidence.

```{r, trust index, dpi = 300, fig.height = 12, fig.width= 10}
#| label: fig-trust-index
#| fig-cap: Percentage of error captured as a function of rank for four metrics (max, softmax, diff$_{max}$ and diff$_{softmax}$) on six UVP5 classes (4 biological, 2 non biological). The number of objects predicted in each class is provided in the title of each subplot.

library(gridExtra)
library(grid)
library(jpeg)

path <- paste0(getwd(), '/image/uvp6/trust_index')

grid.arrange(
  rasterGrob(as.raster(readJPEG(paste0(path, "/Aulacanthidae.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/Aulosphaeridae.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/Copepoda.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/darksphere.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/detritus.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/fiber_detritus.jpeg")))),
  ncol = 2
)
```

### Technical constraints on the classification model

Because the UVP6-LP is designed to work on AUVs, several constraints were imposed on the model to fulfill technical and energy requirements. The UVP6 data treatment at sea shown in @fig-uvp-pipeline briefly describes the steps from the image acquisition to the prediction. The two most energy consuming steps are the image segmentation (median execution time of 650 ms) and the loading of the classification model whose execution time was capped at 700 ms. This limitation imposed a direct constraint on the prediction quality of the model because it implied a limitation on the choice of the model hyperparameters (learning rate, tree depth) and the number of boosting rounds.

The second critical aspect of the model was its limitation at 40 classes. In the UVP5 SD data set, the most detailed (human) classification at a taxonomic level was roughly 150 classes (referred to as $level_{0}$). It was therefore necessary to merge classes based on morphological and ecological attributes to decrease that number in order to get to $level_{1}$ that had 38 classes (reminder, it is currently limited at 40). From $level_{1}$, we thus experimented a lot of different prediction models with a hyperparameters gridsearch in order to find both the right set of hyperparameters and the right number of classes. Still, some $level_{1}$ classes had very low precision and recall scores because they were confused with some specific taxonomic groups hence we merged them accordingly with those groups. This trial-error approach was necessary to create the final aggregation level ($level_{2}$) used for the final UVP5 model. The same work had to be done later with the UVP6 data because the taxonomic groups were not identical.

The last technical limitation was the number of features (see @tbl-features). Before I started my PhD, 55 features (basically geometry, gray levels, statistical moments) were defined. I was not allowed (though, I asked!) to do some feature engineering, that is either doing feature selection (selecting the most useful features) either doing feature combination (combining existing features to create new ones) or creating new features by collecting new data [@Geron2019-is]. Why was this proposition refused? Again, those 55 selected features had been tested earlier and they were compliant with the energy budget. Therefore, adding more features was not a possibility because we would have needed to redo tests and energy budgets that were not possible at the time. In addition, all 55 features were computed quickly hence they were not consuming a lot of energy so it was decided to just keep them. At worse, some features would have been redundant, without impacting the classification quality.

Nonetheless, when we compare the results from a features-based model and a CNN (@tbl-classification-report-uvp6-xgboost-cnn), perhaps the results would have been closer if we actually did some feature engineering and added other kinds of features.

```{r, mean and median abundances in a typical vertical profile}
#| label: tbl-stats-uvp6
#| tbl-cap: Mean and median concentrations and number of segmented objects whose size is above 0.6 mm (referred to as "vignettes" in french) expected from a standard vertical profile taken by a BGC-Argo float equipped with a UVP6 in different parts of the water column.

library(gt)
library(gtsummary)
library(gtExtras)
library(tibble)
library(tidyverse)
library(latex2exp)

stats_uvp6 <- tibble(mean_conc = c(119, 457, 110, 76)/1000, 
                     median_conc = c(85, 332, 59, 38)/1000, 
                     mean_nb =  c(426,146,143,137),
                     median_nb = c(305,106,77,75),
                     layer = c('0-2000', '0-100', '100-500', '500-2000'),
                     water_volume = c(3.57, 0.32, 1.3, 1.95)*1000)

stats_uvp6 |> gt() |> cols_label(mean_conc = 'Mean',
                                 median_conc = 'Median',
                                 mean_nb = 'Mean',
                                 median_nb = 'Median',
                                 layer = 'Depth range (m)',
                                 water_volume = 'Sampled volume (L)') |>
  tab_spanner(label = 'Concentration (#/L)',
              columns = c(mean_conc, median_conc)) |>
   tab_spanner(label = 'Number',
              columns = c(mean_nb, median_nb))
```

### Detritus, this ubiquitous mess

We saw in the previous section that the median number of objects detected in 3.5 m$^{3}$ is roughly 300. In addition, it is usual to have less than 20% of living objects hence the vast majority of images are labelled as detritus. The latter are ubiquitous in the ocean and display a wide variety of shapes, sizes and textures (@fig-detritus). In @sec-improve-classif, I showed that using several classes of detritus did not improve the classification model and that it actually made it worse because the N-detritus classes showed a high level of confusion between each other.

![Examples of UVP objects labelled as detritus. The scale bar is the same for all (2 mm).](image/detritus/moz_detritus.jpg){#fig-detritus}

![Examples of UVP objects labelled as living material. The scale bar is the same for all (2 mm). Note that I have hidden three detritus from @fig-detritus to highlight the difficulty of classifying plankton already at the human level.](image/detritus/uvp_user_meeting/living_except3.jpg){#fig-living}

We assumed that detritus form a continuum in the 55-dimensional feature space. Therefore, we tried several methods to break this continuum while also trying to cope with the data set class imbalance. For this, we tried under-sampling the detritus to decrease their number and non random under-sampling by keeping only detritus close to K-means clustered centroids but it did not improve the classification model. Using a clustering method to split the detritus into several classes (e.g. fluffy gray or dark compact) to improve the classification of biological objects was based on the strong belief of some colleagues that were convinced it would improve the classification. However, my work and the work of another PhD student that tried using several detritus classes using a CNN (not published yet) showed that it did not improve the classification model either. Nonetheless, it is true that other clustering methods could be further explored. On the top of my head, I think about DBSCAN [@Schubert2017-xr] and HDBSCAN [@Campello-2013-cb] which are both density-based clustering methods in contrast to K-Means with is a partitioning clustering method that needs a predefined number of clusters in input. Other clustering methods could obviously be explored [see @Geron2019-is, pp. 259-260] based on their computational complexity and the data set characteristics (number of instances and features).

### The future of the embedded classification

#### UVPec

[UVPec](https://github.com/ecotaxa/uvpec/) - short name for Underwater Vision Profiler Embedded Classifier - is a toolbox to train classification models for UVP6 images and evaluate their performance. This python package has been designed to be as user-friendly as possible in order to be accessible to the largest number of users. Basically, the only mission of the user is to create folders (40 maximum because the maximum number of classes is fixed at 40) of labelled UVP6 images and specify some hyperparameters (e.g. learning rate) to train a classification model. The evaluation of the created model provides a confusion matrix and a classification report to help the user evaluate the quality of the model as well as the cross entropy curve to have information on the optimal boosting round number (before the model overfits).

Overall, the goal of this package is to propose a simple and fast way to create new classification models for UVP6 applications on platforms that need to send data in near-real time or that cannot be easily retrieved.

#### More regions to be explored

The Labrador Sea was the first of 9 key oceanic regions studied in the frame of the REFINE (Robots Explore plankton-driven Fluxes in the marine twIlight zoNE) project (see https://erc-refine.eu). The latter aims to study regions with distinct biogeochemical conditions and responses to climate (@fig-refine).

```{r, map refine}
#| label: fig-refine
#| fig-cap: Oceanic regions (Baffin Bay (BB), Labrador Sea (LS), Greenland Sea (GS), Guinea Dome (GD), Arabian Sea (AS), Tropical Indian Ocean (TIO), South Pacific subtropical Gyre (SPG), West Kerguelen Plateau (WKP), East Kerguelen Plateau (EKP)) investigated in the frame of REFINE.

library(tmap)
library(tibble)
data("World")
library(sf)
library(ggspatial)
library(rnaturalearth)
library(rnaturalearthdata)
library(ggplot2)
library(RColorBrewer)

regions <- tibble(Region = c('BB', 'GS', 'LS', 'GD', 'AS', 'TIO', 'SPG', 'WKP', 'EKP'),
                  long_name = c('Baffin Bay', 'Greenland Sea', 'Labrador Sea', 'Guinea Dome',
                                'Arabian Sea', 'Tropical Indian Ocean', 'South Pacific subtropical gyre','West Kerguelen Plateau', 'East Kerguelen Plateau'),
                  lat = c(74, 77, 62, 9, 19, -7.7, -32, -49.6, -50),
                  lon = c(-67.4, -3, -55, -21, 64, 54, -130, 63.6, 75.6))


points_sf <- st_as_sf(regions, coords = c("lon", "lat"), crs = "+proj=longlat")

# # map
tm_shape(World) +
  tm_polygons() +
  tm_shape(points_sf) +
  tm_bubbles(size = .3, col = 'Region', alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.position = c("left", "center"), legend.frame = T)

# # load world data
# world <- ne_countries(scale = "medium", returnclass = "sf")
# 
# ggplot(data = world) +
#     geom_sf() +
#     xlab("Longitude") + ylab("Latitude") + theme_bw() +
#     geom_point(data = regions, aes(x = lon, y = lat, colour = Region), size = 4) +
#     scale_colour_manual(values = brewer.pal(9, "Set3")) +
#   geom_point(data = regions, aes(x = lon, y = lat), colour = 'black', size = 4, shape = 1)

```

@fig-kerguelen present some preliminary results from the UVP6 embedded classification that highlights the difference in *Calanoida* and *Chaetognatha* concentrations between February and March 2023 in the western and eastern part of the Kerguelen Plateau. For these floats, the configuration was different from the ones deployed in the Labrador Sea with a higher profiling time resolution after deployment, which can be further modified based on observations.

```{r, kerguelen, fig.width=8, fig.height=5}
#| label: fig-kerguelen
#| fig-cap: $\textit{Calanoida}$ and $\textit{Chaetognatha}$ concentrations in February-March 2023 in the Kerguelen Plateau.

library(vroom)
library(tidyverse)
library(patchwork)
library(lubridate)
library(viridis)

# load data
calanoids_east <- vroom('DATA/ARGO/1902593_class_number_4_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Calanoida - East') 

calanoids_west <- vroom('DATA/ARGO/4903657_class_number_4_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Calanoida - West') 

chaeto_east <- vroom('DATA/ARGO/1902593_class_number_5_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Chaetognatha - East')
chaeto_west <- vroom('DATA/ARGO/4903657_class_number_5_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Chaetognatha - West')

taxo_kerguelen <- rbind(calanoids_east, calanoids_west, chaeto_east, chaeto_west) |> 
  mutate(group = factor(group, levels = c('Calanoida - East', 'Calanoida - West',
                                     'Chaetognatha - East', 'Chaetognatha - West')))

ggplot(taxo_kerguelen) + geom_point(aes(x = conc_object, y = depth, colour = date)) + scale_colour_viridis_c(trans = "date",labels = c("2023-02-11","2023-02-18","2023-02-25","2023-03-04","2023-03-11"), breaks = c(as.Date("2023-02-11"), as.Date("2023-02-18"), as.Date("2023-02-25"), as.Date("2023-03-04"), as.Date("2023-03-11"))) + scale_y_reverse() + theme_bw() +
  labs(x = 'Concentration (#/L)', colour = "Date", y = 'Depth (m)') +
  facet_wrap(~group, scale = "free_x")
```

#### New algorithms

Unless we find a cheap and efficient (low power) way to send UVP6 images via Iridium communication, we are stuck with the issue of embedded classification. In @sec-uvp6, I showed that CNNs usually outperform features-based classifiers. However, the field of computer vision is constantly evolving with the arrival of new technologies that could help improve the automatic classification of zooplankton images. Transformers [@Vaswani2017-il], also referred to as Foundation Models in @Bommasani2021-kbare are the latest cutting-edge tool in the field of computer vision and natural language processing [e.g. @Li2022-qt; @Liu2021-jq]. The structure of transformers rely on attention mechanisms able to draw global dependencies between input and output [@Vaswani2017-il]. In other words, it is able to learn dependencies between distant positions in a network architecture in constrast to CNNs. As an example, @Li2022-qt used a vision transformer model combining a CNN and a transformer to identify apple diseases. In their model, the CNN is used to extract the global features of the input image while the transformer extracts local features of the disease region to help the CNN see better. How does it help the CNN? In the case of apple images with different backgrounds (different weather conditions, brightness changes, other objects in the field of view), the CNN could be "distracted" hence the transformer is able to focus the attention of the CNN on the region of interest (the disease region) and not on irrelevant features such as the weather. The use of transformers for automatic zooplankton classification has just begun [@Kyathanahally2022-ma] but it should certainly be explored further in the near future.

#### UVP6m

There is a class called "other\<living" in the UVP6 classification model. I did not mention earlier than I was obliged to place copepods (except *Calanoida*) in that class. Indeed, *Calanoida* are a "large" species of copepods compared to other copepods found in the Mediterranean Sea that are smaller. The classification of the latter was not good (low precision and low recall) hence we put them in the "other\<living" class, which is a shame.

The good news is that the next generation of UVP is already in the making with the UVP6m (m for micro) which will be able to look at smaller objects (particle counter between 10 and 200 $\mu$m and plankton/particle imaging \> 100 $\mu$m). The UVP6m will be more compact than the UVP6 but the principles will be the same and both instruments will share 5 size classes for intercalibration.

## Deriving carbon fluxes from particle size distribution

Before working on the CONVERSE framework, I was trying to find a global relationship (I also tried to find a regional one, without more success) to derive carbon fluxes from UVP5 PSDs. The idea was to reproduce the method of @Guidi2008-vp on the newly quality controlled UVP5 PSD data set [@Kiko2022-sk]. For this, I needed to build a global data set of POC measurements derived from sediment traps and $^{234}Th$ (see @tbl-poc-from-sediment-traps, @tbl-poc-from-thorium and [here](https://fricour.shinyapps.io/carbon_fluxes_app/) for a more interactive view). Here below, I briefly review three methods we explored to find a POC-PSD relationship and what could be further investigated.

```{=latex}
%\includepdf[pages=1]{"/home/flo/dox/THESIS/bg-18-755-2021"}
```
```{=html}
<!--#https://github.com/quarto-dev/quarto-cli/discussions/1042-->
<style type="text/css">
caption, .table-caption {
  text-align: left;
}
</style>
```
```{r, poc database from traps}
#| label: tbl-poc-from-sediment-traps
#| tbl-cap: Compilation of POC flux measurements from sediment traps. Data taken from public databases listed on PANGAEA or extracted from articles (with the help of Frédéric Le Moigne and Meike Vogt). The number of measurements could be different from the original publication after data cleaning and potential removal of duplicates.
#| tbl-cap-location: bottom

library(dplyr)
library(knitr)
library(kableExtra)
library(purrr)

# read data
trap <- vroom::vroom('DATA/Trap_Thorium/TRAP_DATABASE.csv')
#thorium <- vroom::vroom('DATA/Trap_Thorium/THORIUM_DATABASE.csv')

# clean a bit
trap[trap$doi == 'https://doi.org/10.1007/s00227-018-3376-1',]$doi <- '10.1007/s00227-018-3376-1'
trap[trap$doi == 'https://doi.org/10.1016/j.dsr.2016.08.015',]$doi <- '10.1016/j.dsr.2016.08.015'
trap[trap$doi == 'Geochimica et Cosmochimica Acta, Vol. 66, No. 3, pp. 457–465, 2002',]$doi <- '10.1016/S0016-7037(01)00787-6'

# some stats
trap_resume <- trap |> group_by(doi) |> dplyr::summarise(data_points = n(), max_date = lubridate::year(max(start_date)), min_date = lubridate::year(min(start_date))) |> arrange(desc(data_points)) |>
  mutate(period_covered = if_else(max_date > min_date, paste0(min_date,'-',max_date), paste0(max_date)))

# BASED ON : https://stackoverflow.com/questions/48506712/kable-kableextra-cells-with-hyperlinks
# make url links
make_link <- function(doi){
  link <- paste0('https://doi.org/',doi)
  return(link)
}
url <- map_chr(trap_resume$doi, make_link)

# # put url in dataframe
# trap_resume <- trap_resume |> mutate(doi_url = cell_spec(doi, 'html', link = url)) |> select(doi_url, data_points) |>
#   kable('html', escape = FALSE, col.names = c('Sources of data', 'Number of measurements'), align = 'l') #|> kable_styling(bootstrap_options = c("hover", "condensed"))

trap_resume <- trap_resume |> mutate(doi_url = cell_spec(doi, link = url)) |> select(doi_url, period_covered, data_points) |>
  kable(escape = FALSE, col.names = c('Source of data', 'Period covered', 'Number of observations'), align = 'l') 

# make final table
trap_resume
```

```{r, poc database from thorium}
#| label: tbl-poc-from-thorium
#| tbl-cap: Compilation of POC flux measurements derived from Thorium 234. Data taken from public databases listed on PANGAEA or extracted from articles (with the help of Frédéric Le Moigne). The number of measurements could be different from the original publication after data cleaning and potential removal of duplicates.
#| tbl-cap-location: bottom

# read data
thorium <- vroom::vroom('DATA/Trap_Thorium/THORIUM_DATABASE.csv')

# some stats
thorium_resume <- thorium |> group_by(doi) |> summarise(data_points = n(), max_date = lubridate::year(max(start_date)), min_date = lubridate::year(min(start_date))) |> arrange(desc(data_points)) |>
  mutate(period_covered = if_else(max_date > min_date, paste0(min_date,'-',max_date), paste0(max_date)))
thorium_resume$doi <- c('10.1594/PANGAEA.809717', '10.1029/2018GB00615', '10.1525/elementa.2020.030')

# make url links
url <- c('https://doi.org/10.1594/PANGAEA.809717', 'https://doi.org/10.1029/2018GB006158', 'https://doi.org/10.1525/elementa.2020.030')

# put url in dataframe
thorium_resume <- thorium_resume |> mutate(doi_url = cell_spec(doi, link = url)) |> select(doi_url, period_covered, data_points) |>
  kable(escape = FALSE, col.names = c('Source of data', 'Period covered', 'Number of observations'), align = 'l') #|> kable_styling(bootstrap_options = c("hover", "condensed"))

# make final table
thorium_resume
```

### The straightforward approach

This approach consisted in simply applying the method of @Guidi2008-vp (see @eq-carbon-flux-finite). Therefore, I looked for match-ups (i.e. spatially and temporally close observations) between UVP5 and POC observations within a 0.2° latitude and longitude window, a 10 m vertical depth range and a 5-day window. @Guidi2008-vp found 118 match-ups (with UVP2/3/4 data) whereas I only found 108 match-ups with UVP5 data, located on the western and eastern parts of the Kerguelen Plateau, in the Guinea Dome and in the California Current (panel D in @fig-uvp-trap-mapping). As a result, we decided not to go further because we would not have enough match-ups to improve the minimization (see @eq-cost-function).

```{r, uvp traps th234 map}
#| label: fig-uvp-trap-mapping
#| fig-cap: Spatial distribution of sediment traps (A), $^{234}Th$ (B), UVP5 (C) and their spatio-temporal match-ups (D)

d <- vroom::vroom('DATA/CFLUX/uvp_trap.csv')
guidi_matchups <- vroom::vroom('DATA/CFLUX/guidi_matchups.csv')

d_sf <- st_as_sf(d, coords = c("lon", "lat"), crs = "+proj=longlat")

d_uvp <- d_sf |> filter(source == 'UVP5')
d_trap <- d_sf |> filter(source == 'Trap')
d_th <- d_sf |> filter(source == 'Thorium')
d_matchups <- st_as_sf(guidi_matchups, coords = c("lon", "lat"), crs = "+proj=longlat")

# map
# tm_shape(World) +
#   tm_polygons() +
#   tm_shape(d_sf) +
#   tm_dots(size = .2, col = "black", alpha = 1, border.col = 'black', border.alpha = 1) +
#   tm_layout(legend.show = F) +
#   tm_facets(by = 'source')

# up left
ul <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_trap) +
  tm_bubbles(size = .05, col = "#1B9E77", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'A', main.title.position = 'center', main.title.size = 1)

ur <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_th) +
  tm_bubbles(size = .05, col = "#D95F02", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'B', main.title.position = 'center', main.title.size = 1)

bl <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_uvp) +
  tm_bubbles(size = .05, col = "#7570B3", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'C', main.title.position = 'center', main.title.size = 1)

br <- tm_shape(World) +
  tm_polygons() +
  tm_shape(d_matchups) +
  tm_bubbles(size = .05, col = "#E7298A", alpha = 1, border.col = 'black', border.alpha = 1) +
  tm_layout(legend.show = F, main.title = 'D', main.title.position = 'center', main.title.size = 1)

tmap_arrange(ul, ur, bl, br, ncol = 2, nrow = 2)

# https://www.datanovia.com/en/blog/the-a-z-of-rcolorbrewer-palette/
# display.brewer.all()
# brewer.pal(n = 8, name = "Dark2")


```

### The environmental match-ups approach

The spatio-temporal approach lacks match-ups between UVP5 and sediment traps and $^{234}Th$ data. This is not really surprising when we look at their spatial distribution (@fig-uvp-trap-mapping) and especially their temporal distribution (@fig-sediment-trap-thorium-yearly-distribution) because the UVP5 was firstly used in 2008 while the 'historical' data reached their observation peak before 2000. Therefore, we thought of using environmental data to increase the number of match-ups.

The idea was based on the assumption that carbon fluxes could be similar in distinct regions of the ocean if those regions had similar environmental conditions thus removing spatial (horizontal) and temporal constraints on match-ups. For this, I tried different combinations of environmental variables such as surface temperature, salinity, nutrients, combination of nutrients, Chla, $b_{bp}$ or water column properties such as integrated Chla content, oxygen minimum and MLD from monthly climatologies. Then, I used those environmental variables to build a 3D environmental space using a 3D principal component analysis (PCA) and I projected UVP5 data and trap/$^{234}Th$ in that space based on the environmental characteristics from the respective coordinates and month of each UVP5 and trap/$^{234}Th$ profiles.

Once all data were projected in the environmental space, I had to find a new match-up procedure. Because the PCA projection conserves the euclidean distance, I firstly computed the pairwise distance between each UVP5 profile and all trap/$^{234}Th$ profiles. Then, I looked at the minimal euclidean distance for each UVP5 profile and I plotted the minimal distance distribution (\~8000 points in the distribution). From this distribution, I computed a radius of interest (ROI) corresponding to a threshold based on the 95$^{th}$ percentile (considered as a rather objective decision criterion). For each UVP5 profile in the environmental space, I kept the closest trap/$^{234}Th$ profiles within that radius (@fig-match-ups). Finally, I still needed to find the PSD-POC match-up on the vertical axis (depth). For this, I used a depth range proportional to the depth of the trap or $^{234}Th$ measurement following Martin's curve [@Martin1987-um] because the downward carbon flux is less attenuated with depth. Then, I averaged PSDs in that depth range (weighted by the water volume) to obtain a table with weighted averaged PSDs in front of POC fluxes to do the minimization procedure of @Guidi2008-vp.

![Examples of match-ups in the environmental space for 4 UVP5 profiles (red dots) and their respective trap/$^{234}Th$ profiles within the ROI.](DATA/CFLUX/moz.jpeg){#fig-match-ups}

dire que le choix des variables ( + le fait que ce sont des climatos -\> bof bof, ça manquait de variavbles physiques, pas assez de variables probablement) et aussi discuter du plan de phase (voir Rmarkdown de la prez waringan + revoir cette prez là pour vérifier que l'on a rien oublié) + critiquer la figure de lionel car pour moi le problème se situe déjà là, j'ai obsrvé que ça fitte tout, on prend un petit b pour u petite A et grand b pour un grand A

Bottom line, I failed. I tried multiple times but it never worked

### The machine learning approach

on a fait avec des cimatos etc mais après y a moyen d'ajouter des meilleires données (eg alberto)

-   faire un résumé de la photo du tableau sur le tableau blanc d Lionel.. pour dire la marche à suivre.. et les ouvelles données environnemntales aussi et le multioutpubut graidnet boosting
