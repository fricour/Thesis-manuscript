# General discussion and perspectives

## UVP6 classification model: From start to finish

The development of the UVP6 classification model started without a data set of labeled UVP6 images. Therefore, a significant part of my work was to test and prepare a fully functional data preparation and model training pipeline for the upcoming UVP6 data set using UVP5 images. In the following sections, I will describe a brief historic of what has been done prior to the acquisition and human labeling of UVP6 images.

### Trust index

<!-- https://datascience.stackexchange.com/questions/61761/how-does-xgboost-use-softmax-as-an-objective-function -->

In multiclass classification task, the goal is to predict the label (i.e. class) of a given input among a defined number of labels. In @sec-uvp6, I explained that XGBoost minimizes a regularized objective function $L$ (see @eq-objective-function-xgboost) during model training. In practice, the XGBoost library proposes two choices of objective functions for multiclass classification problems: `multi:softmax` and `multi:softprob` (https://xgboost.readthedocs.io/en/latest/parameter.html, last accessed: 03/03/2023). Both functions are based on the softmax normalization which normalizes the classification score of each class using an exponential function (@eq-softmax). `multi:softmax` returns the class of highest probability whereas `multi:softprob` outputs a class probability array, which allows the computation of custom objective functions.

$$
\sigma(x_{i}) = \frac{e^{x_{i}}}{\sum_{j=1}^{N}e^{x_{j}}}, i = 1,...,N
$$ {#eq-softmax}

where $x_{i}$ and $N$ are, respectively, the classification score of the $i^{th}$ class and the number of classes.

::: {.blackbox data-latex=""}
::: {.flushleft data-latex=""}
**Information Box 4.** **Cross entropy loss**
:::

Cross entropy loss is a loss function often used in multiclass classification problems [@Geron2019-is].

$$
J({\Theta}) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{(i)}log(\hat{p}_{k}^{(i)})
$$

where $J$ is the loss (i.e. cost) function, $\Theta$ a parameter matrix, $m$ the number of training data (i.e. instances), $K$ the number of classes, $y_{k}^{(i)}$ the true probability that the $i^{th}$ instance belongs to class $k$ (0 or 1) and $\hat{p}_{k}^{(i)}$ the predicted probability that the instance belongs to class $k$. However, the XGBoost library can be confusing in comparison with the paper of @Chen2016-xt and especially @eq-objective-function-xgboost. In practice, if one sets the objective function to `multi:softmax`, it means that the objective of the model is to predict the highest probability for the true class while minimizing a loss function specified by the user (in our case, cross entropy).
:::

By definition, `multi:softmax` amplifies the difference between classification scores. Here is a simple illustration of this effect. Let A, B and C, three classes of zooplankton whose classification scores are $(1,7,8)$. Using @eq-softmax, the output probabilities are $(0.0006, 0.26, 0.73)$. Therefore, a unit difference in the classification scores of class B and class C results in a probability three times higher for class C than class B.

This lead us to define what we called a trust index, that is a metric whose score indicates the degree of confidence in the classification score. In other words, we looked for the best error estimator (@fig-trust-index). To this end, we trained a first classification model with classes from the UVP5 data set using a `multi:softprob` objective function. Then, we used the model to make a prediction on a test set that returned a matrix $(M \times N)$ of $M$ images and $N$ classes. Afterwards, we computed four metrics on each row of the probability matrix: the maximum probability ($max$, equivalent to `multi:softmax`), the softmax of the probability array ($softmax$, equivalent to a double `multi:softmax`), the difference between the highest and the second highest probability ($diff_{max}$) and the difference between the highest and second highest probability of the softmax array ($diff_{softmax}$). We compared those metrics for each class of the test set in @fig-trust-index which clearly shows that all metrics were very similar and that they captured the errors at the same speed. Thus, it meant that we could keep using XGBoost as we initially did, that is using `multi:softprob` as the objective function. @fig-trust-index also shows that some classes kept accumulating errors in the prediction with increasing rank (e.g. *Copepoda*, darksphere and fiber\<detritus). In their case, an increasing rank is not a trustworthy indicator of increasing confidence.

```{r, trust index, dpi = 300, fig.height = 12, fig.width= 10}
#| label: fig-trust-index
#| fig-cap: Percentage of error captured as a function of rank for four metrics (max, softmax, diff$_{max}$ and diff$_{softmax}$) on six UVP5 classes (4 biological, 2 non biological). The number of objects predicted in each class is provided in the title of each subplot.

library(gridExtra)
library(grid)
library(jpeg)

path <- paste0(getwd(), '/image/uvp6/trust_index')

grid.arrange(
  rasterGrob(as.raster(readJPEG(paste0(path, "/Aulacanthidae.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/Aulosphaeridae.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/Copepoda.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/darksphere.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/detritus.jpeg")))),
  rasterGrob(as.raster(readJPEG(paste0(path, "/fiber_detritus.jpeg")))),
  ncol = 2
)
```

### Technical constraints on the classification model

Because the UVP6-LP is designed to work on AUVs, several constraints were imposed on the model to fulfill technical and energy requirements. The UVP6 data treatment at sea shown in @fig-uvp-pipeline briefly describes the steps from the image acquisition to the prediction. The two most energy consuming steps are the image segmentation (median execution time of 650 ms) and the loading of the classification model whose execution time was capped at 700 ms. This limitation imposed a direct constraint on the prediction quality of the model because it implied a limitation on the choice of the model hyperparameters (learning rate, tree depth) and the number of boosting rounds.

The second critical aspect of the model was its limitation at 40 classes. In the UVP5 SD data set, the most detailed (human) classification at a taxonomic level was roughly 150 classes (referred to as $level_{0}$). It was therefore necessary to merge classes based on morphological and ecological attributes to decrease that number in order to get to $level_{1}$ that had 38 classes (reminder, it is currently limited at 40). From $level_{1}$, we thus experimented a lot of different prediction models with a hyperparameters gridsearch in order to find both the right set of hyperparameters and the right number of classes. Still, some $level_{1}$ classes had very low precision and recall scores because they were confused with some specific taxonomic groups hence we merged them accordingly with those groups. This trial-error approach was necessary to create the final aggregation level ($level_{2}$) used for the final UVP5 model. The same work had to be done later with the UVP6 data because the taxonomic groups were not identical.

The last technical limitation was the number of features (see @tbl-features). Before I started my PhD, 55 features (basically geometry, gray levels, statistical moments) were defined. I was not allowed (though, I asked!) to do some feature engineering, that is either doing feature selection (selecting the most useful features) either doing feature combination (combining existing features to create new ones) or creating new features by collecting new data [@Geron2019-is]. Why was this proposition refused? Again, those 55 selected features had been tested earlier and they were compliant with the energy budget. Therefore, adding more features was not a possibility because we would have needed to redo tests and energy budgets that were not possible at the time. In addition, all 55 features were computed quickly hence they were not consuming a lot of energy so it was decided to just keep them. At worse, some features would have been redundant, without impacting the classification quality.

Nonetheless, when we compare the results from a features-based model and a CNN (@tbl-classification-report-uvp6-xgboost-cnn), perhaps the results would have been closer if we actually did some feature engineering and added other kinds of features.

```{r, mean and median abundances in a typical vertical profile}
#| label: tbl-stats-uvp6
#| tbl-cap: Mean and median concentrations and number of segmented objects whose size is above 0.6 mm (referred to as "vignettes" in french) expected from a standard vertical profile taken by a BGC-Argo float equipped with a UVP6 in different parts of the water column.

library(gt)
library(gtsummary)
library(gtExtras)
library(tibble)
library(tidyverse)
library(latex2exp)

stats_uvp6 <- tibble(mean_conc = c(119, 457, 110, 76)/1000, 
                     median_conc = c(85, 332, 59, 38)/1000, 
                     mean_nb =  c(426,146,143,137),
                     median_nb = c(305,106,77,75),
                     layer = c('0-2000', '0-100', '100-500', '500-2000'),
                     water_volume = c(3.57, 0.32, 1.3, 1.95)*1000)

stats_uvp6 |> gt() |> cols_label(mean_conc = 'Mean',
                                 median_conc = 'Median',
                                 mean_nb = 'Mean',
                                 median_nb = 'Median',
                                 layer = 'Depth range (m)',
                                 water_volume = 'Sampled volume (L)') |>
  tab_spanner(label = 'Concentration (#/L)',
              columns = c(mean_conc, median_conc)) |>
   tab_spanner(label = 'Number',
              columns = c(mean_nb, median_nb))
```

### Detritus, this ubiquitous mess

We saw in the previous section that the median number of objects detected in 3.5 m$^{3}$ is roughly 300. In addition, it is usual to have less than 20% of living objects hence the vast majority of images are labelled as detritus. The latter are ubiquitous in the ocean and display a wide variety of shapes, sizes and textures (@fig-detritus). In @sec-improve-classif, I showed that using several classes of detritus did not improve the classification model and that it actually made it worse because the N-detritus classes showed a high level of confusion between each other.

![Examples of UVP objects labelled as detritus. The scale bar is the same for all (2 mm).](image/detritus/moz_detritus.jpg){#fig-detritus}

![Examples of UVP objects labelled as living material. The scale bar is the same for all (2 mm). Note that I have hidden three detritus from @fig-detritus to highlight the difficulty of classifying plankton already at the human level.](image/detritus/uvp_user_meeting/living_except3.jpg){#fig-living}

We assumed that detritus form a continuum in the 55-dimensional feature space. Therefore, we tried several methods to break this continuum while also trying to cope with the data set class imbalance. For this, we tried under-sampling the detritus to decrease their number and non random under-sampling by keeping only detritus close to K-means clustered centroids but it did not improve the classification model. Using a clustering method to split the detritus into several classes (e.g. fluffy gray or dark compact) to improve the classification of biological objects was based on the strong belief of some colleagues that were convinced it would improve the classification. However, my work and the work of another PhD student that tried using several detritus classes using a CNN (not published yet) showed that it did not improve the classification model either. Nonetheless, it is true that other clustering methods could be further explored. On the top of my head, I think about DBSCAN [@Schubert2017-xr] and HDBSCAN [@Campello-2013-cb] which are both density-based clustering methods in contrast to K-Means with is a partitioning clustering method that needs a predefined number of clusters in input. Other clustering methods could obviously be explored [see @Geron2019-is, pp. 259-260] based on their computational complexity and the data set characteristics (number of instances and features).

### The future of the embedded classification

#### UVPec

[UVPec](https://github.com/ecotaxa/uvpec/) - short name for Underwater Vision Profiler Embedded Classifier - is a toolbox to train classification models for UVP6 images and evaluate their performance. This python package has been designed to be as user-friendly as possible in order to be accessible to the largest number of users. Basically, the only mission of the user is to create folders (40 maximum because the maximum number of classes is fixed at 40) of labelled UVP6 images and specify some hyperparameters (e.g. learning rate) to train a classification model. The evaluation of the created model provides a confusion matrix and a classification report to help the user evaluate the quality of the model as well as the cross entropy curve to have information on the optimal boosting round number (before the model overfits).

Overall, the goal of this package is to propose a simple and fast way to create new classification models for UVP6 applications on platforms that need to send data in near-real time or that cannot be easily retrieved.

#### More regions to be explored

The Labrador Sea was the first of 9 key oceanic regions studied in the frame of the REFINE (Robots Explore plankton-driven Fluxes in the marine twIlight zoNE) project (see https://erc-refine.eu). The latter aims to study regions with distinct biogeochemical conditions and responses to climate (@fig-refine).

```{r, map refine}
#| label: fig-refine
#| fig-cap: Oceanic regions (Baffin Bay (BB), Labrador Sea (LS), Greenland Sea (GS), Guinea Dome (GD), Arabian Sea (AS), Tropical Indian Ocean (TIO), South Pacific subtropical Gyre (SPG), West Kerguelen Plateau (WKP), East Kerguelen Plateau (EKP)) investigated in the frame of REFINE.

library(tmap)
library(tibble)
data("World")
library(sf)

regions <- tibble(Region = c('BB', 'GS', 'LS', 'GD', 'AS', 'TIO', 'SPG', 'WKP', 'EKP'),
                  long_name = c('Baffin Bay', 'Greenland Sea', 'Labrador Sea', 'Guinea Dome',
                                'Arabian Sea', 'Tropical Indian Ocean', 'South Pacific subtropical gyre','West Kerguelen Plateau', 'East Kerguelen Plateau'),
                  lat = c(74, 77, 62, 9, 19, -7.7, -32, -49.6, -50),
                  lon = c(-67.4, -3, -55, -21, 64, 54, -139, 63.6, 75.6))


points_sf <- st_as_sf(regions, coords = c("lon", "lat"), crs = "+proj=longlat")

# map
tm_shape(World) +
    tm_polygons() +
    tm_shape(points_sf) +
    tm_bubbles(size = .3, col = 'Region', alpha = 1, border.col = 'black', border.alpha = 1)

```

@fig-kerguelen present some preliminary results from the UVP6 embedded classification that highlights the difference in *Calanoida* and *Chaetognatha* concentrations between February and March 2023 in the western and eastern part of the Kerguelen Plateau.

```{r, kerguelen, fig.width=8, fig.height=5}
#| label: fig-kerguelen
#| fig-cap: $\textit{Calanoida}$ and $\textit{Chaetognatha}$concentrations between February-March 2023 in the Kerguelen Plateau.

library(vroom)
library(tidyverse)
library(patchwork)
library(lubridate)
library(viridis)

# load data
calanoids_east <- vroom('DATA/ARGO/1902593_class_number_4_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Calanoida - East') 

calanoids_west <- vroom('DATA/ARGO/4903657_class_number_4_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Calanoida - West') 

chaeto_east <- vroom('DATA/ARGO/1902593_class_number_5_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Chaetognatha - East')
chaeto_west <- vroom('DATA/ARGO/4903657_class_number_5_FromNetCDF.csv') |> 
  filter(PhaseName == 'NPAR') |> mutate(date = as.Date(date), group = 'Chaetognatha - West')

taxo_kerguelen <- rbind(calanoids_east, calanoids_west, chaeto_east, chaeto_west) |> 
  mutate(group = factor(group, levels = c('Calanoida - East', 'Calanoida - West',
                                     'Chaetognatha - East', 'Chaetognatha - West')))

ggplot(taxo_kerguelen) + geom_point(aes(x = conc_object, y = depth, colour = date)) + scale_colour_viridis_c(trans = "date",labels = c("2023-02-11","2023-02-18","2023-02-25","2023-03-04","2023-03-11"), breaks = c(as.Date("2023-02-11"), as.Date("2023-02-18"), as.Date("2023-02-25"), as.Date("2023-03-04"), as.Date("2023-03-11"))) + scale_y_reverse() + theme_bw() +
  labs(x = 'Concentration (#/L)', colour = "Date", y = 'Depth (m)') +
  facet_wrap(~group)
```

#### New algorithms

Unless we find a cheap and efficient (low power) way to send UVP6 images via Iridium communication, we are stuck with the issue of embedded classification. In @sec-uvp6, I showed that CNNs usually outperform features-based classifiers. However, the field of computer vision is constantly evolving with the arrival of new technologies that could help improve the automatic classification of zooplankton images. Transformers [@Vaswani2017-il], also referred to as Foundation Models in @Bommasani2021-kbare are the latest cutting-edge tool in the field of computer vision and natural language processing [e.g. @Li2022-qt; @Liu2021-jq]. The structure of transformers rely on attention mechanisms able to draw global dependencies between input and output \[@Vaswani2017-il\]. In other words, it is able to learn dependencies between distant positions in a network architecture in constrast to CNNs. As an example, @Li2022-qt used a vision transformer model combining a CNN and a transformer to identify apple diseases. In their model, the CNN is used to extract the global features of the input image while the transformer extracts local features of the disease region to help the CNN see better. How does it help the CNN? In the case of apple images with different backgrounds (different weather conditions, brightness changes, other objects in the field of view), the CNN could be "distracted" hence the transformer is able to focus the attention of the CNN on the region of interest (the disease region) and not on irrelevant features such as the weather. Therefore, transformers should definitely be on the to-do list of anyone working on improving automatic zooplankton classification.

#### UVP6m

There is a class called "other\<living" in the UVP6 classification model. I did not mention earlier than I had to put the copepods in this class. Indeed, *Calanoida* are a "large" species of copepods compared to other copepods found in the Mediterranean Sea that are smaller. The classification of the latter was really not good (low precision and low recall) hence we put them in the "other\<living" class, which is a shame.

The good news is that the next generation of UVP is already in the making with the UVP6m (m for micro) which will be able to look at smaller objects (particle counter between 10 and 200 $\mu$m and plankton/particle imaging \> 100 $\mu$m). The UVP6m will be more compact than the UVP6 but the principles will be the same and both instruments will share 5 size classes for intercalibration.
