# Case study in the Labrador Sea {#sec-case-study-labrador-sea}

## Introduction

The biological carbon pump (BCP) refers to a series of processes that transport biogenic carbon produced in sunlit surface waters to the ocean interior [@Claustre2021-tx]. It is composed of three main pathways [@Claustre2021-tx; @Boyd2019-ez; @Siegel2022-wd]: the biological gravitational pump that is the gravitational settling of particulate organic carbon (POC); the physically mediated particle injection pump that represents physical mechanisms that transport dissolved organic carbon (DOC) to depth via mixing processes; and the biologically mediated particle injection pump that characterizes the transport of POC (and DOC) by diel and seasonal migration of animals in the water column. Historically, the BCP was mainly studied with sediment traps [@Honjo2008-kn] and Thorium-234 radionuclide disequilibria [@Buesseler2007-ql] focused on the gravitational pump hence the contribution of DOC and migrating animals to the BCP was poorly known. However, the advent of autonomous underwater vehicles (AUVs) equipped with a variety of physical and biogeochemical sensors have demonstrated their ability to better understand them [e.g. @Omand2015-mf; @DallOlmo2016-ex; @Llort2018-pa; @Lacour2019-id; @Haentjens2020-dc; @Stukel2017-lc]. Still, we are missing processes to close the carbon budget [@Giering2014-tj; @Boyd2019-ez] and unkowns remain in the under-sampled mesopelagic (i.e. twilight) zone [@Martin2020-wa]. In order to fill the observational gaps, the Biogeochemical-Argo (BGC-Argo) network aims to develop an operational global array of 1000 profiling floats measuring six core variables from the surface to 2000 m [@Claustre2020-jo]: oxygen, nitrate, pH, chlorophyll a (Chla), suspended particles ($b_{bp}$) and downwelling irradiance. In addition to the six core variables, new sensors have been developped to target specific BCP-related questions [@Claustre2020-jo; @Claustre2021-tx; @Chai2020-uq]. In this study, we will focus on BGC-Argo floats equipped with two additional sensors: the Underwater Vision Profiler (UVP6) [@Picheral2021-sx] and a transmissometer used as an optical sediment trap (OST).

The UVP6 is a portable version of the UVP5 [@Picheral2010-to], specifically designed for AUVs [@Picheral2021-sx]. It consists of an underwater camera that counts particles and images objects in, respectively, the 0.1 - 2.5 cm and 0.8 - 2.5 cm size range. In contrast with UVP5 deployments on cabled platforms, UVP6 deployed on long-term drifting AUVs (e.g. BGC-Argo floats) are rarely recovered. Therefore, the latest version of the UVP6 also embarks an embedded image classification algorithm that transmits the abundance of 20 classes (16 biological classes) in near-real time. The OST consists of a vertically mounted transmissometer whose upward-facing optical window intercepts sinking particles. The latter have already been used on AUVs [@Bishop2004-at; @Bishop2009-ka; @Estapa2013-gw; @Estapa2017-jr; @Estapa2019-nk] and have demonstrated their usefulness to characterize the gravitational pump.

For the first time, two pairs of BGC-Argo floats equipped with both the UVP6 and an OST were deployed (May 2022) in the Labrador Sea (LS) to investigate the seasonal and episodic dynamics of particle and carbon fluxes. The LS is a basin of the subpolar North Atlantic (NA) ocean which represents 20% of the global ocean carbon sink \[@Takahashi2009-rv\]. The efficiency of this sink is explained by both deep winter convection that traps CO$_{2}$ from the atmosphere \[@Vage2008-qa; @Yashayaev2009-fe; @Yashayaev2017-mn\] and by the largest spring phytoplankton bloom \[@Siegel2002-hk\] that consumes CO$_{2}$ for the photosynthetic production of organic matter. The triggering mechanism of the NA bloom was initially explained by the 'Critical Depth Hypothesis' of @Sverdrup1953-ki that defines a critical surface mixing depth where phytoplankton growth is equal to phytoplankton losses. According to @Sverdrup1953-ki, a bloom is therefore triggered when the surface mixed layer depth (MLD) is shallower than this critical depth resulting in a positive phytoplankton growth rate. However, this hypothesis was challenged by @Behrenfeld2010-my who suggested that the initiation of the NA blooms begins in winter with a slow phytoplankton biomass increase over the following months (i.e. 'Dilution-Recoupling' hypothesis). These hypotheses have been investigated with BGC-Argo floats but the debate still remains \[@Mignot2018-gm; @Yang2020-hw; @Boss2010-do\]. For the LS, @Lacour2015-da used a climatology of satellite Chla to divide the NA into 6 bioregions with specific bloom phenologies. In their bioregionalization (see also their Figure 2), the LS is characterized by two bioregions with distinct bloom phenologies on both side of the 60°N parallel. The northern part is characterized by a short spring bloom in late April whereas the bloom in the southern part starts in April-May and slowly develops to reach its maximum in June. @Lacour2015-da concluded that the concurrent influence of surface light and mixing was the first-order mechanism controlling the onset of the spring bloom in both bioregions. This is coherent with the results of @Wu2008-vw and @Frajka-Williams2010-ph who found that early shoaling of the mixed layer was responsible for the early spring bloom in the northern part whereas the light controlled the bloom in the southern part.

In late May 2022, two pairs of BGC-Argo floats were deployed in the southern part of the LS at the time of the spring bloom. While BGC-Argo floats typically drift at 1000 m for \~10 days between each ascending profile, these two pairs of floats were configured to drift at three distinct parking depths (200 m, 500 m and 1000 m) to study POC and particle fluxes derived from beam attenuance \[@Estapa2023-oz\] and UVP6 particle size distributions (PSD). The goals of this study are threefold: 1) Compare the UVP6 and OST as two independent measurements of downward carbon fluxes and particle abundance; 2) Explore the seasonal signal of the downward POC flux and particle abundance with Chla and MLD using BGC-Argo floats and satellite data; 3) Evaluate the new drifting configuration and how it could be improved in regards with our preliminary results.

## Methods

### Deployment and floats configuration

Two couples of BGC-Argo floats were deployed in the Labrador Sea in late May 2022. The first couple (WMO 6904240 and 4903634) was deployed on the 22$^{nd}$ at coordinates (-49.3°E, 58.9°N) while the second (WMO 6904241 and 1902578) was deployed on the 30$^{th}$ at coordinates (-52.4°E, 56.8°N).

All 4 floats used in this study are PROVOR Jumbo floats (NKE Instrumentation, France) equipped with a CTD SBE41-CP (Sea-Bird Scientific, USA), a UVP6-LP (Hydroptic, France), a c-Rover 660 nm beam transmissometer (WETLabs, USA) and a ECO FLBBCD (WETLabs, USA) that includes a chlorophyll-a (Chla) fluorometer (excitation at 470 nm, emission at 695 nm), a colored dissolved organic matter (CDOM) fluorometer (excitation at 370 nm, emission at 460 nm) and a 700 nm particle backscatteringmeter ($b_{bp}$). All floats but one (1902578) were equipped with a OCR-504 Multispectral Radiometer to measure the photosynthetic available radiation.

All 4 floats have been configured to drift at 3 distinct parking depths (respectively, 200 m, 500 m and 1000 m for approximately 1, 3 and 5 days) to study the downward carbon flux. During these drift phases, measurements of particles abundance (14 size classes, between 0.1 mm and 2.5 mm) are taken every 20 minutes at 200 m and every 2h at 500 m and 1000 m whereas $c_{p}$ measurements are taken every 30 minutes at all depths. This configuration can be adjusted during the mission of the float via two-way Iridium communication.

```{r, load libraries}
library(tidyverse)
library(akima)
library(broom)
library(cmocean)
library(lubridate)
library(scales)
library(patchwork)
library(viridis)
library(ggridges)
library(ggvis)
library(pracma)
library(gbm)
library(MLmetrics)
library(purrr)
library(arrow)
library(plotly)
library(oce)
library(ggplot2)
library(latex2exp)
library(cowplot)
library(RColorBrewer)
library(vroom)
library(castr)
library(reactablefmtr)
library(gt)
#library(gtExtras)
library(spatialrisk)
library(ggOceanMaps)
```

```{r, map of 4 floats, fig.height=6, fig.width=6}
#| label: fig-float-position
#| fig-cap: Route of each float since their deployment (black triangles). Dots represent ascending profiles. The profile that concludes the time series for each float is highlighted by a black contour.

# read floats data
float_6904240 <- vroom('/home/flo/dox/ArgoShine/6904240_FromNetCDF.csv')
float_6904241 <- vroom('/home/flo/dox/ArgoShine/6904241_FromNetCDF.csv')
float_4903634 <- vroom('/home/flo/dox/ArgoShine/4903634_FromNetCDF.csv')
float_1902578 <- vroom('/home/flo/dox/ArgoShine/1902578_FromNetCDF.csv')

# combine all data
all_floats <- rbind(float_6904240, float_6904241, float_4903634, float_1902578)

# add levels for some field
all_floats <- all_floats |> dplyr::mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)), park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

# summarize localization of floats
argo_map <- all_floats |> select(WMO = wmo, cycle, lat, lon) |> drop_na(lat) |> dplyr::group_by(WMO, cycle) |> dplyr::mutate(WMO = factor(WMO)) |>
  dplyr::summarise(lat = mean(lat), lon = mean(lon))

argo_map_first <- argo_map |> filter(cycle == 1)
argo_map_last <- argo_map |> filter(cycle == max(cycle))

# prepare data for geomapping, see https://mikkovihtakari.github.io/ggOceanMaps/
argo_map_geo <- transform_coord(argo_map) |> dplyr::mutate(cycle = argo_map$cycle, WMO = as.factor(argo_map$WMO))
argo_map_last_geo <- transform_coord(argo_map_last) |> mutate(WMO = as.factor(argo_map_last$WMO))

p <- basemap(limits = c(-55,-40,55,65), bathymetry = T, lat.interval = 2.5, bathy.style = 'poly_blues') 

# Make the graticules:
lims <- attributes(p)$limits 
graticule <- sf::st_graticule(
  c(lims[1], lims[3], lims[2], lims[4]), 
  crs = attributes(p)$proj,
  lon = attributes(p)$map.grid$lon.breaks, 
  lat = attributes(p)$map.grid$lat.breaks
)

p <- p + 
  geom_sf(data = graticule, color = "grey", size = LS(1)) + # graticules
  coord_sf(xlim = lims[1:2], ylim = lims[3:4], # redefine limits
           crs = attributes(p)$proj) 

p +
  geom_path(data = argo_map_geo, aes(lon, lat, colour = WMO), linewidth = 1) + 
  geom_point(data = argo_map_geo, aes(x= lon, y = lat, colour = WMO), size = 2) +
  scale_color_brewer(palette = 'Dark2') + theme_bw() +
  geom_point(data = transform_coord(argo_map_first), aes(lon, lat), shape = 17, size = 3) +
  geom_point(data = argo_map_last_geo, aes(lon, lat, colour = WMO), size = 4) +
  geom_point(data = argo_map_last_geo, aes(lon, lat), fill = 'black', size = 4, shape = 1) +
  #https://aosmith.rbind.io/2020/07/09/ggplot2-override-aes/#suppress-aesthetics-from-part-of-the-legend
  guides(color = guide_legend(override.aes = list(size = c(1.5,1.5,1.5,1.5),
                                                  shape = c(NA, NA, NA, NA)))) +
  annotation_scale(location = "br") + 
  annotation_north_arrow(location = "tr", which_north = "true") 
```

### Data processing

Data were downloaded from the Coriolis data center (*ftp://ftp.ifremer.fr/ifremer/argo/*, with biogeochemical data at *dac/coriolis/* and UVP6 data at *aux/coriolis/*, last accessed: March 6, 2023). Chla data were quality controlled for Non-Photochemical Quenching (NPQ), a photoprotective mechanism that decreases the apparent fluorescence per unit of Chla following the method implemented by @Terrats2020-ef who modified the method of @Xing2018-pn for shallow-mixing cases. Chla data were also divided by a slope factor of 2.15 following the recommendation of @Roesler2017-me for obtaining unbiased Chla estimates from in situ WETLabs ECO fluorometer. $c_{p}$ data were converted from counts to physical units (m$^{-1}$) using the following relationship

$$
c_{p}~[m^{-1}] = -\frac{1}{x}~log \left(~ \frac{c_{p}~[counts]-CSC_{dark}}{CSC_{cal}-CSC_{dark}} \right)
$$

where $x$, $CSC_{dark}$ and $CSC_{cal}$ are respectively the transmissometer pathlength (0.25 m), the sensor output when the beam is blocked (i.e. offset, 0 for each c-Rover of this study) and the sensor output with clean water in the path (12998, 13598, 13115 and 12855 for, respectively, float 4903634, 6904240, 6904241 and 1902578). The latter were obtained from the calibration sheets of each c-Rover.

UVP6 data and in particular the averaged number of particles per size class (i.e. particle size distribution hereafter referred to as PSD) was divided by the number of images at each depth and the water volume imaged by the UVP (0.7L) to obtain particle concentrations (units: #/L). Note that in an impending update by the Coriolis data center, it should not be necessary to divide by the number of images anymore.

All data were divided into vertical profiles and drifting data using time stamps for profiling and drifting acquisition periods. Finally, data were checked for instrumental anomalies. As a result, $c_{p}$ values from float 6904241 cannot be used from its 20$^{th}$ cycle onwards (\> November 17) due to a likely failure of the pump cleaning the optical window of the transmissometer. In total, the data coverage of this study spans a period of 9 months (end of May 2022 to early March 2023).

### Derivation of carbon fluxes from OST

We applied a similar approach to @Estapa2013-gw and @Estapa2017-jr to derive the continuous component (i.e. continuous deposition of small particles on the upward-facing window of the transmissometer) and discontinuous (i.e. deposition of stochastic big particles) components of the downward POC flux, hereafter referred to as, respectively, $F_{small}$ and $F_{big}$.

First, the drifting $c_{p}$ signal was cleaned from spikes (hypothesized as transient particles not landing on the window or active swimmers [@Estapa2013-gw]). For this, we computed a 7-point moving median and a 7-point moving median absolute deviation (MAD). Then, we computed the anomaly to the moving median as the difference between the drifting $c_{p}$ signal and the moving median. We considered that a $c_{p}$ measurement was a spike when the anomaly was above $5.2 \times MAD$ following @Leys2013-eu. Despiked $c_{p}$ data were then smoothed with a 7-point moving median filter.

Contrarily to previous works [@Estapa2013-gw; @Estapa2017-jr; @Estapa2023-oz], we chose an empirical statistical approach instead of fixed thresholds to detect jumps. Therefore, we firstly computed the slope between each $c_{p}$ observation for each drifting phase and computed the z-score (i.e. standard score) of each of them. The detection of jumps therefore consisted in spotting outliers in the z-score distribution. For this, we computed the interquartile range ($IQR$) between the first ($Q_{1}$) and third ($Q_{3}$) quartiles. Positive and negative jumps were defined as, respectively, a z-score above $Q_{3} + 1.5 \times IQR$ and below $Q_{1} - 1.5 \times IQR$. We found that this method better detected outliers than the one used to despike the $c_{p}$ signal.

In order to compute $F_{small}$, we firstly computed the slope of each group of observations between jumps (positive and negative, see @fig-estapa-method_article). Undefined slopes (a single point between two jumps) were removed as well as any negative slope assuming that either the so-called fit failed (leading to undetected jumps) either the transmissometer window was perturbed by something else than the pump cleaning the optical window (e.g. turbulence ou zooplankton consumption of accumulated material). Afterwards, we computed the weighted average slope ($slope_{w}$) using the previously computed slopes of each remaining group in order to derive the small particles flux using the linear relationship between the beam attenuance flux and the POC flux (see first panel of Figure 2 in @Estapa2023-oz).

$$
F_{small} = 633(0.25~slope_{w})^{0.77}
$$ {#eq-estapa}

where $slope_{w}$ is multiplied by the transmissometer pathlength due to the fact that @eq-estapa is an empirical POC-to-attenuance (ATN) relationship where ATN is the product of $c_{p}$ and the transmissometer pathlength.

We used the same relationship to compute $F_{big}$ by replacing $slope_{w}$ as the sum of positive jumps divided by the drifting time.

```{r, example of a cp signal analyzed for jumps (with an old method here)}
#| label: fig-estapa-method_article
#| fig-cap: Application of the method of @Estapa2017-jr on a drifting transmissometer to divide the $c_{p}$ signal into a continuous (dark blue) component and a discontinuous (positive jumps, red) component. Slopes for each group are depicted in light blue. A negative jump was also detected (yellow). Data from BGC-ARGO float WMO 6904240, drifting at 1000 m during its 21$^{st}$ cycle.

clean_cp_data <- function(data, moving_median_order){
  
  data <- data |> arrange(dates)
  
  if(nrow(data) == 0){ # no cp data
    return(data)
  }else{
    
    # # Remove cp > 2 m-1 (don't really know why but eh)
    # if (any(data$cp > 2)){
    #   data <- data[-which(data$cp > 2),]
    # }
    
    # despike cp data
    data$cp <- oce::despike(data$cp, reference = 'median', k = 7, replace = 'NA')
    
    # median average
    data$cp <- pastecs::decmedian(data$cp, order = moving_median_order, ends = "fill") |> pastecs::extract(component = "filtered")
  }
  return(data)
  
}

tmp <- all_floats |> filter(wmo == 6904240, park_depth == '1000 m', cycle == 21, PhaseName == 'PAR') |> drop_na(cp) |> select(dates = juld, everything())

threshold <- 0.08

clean_data <- clean_cp_data(tmp, 3)

# save cleaned Cp signal from spikes to a temporary variable
tmp2 <- clean_data
tmp2$jump <- NA
# compute slope between two adjacent points (except first and last point of drifting time series)
for (i in 2:(nrow(tmp2)-1)){
  #print(i)
  delta_x <- as.numeric(difftime(tmp2$dates[i], tmp2$dates[i-1], units = 'days'))
  delta_y <- tmp2$cp[i]-tmp2$cp[i-1]
  tmp2$jump[i] <- delta_y/delta_x
}
  
# assign a colour for the plot
tmp2$colour <- 'base signal'
tmp2$colour[which(abs(tmp2$jump)>threshold)] <- 'jump' # abs is needed for 'negative jumps'
  
# add group to compute slope (for each subgroups of points, separated from a jump)
tmp2$group <- NA
  
# compute where the jumps are (index value of the array)
jump_index <- which(tmp2$colour == 'jump')
  
# assign group 
for (i in jump_index){
  for (j in 1:nrow(tmp2)){
    if ((j < i) & (is.na(tmp2$group[j]))){
      tmp2$group[j] <- paste0('group_',i)
    }
  }
}
  
# add last group (because it is AFTER the last index so it was not computed before)
tmp2$group[which(is.na(tmp2$group))] <- 'last_group'
  
# compute slope for each subgroups
slope_df <- tmp2 |> filter(colour == 'base signal', jump != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x)
  
# remove negative slope from the mean slope (no physical meaning)
slope_df <- slope_df |> filter(slope > 0)
  
# remove if only one point (cannot fit a slope with one point)
slope_df <- slope_df |> filter(nb_points > 1)
  
# compute mean slope 
#mean_slope <- mean(slope_df$slope)
mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
# estapa relationship (see POSTER)
#poc_flux <- 633*((mean_slope*0.25)**0.77) # *0.25 because ATN = cp*0.25
poc_flux <- 633*((mean_slope)**0.77) # /!\ slope for computed for ATN on y axis (delta_y *0.25 because ATN  = cp*0.25) -> should be OK
  
# build dataframe to plot each subgroup
part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
part_slope <- rbind(part1, part2)
  
# spot negative jump
tmp2$colour[which((tmp2$colour == 'jump') & (tmp2$jump < 0))]  <- 'negative jump'
  
# add big particles flux
rows_to_keep <- c(jump_index, jump_index-1) 
tmp3 <- tmp2[rows_to_keep,] |> select(dates, cp, jump, colour, group) |> dplyr::arrange(dates)
  
# remove negative jump, if any
check_colour <- unique(tmp3$colour)
if(length(check_colour) == 3){
  index_neg_jump <- which(tmp3$colour == 'negative jump')
  tmp3 <- tmp3[-c(index_neg_jump, index_neg_jump+1),]
  tmp3 <- tmp3 |> dplyr::mutate(diff_jump = cp - lag(cp))
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else if(length(check_colour) == 2){
  tmp3 <- tmp3 |> dplyr::mutate(diff_jump = cp - lag(cp))
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else{ # NO jump
  tmp3 <- NULL
  }
  
# big particles flux
if(is.null(tmp3)){ # no jumps
  big_part_poc_flux <- 0
}else{
  tmp4 <- tmp3 |> filter(diff_jump > 0)
  if(nrow(tmp4) == 0){ # no positive jumps
    big_part_poc_flux <- 0
  }else{
    delta_y <- sum(tmp4$diff_jump) *0.25 # to get ATN (= cp*0.25)
    max_time <- max(tmp2$dates)
    min_time <- min(tmp2$dates)
    delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
    slope_big_part <- delta_y/delta_x
    big_part_poc_flux <- 633*(slope_big_part**0.77)
  }
}
  
# compute total drifting time
max_time <- max(tmp2$dates)
min_time <- min(tmp2$dates)
drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))

check <- tmp2 |> select(dates, cp, colour)
check <- check |> filter(colour %in% c('jump', 'negative jump'))

ggplot(tmp2, aes(x = dates, y = cp, colour = colour)) + geom_point(size = 3) +
    scale_colour_manual(values = c('#003366','#E31B23', '#FFC325')) + 
    #scale_shape_manual(values = c(1,1,4)) +
    geom_path(data = part_slope, aes(x = time, y = cp, group = group), colour = '#DCEEF3', size = 2) + theme_bw() + 
    labs(x = 'Time', y = TeX('$c_{p}~(m^{-1})$')) +
  geom_point(data = check, aes(x = dates, y = cp, colour = colour), size = 3) +
  geom_point(data = check, aes(x = dates, y = cp), shape = 1, size = 3, colour = 'black')  +
    theme(legend.position = 'none')
```

### Satellite data

Satellite Chla data were downloaded from a daily-merged cloud free GlobColour product (doi: [10.48670/moi-00281](https://doi.org/10.48670/moi-00281)) with a spatial resolution of 4 km. Mixed layer depth (MLD) derived from satellite observations were downloaded from near-real-time weekly data from the ARMOR3D product (doi: [10.48670/moi-00052](https://doi.org/10.48670/moi-00052)).

## Results

### OST-derived carbon fluxes

```{r, function to detect and plot jumps}

plot_jumps <- function(data){
  
  # make sure that the cp signal is in chronological order
  tmp <- data |> arrange(dates)
  
  # despike cp data with a 7-point moving window
  tmp$cp <- despike(tmp$cp, k = 3)
  
  # smooth cp data with a 3-point moving median, n time(s)
  tmp$cp <- slide(tmp$cp, fun = median, k = 3, n = 1, na.rm=T)
  
  # compute slope between two adjacent points (except first point) # we could start after 1h to let the float stabilize
  delta_x <- as.numeric(tmp$dates - lag(tmp$dates), units = 'days')
  delta_y <- tmp$cp - lag(tmp$cp)
  tmp$slope <- delta_y/delta_x
  
  # compute a Z score (assuming a normal distribution of the slopes) on the slopes
  tmp <- tmp |> dplyr::mutate(zscore = (slope - mean(slope, na.rm = T))/sd(slope, na.rm = T))
  
  # spot outliers on the Z score signal
  # interquartile range between Q25 and Q75 -> had to used that and not the despike function because slopes are often close (or equal) to 0 so it can miss clear jumps. Q25 and Q75 are more trustworthy in this case than the despike function of Jean-Olivier (see package castr on his github: https://github.com/jiho/castr)
  IQR <- quantile(tmp$zscore, probs = 0.75, na.rm=T) - quantile(tmp$zscore, probs = 0.25, na.rm=T)
  # outliers ('spikes' in the Z score signal)
  spikes_down <- tmp$zscore < quantile(tmp$zscore, 0.25, na.rm=T) - 1.5 *IQR
  spikes_up <-  tmp$zscore > quantile(tmp$zscore, 0.75, na.rm=T) + 1.5 *IQR
  spikes <- as.logical(spikes_down + spikes_up)
  
  # assign spikes
  tmp$spikes <- spikes
  
  # assign colour code to cp signal
  tmp$colour <- 'base signal' # base signal = smoothed despiked cp signal
  tmp[which(tmp$spikes == TRUE),]$colour <- 'jump'
  
  # add group to compute the slope of each group of points, separated by a jump
  tmp$group <- NA
  
  # index of jumps in the array
  jump_index <- which(tmp$colour == 'jump')
  
  # assign group identity to each group of points, separated by a jump (= subgroup)
  for (i in jump_index){
    for (j in 1:nrow(tmp)){
      if ((j < i) & (is.na(tmp$group[j]))){
        tmp$group[j] <- paste0('group_',i)
      }
    }
  }
  tmp$group[which(is.na(tmp$group))] <- 'last_group'
  
  # compute slope for each subgroup
  slope_df <- tmp |> filter(colour == 'base signal', slope != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), 
                                                                                                                  nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],
                                                                                                                  delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),
                                                                                                                  delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x) # *0.25 to convert cp to ATN
  
  # remove negative slope from the mean slope (no physical meaning)
  slope_df <- slope_df |> filter(slope > 0)
  
  # remove if only one point (cannot fit a slope with one point)
  slope_df <- slope_df |> filter(nb_points > 1)
  
  # compute weighted average slope (to take into account the fact that some subgroups might have 2 points and a high slope vs. large group of points with a small slope)
  mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
  # convert cp to POC using Estapa's relationship 
  poc_flux <- 633*(mean_slope**0.77) # /!\ slope computed for ATN on y axis (delta_y *0.25 because ATN = cp*0.25) -> should be OK
  
  # build dataframe to plot each subgroup
  part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
  part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
  part_slope <- rbind(part1, part2)
  
  # spot negative jump
  tmp$colour[which((tmp$colour == 'jump') & (tmp$slope < 0))]  <- 'negative jump'
  
  # add big particles flux to the party
  rows_to_keep <- c(jump_index, jump_index-1)
  tmp2 <- tmp[rows_to_keep,] |> select(dates, cp, slope, colour, group) |> arrange(dates)
  
  # remove negative jumps, if any
  check_colour <- unique(tmp2$colour)
  if(length(check_colour) >= 2){ # there is a least a jump (positive or negative)
    tmp2 <- tmp2 |> dplyr::mutate(diff_jump = cp - lag(cp)) 
    even_indexes <- seq(2,nrow(tmp2),2)
    tmp2 <- tmp2[even_indexes,]
  }else{ # No jump
    tmp2 <- NULL
  }
  
  if(is.null(tmp2)){ # no jump
    big_part_poc_flux <- 0
    tmp3 <- NULL
  }else{
    tmp3 <- tmp2 |> filter(diff_jump > 0)
    if(nrow(tmp3) == 0){ # no positive jumps
      big_part_poc_flux <- 0
    }else{
      delta_y <- sum(tmp3$diff_jump) *0.25 # to get ATN (= cp*0.25)
      max_time <- max(tmp$dates)
      min_time <- min(tmp$dates)
      delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
      slope_big_part <- delta_y/delta_x
      big_part_poc_flux <- 633*(slope_big_part**0.77)
    }
  }
  
  # compute total drifting time
  max_time <- max(tmp$dates)
  min_time <- min(tmp$dates)
  drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))
  
  # to plot subgroups
  part_slope_tmp <- part_slope |> dplyr::mutate(dates = time, colour = 'slope')
  
  # plot
  jump_plot <- plot_ly(tmp, x = ~dates, y = ~cp, type = 'scatter', mode = 'markers', color = ~colour, colors = c('#003366','#E31B23', '#FFC325')) |>
    add_lines(data= part_slope_tmp, x = ~dates, y = ~cp, split = ~group, color = I('#DCEEF3'), showlegend = F) |>
    layout(title= paste0('Drifting time: ', round(drifting_time,3), ' days\n',
                         'Mean ATN slope (light blue): ', round(mean_slope,3), ' day-1\n',
                         'POC flux (small particles): ', round(poc_flux,1), ' mg C m-2 day-1\n',
                         'POC flux (big particles): ', round(big_part_poc_flux,1), ' mg C m-2 day-1'), yaxis = list(title = 'Cp (1/m)'), xaxis = list(title = 'Time'))
  
  #return(jump_plot)
  #return(list('jump_plot' = jump_plot, 'jump_table' = tmp3))
  
    # adapt script to return big and small flux
  df <- tibble('max_time' = max_time, 'min_time' = min_time, 'small_flux' = poc_flux, 'big_flux' = big_part_poc_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],
               cycle = data$cycle[1])
  
  return(df)
  #return(jump_plot)
  
}

# tmp <- all_floats_slope |> filter(wmo == 6904240, park_depth == '1000 m', cycle == 21)
# plot_jumps(tmp)
```

```{r, compute Fsmall and F big}

# remove bad data from float 6904241
float_6904241_slope <- float_6904241 |> filter(cycle < 20)
# create dataframe for all drifting data (for all floats)
all_floats_slope <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
all_floats_slope <- all_floats_slope |> filter(PhaseName == 'PAR') |> drop_na(cp) |> select(depth = pres, cp, dates = juld, park_depth, wmo, cycle) |>
   dplyr::mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
          park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

# start loop for all data
wmo <- c(4903634, 6904240, 6904241, 1902578)
park_depth <- c('200 m', '500 m', '1000 m')

res <- data.frame()
for (i in wmo){
  max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
  for (j in park_depth){
    for (k in seq(1:max_cycle)){
      tmp <- all_floats_slope |> filter(wmo == i, park_depth == j, cycle == k)
      if(nrow(tmp) == 0){
        next
      }else if(nrow(tmp) < 3){ # case where there is not enough data
        next
      }else{
          output <- plot_jumps(tmp)
          res <- rbind(res, output)
      }
    }
  }
}

# keep the good fluxes
cflux <- res |> dplyr::mutate(drifting_time = difftime(max_time, min_time, units = 'days'), WMO = factor(wmo))

cflux_info_table <- cflux |> dplyr::group_by(park_depth) |> summarize(min_smallf = min(small_flux, na.rm=T), max_smallf = max(small_flux, na.rm=T),
                                                                mean_smallf = mean(small_flux, na.rm=T), median_smallf = median(small_flux, na.rm=T),
                                                                std_smallf = sd(small_flux, na.rm=T),
                                                                min_bigf = min(big_flux, na.rm=T), max_bigf = max(big_flux, na.rm=T),
                                                                mean_bigf = mean(big_flux, na.rm=T), median_bigf = median(big_flux, na.rm=T),
                                                                std_bigf = sd(big_flux, na.rm=T))

cflux$date <- as_date(cflux$min_time)
```

$F_{small}$ and $F_{big}$ were computed at each drifting depth for all floats (@fig-OST-flux). At 200 m, $F_{small}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ (average $\pm$ standard deviation). At 500 m, $F_{small}$ is comprised between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_smallf, 1)` mg C m$^{2}$ day$^{-1}$. For both drifting depths, the trend shows a decrease of $F_{small}$ from June to, respectively, January and February. Note that the drifting configuration of all 4 floats was adapted to the deepening of the MLD (Fig. XX) hence drifting at 200 m and 500 m was stopped at this period to avoid measurements bias due to turbulence. At 1000 m, all floats measured an increase of $F_{small}$ from late May to mid/end of July followed by a steady decrease towards March, with $F_{small}$ ranging between `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ on average.

```{r, plot Fsmall and Fbig, fig.width=8, fig.height=6}
#| label: fig-OST-flux
#| fig-cap: OST-derived small (top) and big (bottom) particles carbon flux for all floats at each drifting depth. Black lines and shaded areas represent, respectively, the monthly mean flux and the 95% confidence interval around the mean. There is no data after January and February at, respectively, 200 m and 500 m because the drifting was stopped at those depths due to the increase of the MLD.

# add black data to have the same x axis for all drifting depths (to show that we stopped the drifting at 200 and 500 m)
blank_data <- tibble(max_time = c(NA, NA), min_time = c(NA,NA), small_flux = c(NA,NA), big_flux = c(NA,NA), park_depth = c('200 m', '500 m'),
                     wmo = rep(4903634,2), cycle = c(NA,NA), drifting_time = c(NA,NA), WMO = rep(as.factor(4903634),2), date = rep('2023-03-05',2))

tmp <- rbind(cflux, blank_data)

up <- tmp |> ggplot(aes(x = date, y = small_flux)) +
    geom_point(data = tmp, aes(x = date, y = small_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = tmp, aes(x = date, y = small_flux), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = '', y = TeX('$F_{small}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  scale_y_continuous(trans = 'log10') +
  theme_bw() 

down <- tmp |> ggplot(aes(x = date, y = big_flux)) +
  geom_point(data = tmp, aes(x = date, y = big_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = tmp, aes(x = date, y = big_flux), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = 'Month', y = TeX('$F_{big}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  theme_bw() +
  scale_y_continuous(trans = 'log10') +
  theme(legend.position = 'none') 
  
up / down

```

$F_{big}$ was estimated at 200 m between 0 (no deposition of big particles on the transmissometer window) and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$. High fluxes (\> 100 mg C m$^{-2}$ day$^{-1}$) were measured by 3 out of 4 floats at the end of May before a sharp decrease towards July. At 500 m, $F_{big}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$, decreasing from September to February. At 1000 m, $F_{big}$ shows a similar pattern than the one observed for $F_{small}$ with an increase from June to mid-July, followed by a decrease towards March, with values ranging from 0 to `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$.

### UVP6 particle abundance

#### On drifting mode

```{r, uvp data}

lpm_class <- c("NP_Size_102", "NP_Size_128", "NP_Size_161",
                         "NP_Size_203", "NP_Size_256", "NP_Size_323",
                         "NP_Size_406", "NP_Size_512", "NP_Size_645",
                         "NP_Size_813", "NP_Size_1020", "NP_Size_1290",
                        "NP_Size_1630", "NP_Size_2050")

uvp_data <- all_floats |> filter(PhaseName == 'PAR') |> dplyr::mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date, cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> dplyr::mutate(juld = as_date(juld), WMO = factor(wmo))

uvp_data <- uvp_data |> pivot_longer(cols = lpm_class, names_to = 'size', values_to = 'conc') |> dplyr::mutate(size = size |> str_remove('NP_Size_') |> as.numeric())

# compute daily mean chla for each float at each cycle and at each drifting depth
mean_uvp_data <- uvp_data |> dplyr::group_by(cycle, WMO, size, park_depth, juld) |> dplyr::summarize(mean_conc = mean(conc, na.rm=T))
```

In this study, we focus on 14 size classes, from 102 $\mu$m to 2.5 mm. In @fig-uvp-200m, @fig-uvp-500m and @fig-uvp-1000m, we show the daily averaged concentrations of particles in each size class for each float at their respective drifting depth. At 200 m, it can be observed that particle concentration in the 102 - 512 $\mu$m range are relatively uniform between June and October before decreasing sharply towards January. Between 512 $\mu$m and 1.63 mm, concentrations are steadily decreasing from June. Above 1.63 mm, the particle abundance is already very low (\< 0.1 particle/L) in June but keeps decreasing until September when they are practically absent (\< 0.01 particle/L).

```{r, uvp data at 200 m, fig.height=8, fig.width=8}
#| label: fig-uvp-200m
#| fig-cap: Daily averaged concentrations of particle abundance in 14 size classes measured by the UVP6 for each float at 200 m. Black lines and shaded areas represent, respectively, the daily mean particle abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particle abundance is maximum.

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp200m <- ggplot_build(p200m)

test <- tmp200m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> dplyr::summarize(index_max = which.max(y), max_date = x[index_max]) |> dplyr::mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p200m
```

At 500 m, particle concentrations in the 102 - 645 $\mu$m range slowly decrease from July to September with a sharp decrease starting in October. Above 645 $\mu$m, the trend shows a maximum between the end of July and the first half of August though the concentrations of particles in those classes are very low (\< 0.5 particle/L) on the entire timeserie.

```{r, uvp data at 500 m, fig.width=8, fig.height=8}
#| label: fig-uvp-500m
#| fig-cap: Daily averaged concentrations of particle abundance in 14 size classes measured by the UVP6 for each float at 500 m. Black lines and shaded areas represent, respectively, the daily mean particle abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particle abundance is maximum.

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp500m <- ggplot_build(p500m)

test <- tmp500m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> dplyr::summarize(index_max = which.max(y), max_date = x[index_max]) |> dplyr::mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p500m
```

At 1000 m, all 4 floats measured a clear maximum in particle abundance in the 102 - 645 $\mu$m in the second half of July. Above 645 $\mu$m, the same trend can still be observed with concentrations of less than 1 particle/L.

```{r, uvp data at 1000 m, fig.width=8, fig.height=8}
#| label: fig-uvp-1000m
#| fig-cap: Daily averaged concentrations of particle abundance in 14 size classes measured by the UVP6 for each float at 1000 m. Black lines and shaded areas represent, respectively, the daily mean particle abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particle abundance is maximum.
#| fig-dpi: 96

#{r, uvp data at 1000 m, out.width="900px", out.height="800px"}

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp1000m <- ggplot_build(p1000m)

test <- tmp1000m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> dplyr::summarize(index_max = which.max(y), max_date = x[index_max]) |> dplyr::mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p1000m
```

#### On profiling mode

Vertical profiles of particle concentration in the lower (102 - 512 $\mu$m) and upper (0.512 - 2.5 mm) size range of the UVP6 shows similar patterns across all floats. In the lower part of the size spectrum, @fig-uvp6-profiling-102-406 shows the rapid emptying of the water column in November. This process occurred on the entire water column (e.g. size class 406, float 4903634) but also started with more intensity from below the surface (e.g. size class 102, float 6904240). All floats also measured a local particle abundance maximum around 800 - 1000 m whose origin could be due to a previous particle export or remineralization of bigger particles.

```{r, UVP6 profiling mode without sinking speed small classes, fig.height=8, fig.width=8}
#| label: fig-uvp6-profiling-102-406
#| fig-cap: Vertical profiles of UVP6 particle concentration for size classes in the 102 - 512 $\mu$m size range. Concentrations were normalized for each size class. The colorbar is in log$_{10}$ scale.

particles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class), MLD) |> drop_na(NP_Size_102) |> dplyr::mutate(juld = as_date(juld))

particles_bis <- particles |> pivot_longer(cols = all_of(lpm_class), names_to = 'size', values_to = 'concentration') |> dplyr::mutate(size = size |> str_remove('NP_Size_') |> as.numeric()) 

# remove buggy data from float 4903634 (negative depth for some reason)
particles_bis <- particles_bis |> filter(depth >= 0)

# rel_conc
particles_bis <- particles_bis |> dplyr::group_by(wmo, size) |> dplyr::mutate(rel_conc = (concentration-min(concentration))/(max(concentration) - min(concentration)))

up_part <- particles_bis |> filter(depth < 2000, size < 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)') 

up_part
```

In the upper part of the size range, @fig-uvp6-profiling-512-2500 shows a rapid emptying of the entire water column starting in November. In contrast with the small classes, some particles, especially above 1.5 mm, have already virtually disappeared from the water column by September. However, the rapid increase and decrease of particles on the entire water column around July-August for floats 6904241 and 1902578 (still in the same zone at that time) could suggest an export event.

```{r, UVP6 profiling mode without sinking speed big classes, fig.height=8, fig.width=8}
#| label: fig-uvp6-profiling-512-2500
#| fig-cap: Vertical profiles of UVP6 particle concentration for size classes in the 0.512 - 2.5 mm size range. Concentrations were normalized for each size class. The colorbar is in log$_{10}$ scale. The gray background depicts the absence of particle in the specified size class.

down_part <- particles_bis |> filter(depth < 2000, size >= 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_y_reverse() +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)') +
  #geom_vline(xintercept = as_date('2022-01-01')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') 

down_part
```

## Discussion

### Environmental conditions before and shortly after deployment

All floats measured similar particle concentrations at each drifting depth and on the vertical. In addition, except perhaps for the upper part of the UVP6 size spectrum for floats 6904241 and 1902578, the results do not indicate a clear hint of particle export signature. Therefore, we suggest that the 4 BGC-Argo floats mostly measured the dynamics of the seasonal, continuous, particle signal but that they missed the main transient particle export events associated with the LS spring bloom.

To estimate the time of the bloom in the deployment area, we compared MLD and surface (0 - 20 m) Chla concentrations from both satellite and in situ data (@fig-chla-mld-satellite) in a 100 km radius around each deployment. It can be seen that these estimates are in agreement, therefore we hypothesize that we can trust satellite data prior to the floats deployment. Taking this into consideration, it seems that the pair 6904240-4903634 missed a bloom that occurred in early May whereas the pair 6904241-1902578 missed a first Chla increase in late May days before their deployment. However, a second equivalent increase (median of 2 mg m$^{-3}$) happened in late June - early July, which could be a contributing factor to the apparent particle track observed around July and August.

```{r, chla and MLD from satellite, fig.height=6, fig.width=6}
#|: label: fig-chla-mld-satellite
#| fig-cap: Comparison of MLD and Chla concentrations measured from satellites (black) and BGC-Argo floats (red) following their deployment. Satellite error bars represent the first and third quartiles around the median in a 50 km radius around each deployment (each pair of floats) coordinates. In situ Chla represent the Chla median in the upper 20 m of the water column with the upper and lower incertitude in the correction factor of @Roesler2017-me.

# fetch data
npq_data <- vroom('/home/flo/dox/ArgoShine/npq_corrected_data.csv')

# clean and smoothed chla data
d <- npq_data |> select(cycle, MLD, pres, juld, wmo, chla_npq) |> dplyr::mutate(date = as_date(juld))
d <- d |> group_by(date, wmo, pres, cycle) |> summarize(mean_chla = mean(chla_npq, na.rm=T), mld = unique(MLD))
d <- d |> dplyr::mutate(smoothed_chla = smooth(mean_chla)/2.15, smoothed_chla_up = smooth(mean_chla)/(2.15-0.37), smoothed_chla_down = smooth(mean_chla)/(2.15+0.37)) # 2.15 = Roesler factor in the Labrador Sea

d$wmo <- factor(d$wmo, levels = c(6904240,4903634,6904241,1902578))

# compute chla integration in the MLD and in the 0-100 m zone
d_chla <- d |> group_by(wmo, date) |> summarize(chla_0_100m = integrate(smoothed_chla, pres, from=0, to=100),
                                       mld = unique(mld),
                                       chla_0_mld = integrate(smoothed_chla, pres, from = 0, to=mld),
                                       median_chla_0_20m = median(smoothed_chla[pres<=20], na.rm=T),
                                       median_chla_0_20m_up = median(smoothed_chla_up[pres<=20], na.rm=T),
                                       median_chla_0_20m_down = median(smoothed_chla_down[pres<=20], na.rm=T))
#
# ggplot(d_chla) + geom_point(aes(date, chla_0_100m)) + facet_wrap(~wmo, scales = 'free') + theme_bw() + labs(x = 'Month', y = 'Integrated Chla 0-100 m (CHLA UNITS)')
#ggplot(d_chla) + geom_point(aes(date, chla_0_mld)) + facet_wrap(~wmo, scales = 'free')

# add couple info
d_chla <- d_chla |> dplyr::mutate(group = if_else(wmo %in% c(6904240,4903634), '6904240 - 4903634', '6904241 - 1902578'))

#chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_with_clouds_sat.feather') |> select(time, lat, lon, chla = CHL)
chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_without_clouds_sat.feather') |> select(time, lat, lon, chla = CHL)
mld_sat <- arrow::read_feather('DATA/SATELLITE/mld_sat.feather') |> dplyr::mutate(longitude = longitude - 360) |> select(time, lat = latitude, lon = longitude, mld = mlotst)
  
# CHLA
chla_cover_deployment1 <- points_in_circle(chla_sat, lon_center = -49.2, lat_center = 58.9, radius = 100000) # 50 km around center
chla_cover_deployment1 <- chla_cover_deployment1 |> dplyr::mutate(date = as_date(time), group = '6904240 - 4903634') |> dplyr::mutate(deploy = as_date('2022-05-22'))

chla_cover_deployment2 <- points_in_circle(chla_sat, lon_center = -52.3, lat_center = 56.7, radius = 10000) # 50 km around center
chla_cover_deployment2 <- chla_cover_deployment2 |> dplyr::mutate(date = as_date(time), group = '6904241 - 1902578') |> dplyr::mutate(deploy = as_date('2022-05-30'))

# all deploy
chla_cover <- rbind(chla_cover_deployment1, chla_cover_deployment2)

chla_plot <- ggplot(chla_cover |> filter(date > '2022-03-01')) + geom_errorbar(aes(x = date, y = chla),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'black', width = .5) + theme_bw() +
  labs(x = 'Month', y = TeX('Chla (mg m$^{-3}$)'))+ facet_wrap(~group) +
  geom_vline(data = chla_cover, aes(xintercept = deploy), colour = "black", linetype = 'dashed') +
  geom_pointrange(data = d_chla |> filter(date < '2022-08-01'), aes(x = date, ymin = median_chla_0_20m_down, ymax = median_chla_0_20m_up, y = median_chla_0_20m), colour = "#E31B23", size = .25) +
   scale_x_date(labels = date_format("%m"), date_breaks = '1 month')
                
# MLD
mld_cover_deployment1 <- points_in_circle(mld_sat, lon_center = -49.2, lat_center = 58.9, radius = 100000) # 50 km around center
mld_cover_deployment1 <- mld_cover_deployment1 |> dplyr::mutate(date = as_date(time), group = '6904240 - 4903634') |> dplyr::mutate(deploy = as_date('2022-05-22'))

mld_cover_deployment2 <- points_in_circle(mld_sat, lon_center = -52.3, lat_center = 56.7, radius = 100000) # 50 km around center
mld_cover_deployment2 <- mld_cover_deployment2 |> dplyr::mutate(date = as_date(time), group = '6904241 - 1902578') |> dplyr::mutate(deploy = as_date('2022-05-30'))

mld_cover <- rbind(mld_cover_deployment1, mld_cover_deployment2)

mld_plot <- ggplot(mld_cover |> filter(date > '2022-03-01')) + geom_errorbar(aes(x = date, y = mld),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'black', width = 5) +
  labs(x = 'Month', y = 'MLD (m)') + facet_wrap(~group) + theme_bw() +
  geom_vline(data = chla_cover, aes(xintercept = deploy), colour = "black", linetype = 'dashed') +
  geom_point(data = d_chla |> filter(date < '2022-08-01') |> dplyr::mutate(week = week(date), DOY = yday(date)) |> group_by(week, group) |> summarize(median_mld = median(mld), mean_date = as_date(mean(DOY), origin = '2022-01-01')), aes(mean_date, median_mld), colour = "#E31B23") +
   scale_x_date(labels = date_format("%m"), date_breaks = '1 month') + scale_y_continuous(trans = 'log10')

chla_plot / mld_plot
```

The Chla satellite pattern for the pair 6904240-4903634 typically describes the phenology of the spring bloom in the northern part of the LS [@Lacour2015-da]. This is not surprising given that this pair was deployed one degree southward. The other pair was deployed in the bioregion where the phytoplankton biomass slowly develops during May to reach its maximum in June [@Lacour2015-da]. The Chla pattern observed in @fig-chla-mld-satellite could suggest that the development of the phytoplankton biomass was stopped by an increase in the MLD and that it started again in mid-June when the water column was well stratified. However, the Chla satellite concentration for that pair was three times lower than for the first one. Hence, it is possible that the bloom intensity was not sufficient to produce clear particle export signals, at least for BGC-Argo floats in their 10-day vertical sampling configuration.

```{r, chla corrected for NPQ and MLD}
# #|: label: fig-chla-bgc
# #|: fig-cap : Chla concentrations and MLD measured by all floats in the upper 250 m for the period following the deployment.
# 
# 
# # plot it
# ggplot(d |> filter(date < '2022-08-01')) + geom_point(aes(x = date, y = pres, colour = log10(smoothed_chla))) +
#   scale_y_reverse(limits = c(250,0)) +
#   #scale_y_continuous(trans = 'rev_sqrt') +
#   #scale_colour_viridis(option = 'A') +
#   scale_colour_cmocean(name = 'algae', oob = scales::squish, limits = c(-1,1), na.value = 'grey') +
#   scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
#   labs(x = 'Time', y = 'Depth (m)', colour = TeX('Chla (mg m$^{-3}$)')) + facet_wrap(~wmo) + theme_bw() +
#   geom_line(aes(x = date, y = mld), colour = 'black', size = 1)
```

#### Just because it needs to go somewhere

```{r, mld}

ggplot(d) + geom_line(aes(x = date, y = mld), colour = 'black') +
  facet_wrap(~wmo) + labs(x = 'Month', y = 'MLD (m)') + scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month')

```

### Comparing OST and UVP data

For the first time, BGC-Argo floats equipped with both a transmissometer and a UVP6 have been deployed to better characterize the downward carbon flux and the nature of sinking particles from a multi-instrument approach [@Claustre2021-tx]. The quantification of the POC flux using the OST method [@Bishop2004-at; @Estapa2013-gw; @Estapa2017-jr] is based on an empirical POC:ATN ratio \[REF estapa 2022\] whereas UVP6 PSD measurements are used to compute POC fluxes using @eq-guidi-article3:

$$
F = \sum_{i}^{N}C_{i}Ad_{i}^{b}
$$ {#eq-guidi-article3}

where $C_{i}$, A and b represent, respectively, the concentration of particles (number/L) for the $i^{th}$ class of mean diameter $d_{i}$ (in mm) and 2 constants with $b$ being related to the fractal dimension ($D$) of an aggregate by $D = (b +1)/2$ [@Guidi2008-vp].

The use of @eq-guidi-article3 to derive carbon fluxes from PSD was firstly implemented by @Guidi2008-vp who used a minimization procedure between sediment traps data and estimated fluxes using PSD from several previous versions of the UVP to find the optimal value of $A$ and $b$ (i.e. 12.5 and 3.81). @Iversen2010-an and @Fender2019-ou followed the same procedure and concluded that the couple of parameters found by [@Guidi2008-vp] could not be used globally at the expense of greatly over- or underestimating POC fluxes, depending on the sampling region. In addition, the formulation of such a model implies that $A$ and $b$ would be valid at global scale, at all depths, for all UVP models and across all size classes which is arguable [@Bisson2022-bt]. @eq-guidi-article3 also assumes that the flux is only size-dependent though the main driver for the settling of aggregates is still debated [@Iversen2020-kb; @Iversen2023-qj]. As a result, the direct comparison of carbon fluxes derived from both OST and UVP data with @eq-guidi-article3 should be focused on the trend rather than on absolute carbon flux values.

```{r, compute guidi fluxes at parking depth, fig.height=6, fig.width=6}
#| label: fig-comparison-ost-uvp
#| fig-cap: Comparison of total carbon fluxes derived from the OST (black) and the UVP6 (red) where the error bars represent the 25$^{th}$ and 75$^{th}$ percentiles around the median. 

compute_flux <- function(A, b, PSD){ # PSD = Particle Size Distribution
  # for classes sizes between 0.102 mm and 1.5 mm
  #mid_ESD <- c(0.1164512, 0.1463600, 0.1843915, 0.2325200, 0.2933257, 0.3691650, 0.4650400, 0.5860455, 0.7385533, 0.9280422, 1.1705684, 1.4795321) # With Rainer's code (paruvpy)
  # mid_ESD <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685,
  #              0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
  #              1.16170742, 1.46365963) # With Lionel's script
  # mid_ESD <- c(0.23051195, 0.29042685,
  #              0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
  #              1.16170742, 1.46365963) # With Lionel's script
  #mid_ESD <- c(0.287555212,0.362129811,0.455929819,0.57466512,0.724144323,0.91063714,1.147083258,1.450068964)
  mid_ESD <- c(0.29042685,
               0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
               1.16170742, 1.46365963) # With Lionel's script
  exp <- A * mid_ESD ** b
  estimated_flux <- rowSums(t(t(PSD)*exp))
  return(estimated_flux)
}

parking_data <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
parking_data <- parking_data |> filter(PhaseName == 'PAR') |> drop_na(NP_Size_102) |> select(park_depth, all_of(lpm_class), dates = juld, park_depth, wmo, cycle) |> dplyr::mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
                                   park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))
parking_spectra <- parking_data |> dplyr::select(NP_Size_256:NP_Size_1290)

parking_data <- parking_data |> dplyr::mutate(guidi_flux = compute_flux(12.5, 3.81, parking_spectra)) #|> group_by(park_depth, cycle, wmo) 

# add rainer computation
compute_flux_rainer <- function(A, b, PSD){
  
  # Rainer uses an ESD in cm ..
  mid_ESD <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685,
       0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
       1.16170742, 1.46365963, 1.84409558, 2.32341484) / 10
  exp <- A * mid_ESD ** b # because Rainer uses the units of particles/m3 and not particles/L
  estimated_flux <- rowSums(t(t(PSD)*exp))
  return(estimated_flux)
}

parking_spectra_rainer <- parking_data |> dplyr::select(lpm_class)
parking_data <- parking_data |> dplyr::mutate(rainer_flux = compute_flux_rainer(2.8649, 2.24, parking_spectra_rainer)) 
parking_data <- parking_data |> dplyr::mutate(iversen_flux = compute_flux(273, 4.27, parking_spectra)) 

# plot it
comparison_flux_ost_uvp <- merge(parking_data, cflux) |> dplyr::mutate(total_flux = small_flux + big_flux)

# add guidi flux obtained with fminsearch
# mean_d <- c(0.29042685, 0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779, 1.16170742, 1.46365963)
# comparison_flux_ost_uvp <-  comparison_flux_ost_uvp[-c(bad_index),]
# ost_flux <- comparison_flux_ost_uvp$total_flux
# spectra <- comparison_flux_ost_uvp |> select(NP_Size_256:NP_Size_1290)
# # some spectra gives 0 for all classes - makes a buggy fminsearch -> remove those lines
# bad_index <- which((rowSums(t(t(spectra)))) == 0)
# spectra <- spectra[-c(bad_index),]
# compute_flux <- function(x) sum((log10(rowSums(t(t(spectra)*x[1]*mean_d ** x[2]))) - log10(ost_flux))**2)
# x0 <- c(12.5, 2.2)
# fminsearch(compute_flux, x0 = x0)
# fmincon(x0 = x0, compute_flux, lb = c(1, 1.5), ub = c(1000, 3))

# add guidi flux using fminsearch using the optimize function of scipy in python (see a paper of someoe who did it, see Giering 2020 ou willimaoson et giering 2022)
comparison_flux_ost_uvp <- comparison_flux_ost_uvp |> dplyr::mutate(guidi_flux_fminsearch = compute_flux(21.98, 1.18, parking_spectra))

# https://stackoverflow.com/questions/41077199/how-to-use-r-ggplot-stat-summary-to-plot-median-and-quartiles
ggplot(comparison_flux_ost_uvp) + geom_line(aes(x = date, y = total_flux)) + facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) +
  geom_errorbar(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      #fun.min = function(z) {quantile(z, 0.25)},
                      #fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = '#E31B23', width = 5) +
    geom_line(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      fun = median, colour = '#E31B23') +
    geom_errorbar(aes(x = date, y = iversen_flux),
                      stat = 'summary',
                      #fun.min = function(z) {quantile(z, 0.25)},
                      #fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = '#003366', width = 5) +
    geom_line(aes(x = date, y = iversen_flux),
                      stat = 'summary',
                      fun = median, colour = '#003366') + 
  scale_y_continuous(trans = 'log10') + theme_bw() + labs(x = 'Month', y = TeX('$F_{total}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month')

```

```{r, and on the vertical?}



```

ici je compare finalement un système ultra productif (iversen) et un système pas produtif (lionel) -\> on se situerait entre les deux. Après je peux fournir els valeurs que j'ai trouvées avec le fminsearch avec la fonctiono optimize depython (21.98 et 1.18. Mais bon, cela assumerait que 'lon pourrait prendre l'OST comme référence, ce qui est un peu too much..)

```{r, correlations between OST and UVP carbon fluxes}
#| label: tbl-correlation
#| tbl-cap: TODO
correlations <- comparison_flux_ost_uvp |> dplyr::select(park_depth, wmo, cycle, guidi_flux, small_flux, big_flux, date, total_flux) |> dplyr::group_by(wmo, park_depth, cycle) |> dplyr::mutate(median_guidi_flux = median(guidi_flux)) |> dplyr::group_by(park_depth, wmo) |> dplyr::summarize(cor_small = round(cor(median_guidi_flux, small_flux),2), cor_big = round(cor(median_guidi_flux, big_flux),2), cor_total = round(cor(median_guidi_flux, total_flux),2))

correlations <- correlations |> pivot_wider(names_from = park_depth, values_from = c(cor_small, cor_big, cor_total))

gt(correlations) |> tab_spanner(label = "Small flux", columns = 2:4) |> tab_spanner(label = "Big flux", columns = 5:7) |> tab_spanner(label = 'Total flux', columns = 8:10) |>
  cols_label(wmo = 'WMO', `cor_small_200 m` = '200 m',
             `cor_small_500 m` = '500 m',
             `cor_small_1000 m` = '1000 m',
             `cor_big_200 m` = '200 m',
             `cor_big_500 m` = '500 m',
             `cor_big_1000 m` = '1000 m',
             `cor_total_200 m` = '200 m',
             `cor_total_500 m` = '500 m',
             `cor_total_1000 m` = '1000 m')

```

Figure @fig-comparison-ost-uvp shows a comparison of carbon fluxes estimated with the OST ($F_{total} = F_{small} + F_{big}$) and the UVP6 (median of all UVP measurements while the float was drifting) at all drifting depths. Overall, carbon fluxes estimated with the coefficients ($A$, $b$) of @Guidi2008-vp, are lower than the ones measured by the OST. Without additional measurements (e.g. co-deployed sediment traps), it is however impossible to decipher which measurements is closest to the truth. Nonetheless, the trends measured by both instruments are very well correlated at all drifting depths for floats 6904240 and 6904241 with correlations comprised between `r min(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,8:10])` and `r max(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,8:10])`. @tbl-correlation also shows that the correlation between UVP-derived carbon fluxes and $F_{small}$ and $F_{big}$ is `r mean(as.vector(as.matrix(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,2:7])))` on average. We calculated lower correlations with the other two floats, especially the float 1902578 at 200 m, where correlations between the UVP-derived carbon fluxes and all fluxes (small, big and total) are close to 0.

Floats equipped with both a transmissometer and a UVP6 represent an opportunity to look at the influence of parts of the size spectrum on $F_{small}$ and $F_{big}$. For this, we decided to build a regression model using XGBoost, a gradient boosting method \[REF\], using biovolumes measured by the UVP to predict the slope of $F_{small}$ and $F_{big}$ measured by the OST.

```{r, get jump info}

get_jump_info <- function(data){
  
  # make sure that the cp signal is in chronological order
  tmp <- data |> arrange(dates)
  
  # despike cp data with a 7-point moving window
  tmp$cp <- despike(tmp$cp, k = 3)
  
  # smooth cp data with a 3-point moving median, n time(s)
  tmp$cp <- slide(tmp$cp, fun = median, k = 3, n = 1, na.rm=T)
  
  # compute slope between two adjacent points (except first point) # we could start after 1h to let the float stabilize
  delta_x <- as.numeric(tmp$dates - lag(tmp$dates), units = 'days')
  delta_y <- tmp$cp - lag(tmp$cp)
  tmp$slope <- delta_y/delta_x
  
  # compute a Z score (assuming a normal distribution of the slopes) on the slopes
  tmp <- tmp |> dplyr::mutate(zscore = (slope - mean(slope, na.rm = T))/sd(slope, na.rm = T))
  
  # spot outliers on the Z score signal
  # interquartile range between Q25 and Q75 -> had to used that and not the despike function because slopes are often close (or equal) to 0 so it can miss clear jumps. Q25 and Q75 are more trustworthy in this case than the despike function of Jean-Olivier (see package castr on his github: https://github.com/jiho/castr)
  IQR <- quantile(tmp$zscore, probs = 0.75, na.rm=T) - quantile(tmp$zscore, probs = 0.25, na.rm=T)
  # outliers ('spikes' in the Z score signal)
  spikes_down <- tmp$zscore < quantile(tmp$zscore, 0.25, na.rm=T) - 1.5 *IQR
  spikes_up <-  tmp$zscore > quantile(tmp$zscore, 0.75, na.rm=T) + 1.5 *IQR
  spikes <- as.logical(spikes_down + spikes_up)
  
  # assign spikes
  tmp$spikes <- spikes
  
  # assign colour code to cp signal
  tmp$colour <- 'base signal' # base signal = smoothed despiked cp signal
  tmp[which(tmp$spikes == TRUE),]$colour <- 'jump'
  
  # add group to compute the slope of each group of points, separated by a jump
  tmp$group <- NA
  
  # index of jumps in the array
  jump_index <- which(tmp$colour == 'jump')
  
  # assign group identity to each group of points, separated by a jump (= subgroup)
  for (i in jump_index){
    for (j in 1:nrow(tmp)){
      if ((j < i) & (is.na(tmp$group[j]))){
        tmp$group[j] <- paste0('group_',i)
      }
    }
  }
  tmp$group[which(is.na(tmp$group))] <- 'last_group'
  
  # compute slope for each subgroup
  slope_df <- tmp |> filter(colour == 'base signal', slope != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), 
                                                                                                                  nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],
                                                                                                                  delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),
                                                                                                                  delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x) # *0.25 to convert cp to ATN
  
  # remove negative slope from the mean slope (no physical meaning)
  slope_df <- slope_df |> filter(slope > 0)
  
  # remove if only one point (cannot fit a slope with one point)
  slope_df <- slope_df |> filter(nb_points > 1)
  
  # compute weighted average slope (to take into account the fact that some subgroups might have 2 points and a high slope vs. large group of points with a small slope)
  mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
  # convert cp to POC using Estapa's relationship 
  poc_flux <- 633*(mean_slope**0.77) # /!\ slope computed for ATN on y axis (delta_y *0.25 because ATN = cp*0.25) -> should be OK
  
  # build dataframe to plot each subgroup
  part1 <- slope_df |> dplyr::select(group, time = min_time, cp = first_cp)
  part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
  part_slope <- rbind(part1, part2)
  
  # spot negative jump
  tmp$colour[which((tmp$colour == 'jump') & (tmp$slope < 0))]  <- 'negative jump'
  
  # add big particles flux to the party
  rows_to_keep <- c(jump_index, jump_index-1)
  tmp2 <- tmp[rows_to_keep,] |> select(dates, cp, slope, colour, group) |> arrange(dates)
  
  # remove negative jumps, if any
  check_colour <- unique(tmp2$colour)
  if(length(check_colour) >= 2){ # there is a least a jump (positive or negative)
    tmp2 <- tmp2 |> dplyr::mutate(diff_jump = cp - lag(cp)) 
    even_indexes <- seq(2,nrow(tmp2),2)
    tmp2 <- tmp2[even_indexes,]
  }else{ # No jump
    tmp2 <- NULL
  }
  
  if(is.null(tmp2)){ # no jump
    big_part_poc_flux <- 0
    tmp3 <- NULL
  }else{
    tmp3 <- tmp2 |> filter(diff_jump > 0) # remove negative
    if(nrow(tmp3) == 0){ # no positive jumps
      big_part_poc_flux <- 0
    }else{
      delta_y <- sum(tmp3$diff_jump) *0.25 # to get ATN (= cp*0.25)
      max_time <- max(tmp$dates)
      min_time <- min(tmp$dates)
      delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
      slope_big_part <- delta_y/delta_x
      big_part_poc_flux <- 633*(slope_big_part**0.77)
    }
  }
  
  # compute total drifting time
  max_time <- max(tmp$dates)
  min_time <- min(tmp$dates)
  drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))
  
  # to plot subgroups
  part_slope_tmp <- part_slope |> dplyr::mutate(dates = time, colour = 'slope')
  
  # adapt script to return big and small flux
  df <- tibble('max_time' = max_time, 'min_time' = min_time, 'small_flux' = poc_flux, 'big_flux' = big_part_poc_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],
               cycle = data$cycle[1])
  
  # clean data to send
  slope_df <- slope_df |> arrange(min_time) |> dplyr::mutate(min_drifting_time = df$min_time, max_drifting_time = df$max_time, small_flux = df$small_flux, big_flux = df$big_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],cycle = data$cycle[1])
  if(is.null(tmp3)){
    big_jump <- tmp3
  }else{
     big_jump <- tmp3 |> dplyr::mutate(park_depth = data$park_depth[1], wmo = data$wmo[1],cycle = data$cycle[1]) 
  }
  
  return(list(slope_info = slope_df, jump_info = big_jump))
}
```

TO DO HERE

```{r, compute slope and jump info}

# start loop for all data
wmo <- c(4903634, 6904240, 6904241, 1902578)
park_depth <- c('200 m', '500 m', '1000 m')

# res_slope <- data.frame()
# res_jumps <- data.frame()
# for (i in wmo){
#   max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
#   for (j in park_depth){
#     for (k in seq(1:max_cycle)){
#       tmp <- all_floats_slope |> filter(wmo == i, park_depth == j, cycle == k)
#       if(nrow(tmp) == 0){
#         next
#       }else if(nrow(tmp) < 3){ # case where there is not enough data
#         next
#       }else{
#           output <- get_jump_info(tmp)
#           res_slope <- rbind(res_slope, output$slope_info)
#           res_jumps <- rbind(res_jumps, output$jump_info)
#       }
#     }
#   }
# }

# write data because it's long
# vroom_write(res_slope, file = '/home/flo/dox/THESIS/DATA/ARGO/res_slope.csv')
# vroom_write(res_jumps, file = '/home/flo/dox/THESIS/DATA/ARGO/res_jumps.csv')
res_slope <- vroom::vroom(file = '/home/flo/dox/THESIS/DATA/ARGO/res_slope.csv')
res_jumps <- vroom::vroom(file = '/home/flo/dox/THESIS/DATA/ARGO/res_jumps.csv')
```

```{r, check total biovolume with slope value}

uvp_drifting <- all_floats |> filter(PhaseName == 'PAR') |> select(juld, cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> dplyr::mutate(date = as_date(juld))

# DSE for computing biovolumes
#DSE <- c(0.29042685,0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779, 1.16170742, 1.46365963)
DSE_long <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685, 0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
                1.16170742, 1.46365963, 1.84409558, 2.32341484)

# compute biovolume
get_biovolume <- function(DSE){
  r <- DSE/2
  biovolume <- (4*pi/3)*r**3
  return(biovolume)
}

# get biovolume for each DSE (equivalent to say for each size bin)
biovolumes <- map_dbl(DSE_long, get_biovolume)  # unit = mm^3

# compute biovolume and add them to the dataframe
spectra_biovolume <- tibble(as.data.frame(t(t(uvp_drifting[,all_of(lpm_class)])*biovolumes)))
colnames(spectra_biovolume) <- colnames(spectra_biovolume)|> str_replace('NP_Size', 'Biovolume')
biovolumes_names <- colnames(spectra_biovolume)
uvp_drifting <- cbind(uvp_drifting, spectra_biovolume) |> rowwise() |> dplyr::mutate(total_biovolume = sum(across(all_of(biovolumes_names))))

assign_time_group <- function(d_uvp, d_ost){
  d_uvp$group <- NA
  for (i in 1:nrow(d_ost)){
    low_time <- d_ost$min_time[i]
    high_time <- d_ost$max_time[i]
    #print(c(low_time, high_time))
    group <- d_ost$group[i]
    time_index <- which(d_uvp$juld >= low_time & d_uvp$juld <= high_time)
    d_uvp$group[time_index] <- group
  }
  return(d_uvp)
}

# loop for it
# start loop for all data
wmo <- c(4903634, 6904240, 6904241, 1902578)
park_depth <- c('200 m', '500 m', '1000 m')

# test <- data.frame()
# for (i in wmo){
#   max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
#   for (j in park_depth){
#     for (k in seq(1:max_cycle)){
#       tmp_uvp <- filter(uvp_drifting, wmo == i, cycle == k, park_depth == j) |> arrange(juld) |> select(juld, cycle, wmo, park_depth, total_biovolume) 
#       tmp_ost <- res_slope |> filter(wmo == i, cycle == k, park_depth == j)
#       if(nrow(tmp_uvp) == 0 | nrow(tmp_ost) == 0){
#         next
#       }else{
#       tmp <- assign_time_group(tmp_uvp, tmp_ost)
#       #tmp2 <- tmp |> filter(!is.na(group)) |> dplyr::group_by(group) |> dplyr::mutate(uvp_points = n(), min_date = min(juld), max_date = max(juld), mean_date = mean(floor_date(juld))) # https://stackoverflow.com/questions/44814435/calculate-average-daily-value-from-large-data-set-with-r-standard-format-date-ti
#       tmp2 <- tmp |> filter(!is.na(group)) |> group_by(group) |> dplyr::mutate(uvp_points = n(), mean_date = mean(floor_date(juld)))
#       tmp3 <- merge(tmp2, tmp_ost) |> dplyr::group_by(group, cycle, wmo, park_depth) |> dplyr::summarize(mean_date = unique(mean_date), uvp_points = unique(uvp_points), ost_points = unique(nb_points), group_total_biovolume = sum(total_biovolume), slope = unique(slope), weighted_group_biovolume = group_total_biovolume/uvp_points)
#       test <- rbind(test, tmp3)
#       }
#     }
#   }
# }

# write data because it's long
# vroom_write(test, file = '/home/flo/dox/THESIS/DATA/ARGO/slope_vs_biovolume.csv')

# load data computed with the above commented code
slope_vs_biovolume <- vroom::vroom('DATA/ARGO/slope_vs_biovolume.csv')
slope_vs_biovolume <- slope_vs_biovolume |> mutate(park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

# ggplot(test) + geom_line(aes(x = mean_date, y = weighted_group_biovolume)) + facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) +
# ggplot(test) + geom_line(aes(x = mean_date, y = slope)) + facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4)

# check
# test2 <- test |> group_by(wmo, cycle) |> dplyr::mutate(mean_mean_date = mean(floor_date(mean_date))) 
# 
# ggplot(test2) +
#   geom_pointrange(aes(x = mean_mean_date, y = weighted_group_biovolume),
#                       stat = 'summary',
#                       fun.min = function(z) {quantile(z, 0.25)},
#                       fun.max = function(z) {quantile(z, 0.75)},
#                       fun = median, colour = 'black', size = .1) +
#     facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) +
#   
# ggplot(test2) +
#   geom_pointrange(aes(x = mean_mean_date, y = slope),
#                       stat = 'summary',
#                       fun.min = function(z) {quantile(z, 0.25)},
#                       fun.max = function(z) {quantile(z, 0.75)},
#                       fun = median, colour = 'black', size = .1) +
#     facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) 
# 
# test2 |> group_by(wmo, park_depth) |> summarize(correl = cor(slope, weighted_group_biovolume))

# ggplot(slope_vs_biovolume) + geom_point(aes(x = log(slope), y = log(weighted_group_biovolume))) + facet_wrap(~park_depth) + labs(x = 'OST slope [UNITS]', y = 'UVP biovolume [UNITS]') #+ theme(axis.title.x = element_blank())
# #ggplot(test) + geom_point(aes(x = slope, y = weighted_group_biovolume)) + facet_wrap(~park_depth)
ggplot(slope_vs_biovolume |> filter(slope > 0.00001)) + geom_point(aes(x = slope*0.25, y = weighted_group_biovolume)) + facet_wrap(~park_depth, scales = 'free') + labs(x = TeX('$ATN~slope~(day^{-1})$'), y = TeX('$UVP~biovolume~(mm^{3})$')) +
  scale_x_continuous(trans = 'log10') + scale_y_continuous(trans = 'log10') + theme_bw()


# what about jump values with uvp measurements at jump
uvp_drifting <- uvp_drifting |> dplyr::mutate(num_date = as.numeric(juld))
res_jumps <- res_jumps |> dplyr::mutate(num_date = as.numeric(dates))

assign_uvp_to_jump <- function(d_uvp, d_jump){
  d_jump$biovolume <- NA
  for (i in 1:nrow(d_jump)){
    index_jump_for_uvp <- which.min(abs(tmp_uvp$num_date - tmp_ost_jump$num_date[i]))
    d_jump$biovolume[i] <- tmp_uvp$total_biovolume[index_jump_for_uvp]
  }
  return(d_jump)
}

# test_jump <- data.frame()
# for (i in wmo){
#   max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
#   for (j in park_depth){
#     for (k in seq(1:max_cycle)){
#       tmp_uvp <- filter(uvp_drifting, wmo == i, cycle == k, park_depth == j) |> arrange(juld) |> select(juld, cycle, wmo, park_depth, total_biovolume, num_date) 
#       tmp_ost_jump <- res_jumps |> filter(wmo == i, cycle == k, park_depth == j)
#       if(nrow(tmp_ost_jump) == 0 | nrow(tmp_uvp) == 0){
#         next
#       }else{
#         tmp <- assign_uvp_to_jump(tmp_uvp, tmp_ost_jump)
#       }
#       test_jump <- rbind(test_jump, tmp)
#     }
#   }
# }

#vroom_write(test_jump, file = '/home/flo/dox/THESIS/DATA/ARGO/diff_jump_vs_biovolume.csv')

# load data computed with the above commented code
diff_jump_vs_biovolume <- vroom::vroom('DATA/ARGO/diff_jump_vs_biovolume.csv')
diff_jump_vs_biovolume <- diff_jump_vs_biovolume|> mutate(park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

#ggplot(test_jump) + geom_point(aes(diff_jump, biovolume))
# ggplot(diff_jump_vs_biovolume) + geom_point(aes(log(diff_jump), log(biovolume))) + facet_wrap(~park_depth) + labs(x = 'OST jump difference [UNITS]', y = 'UVP biovolume [UNITS]') +
#   scale_x_continuous(breaks = c(-6,-4,-2,0))
ggplot(diff_jump_vs_biovolume) + geom_point(aes(x = diff_jump*0.25, y = biovolume)) + facet_wrap(~park_depth, scales = 'free') + labs(x = TeX('$ATN~jump~difference$'), y = TeX('$UVP~biovolume~(mm^{3})$')) +
  scale_x_continuous(trans = 'log10') + scale_y_continuous(trans = 'log10') + theme_bw()


```

TODO :

-   science: on voit le système latent, on a raté le bloom
-   rajouter les vitesses de sédimentations (n'importe quelle méthode fera l'affaire) -\> permet d'utiliser le graphique de l'UVP en vertical -\> hop une justification en plus
-   rajouter dans les méthodes les produits satellites qu'on utilise
-   différentes dynamiques post-bloom
-   phénologie du bloom printanier
-   qinking speed -\> trouver un truc à dire et estimer l'incertitude (en lien avec le graphique ou le DZ augmente) + expliquer la méthode vite fait (Léo Lacour, personal communication)
-   on se trouve dans un sytème producitve entre guidi 2008 et iversen (très productif)

### Sinking speed

-   Montrer que les sinking speed observées ne sont pas compatibles avec des valeurs de bloom (càd rapides pour les grosses particules à minima)
-   rajouter dans les méthodes le calcul de cette sinking speed.

```{r, vertical plots + sinking speed bis, fig.height=8, fig.width=8}
# 
# particles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class), MLD) |> drop_na(NP_Size_102) |> dplyr::mutate(juld = as_date(juld), doy = yday(juld), year = year(juld))
# 
# particles_bis <- particles |> pivot_longer(cols = all_of(lpm_class), names_to = 'size', values_to = 'concentration') |> dplyr::mutate(size = size |> str_remove('NP_Size_') |> as.numeric()) 
# #particles_bis$size <- factor(particles_bis$size, levels = lpm_class)
# 
# # remove buggy data from float 4903634 (negative depth for some reason)
# particles_bis <- particles_bis |> filter(depth >= 0)
# 
# # rel_conc
# particles_bis <- particles_bis |> dplyr::group_by(wmo, size) |> dplyr::mutate(rel_conc = (concentration-min(concentration))/(max(concentration) - min(concentration)))
# 
# up_part <-  particles_bis |> filter(depth < 2000, size < 512, doy < 220, year == 2022) |>
#   #group_by(size) |>
#   ggplot() + facet_grid(wmo~size) + theme_bw() +
#   geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
#   scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
#   scale_y_reverse() +
#   scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
#   labs(colour = '#/L', x = 'Month', y = 'Depth (m)') +
#   geom_pointrange(data = sinking_fit |> filter(size < 512, DOY < 220, standard_dev <= 30), aes(xmin = as_date(xmin, origin = '2022-01-01'), xmax = as_date(xmax, origin = '2022-01-01'), x = as_date(DOY, origin = '2022-01-01'), y = depth), size = .2, colour = 'black')
# 
# up_part
# 
# # down_part <- particles_bis |> filter(depth < 2000, size > 512) |>
# #   #group_by(size) |>
# #   ggplot() + facet_grid(wmo~size) + theme_bw() +
# #   geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
# #   scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
# #   scale_y_reverse() +
# #   scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
# #   labs(colour = '#/L', x = 'Month', y = 'Depth (m)') +
# #   geom_pointrange(data = sinking_fit |> filter(size > 512), aes(xmin = as_date(xmin, origin = '2022-01-01'), xmax = as_date(xmax, origin = '2022-01-01'), x = as_date(DOY, origin = '2022-01-01'), y = depth), size = .2, colour = 'white') 
# 
# down_part <-  particles_bis |> filter(depth < 2000, size > 512, doy < 220, year == 2022) |>
#   #group_by(size) |>
#   ggplot() + facet_grid(wmo~size) + theme_bw() +
#   geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
#   scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
#   scale_y_reverse() +
#   scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
#   labs(colour = '#/L', x = 'Month', y = 'Depth (m)') +
#   geom_pointrange(data = sinking_fit |> filter(size > 512, DOY < 220, standard_dev <= 30), aes(xmin = as_date(xmin, origin = '2022-01-01'), xmax = as_date(xmax, origin = '2022-01-01'), x = as_date(DOY, origin = '2022-01-01'), y = depth), size = .2, colour = 'black')
# 
# down_part

```

```{r, sinking speed plot}

# # NOTE : pas besoin d'updater ici vu que l'on regarde par rapport au 'bloom' de fin mai/juin
# sinking_fit <- vroom('DATA/ARGO/gaussian_fit_sinking_speed_full_before_06032023.csv') |> dplyr::mutate(size = size_class |> str_remove('NP_Size_') |> as.numeric(), date = as_date(date))
# 
# estimate_sinking_speed <- function(siz, data, threshold){
#   
#   # above 500 m
#   tmp <- data |> filter(size == siz) |> filter(depth <= threshold)
#   model <- lm(formula = depth ~ time_max_abundance, data = tmp)
#   tmp$sinking_speed_above_threshold <- model$coefficients[2] # equivalent to slope of the fit
#   tmp2 <- tmp
#   
#   # below 500 m
#   tmp <- data |> filter(size == siz) |> filter(depth > threshold)
#   model <- lm(formula = depth ~ time_max_abundance, data = tmp)
#   tmp2$sinking_speed_below_threshold <- model$coefficients[2] # equivalent to slope of the fit
#   
#   return(tmp2)
# }
# 
# # sinking speed for each float
# sinking_6904240 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 6904240), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
# sinking_6904241 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 6904241), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
# sinking_1902578 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 1902578), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
# sinking_4903634 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 4903634), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
# all_sinking_speed <- rbind(sinking_1902578, sinking_4903634, sinking_6904240, sinking_6904241)

```

```{r, sinking speed values}

# gt(all_sinking_speed)
# 
# all_sinking_speed |> ggplot() + geom_point(aes(y = sinking_speed_below_threshold, x = size))

```

```{r, check gaussian profiles}

# # depth layers for Gaussian fit
# depthL <- c(seq(40,200,20), c(250,300,400,500,600,800,1000,1250,1500,1750,2000))
# 
# # Gaussian formula #> but z is not the depth but the TIME
# fgauss <- function(z, peak_conc, peak_doy, dz){
#   peak_conc*exp(-((z-peak_doy)**2)/(dz**2))
# }
# 
# return_fitted <- function(d){
#   
#   #timeframe near the bloom so to say
#   ztime <- seq(160,240,1)
#   # gaussian fit
#   tib <- map_dfr(1:nrow(d), function(i){tibble(ztime = ztime, fgauss = fgauss(ztime, d$max_abundance[i], d$time_max_abundance[i], d$standard_dev[i]), depth = d$depth[i])})
#   return(tib)
# }
# 
# # test
# tmp <- sinking_fit |> filter(wmo == 6904240, size == 645)
# 
# test <- return_fitted(tmp)
# 
# ggplot(test, aes(x = ztime, y = fgauss, colour = factor(depth))) + geom_point() + 
#   scale_color_viridis_d() +
#   theme_bw() 
# 

```

### TS diagram

-   Pour le drift à 200, 500 et 1000 m -\> interpète ce graph -\> va checker les masses d'eau de la region... attention on est en PARKING DEPTH -\> mettre la couleur de la parking depth dans le truc, ça permettrait de voir si on peut comparer les processus (style ratio 500/1000m) dans les couches qui sont dans la meme masse d'eau.

```{r, TS diagram}
#ts_data <- all_floats |> filter(PhaseName == 'PAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth, lon, lat) |> drop_na(temp) |> filter(sigma > 26, park_depth != '200 m') |> dplyr::mutate(date = as_date(date))
# ts_data <- all_floats |> filter(PhaseName == 'PAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth, lon, lat) |> drop_na(temp) |> filter(sigma > 26) |> 
#   dplyr::mutate(date = as_date(date), colour = if_else(park_depth == '200 m', '#E31B23', if_else(park_depth == '500 m', '#FFC325', 'black')))

ts_data <- all_floats |> filter(PhaseName == 'NPAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth, lon, lat) |> drop_na(temp) |> filter(sigma > 26) |> 
  dplyr::mutate(date = as_date(date))

ts_data <- ts_data |> mutate(spiciness = swSpice(ts_data$psal, ts_data$temp, ts_data$depth, ts_data$lon, ts_data$lat))

# # https://dankelley.github.io/oce/reference/plotTS.html
# # https://dankelley.github.io/oce/reference/argo-class.html
# test <- as.argo(time = ts_data$date, longitude = ts_data$lon, latitude = ts_data$lat, salinity = ts_data$psal, temperature = ts_data$temp, pressure = ts_data$depth,
#         id = ts_data$wmo) 
# 
# plotTS(test, inSitu = T, type = 'p', xlab = 'Salinity [PSU]')
# 
# 
# ggplot(ts_data) + geom_point(aes(x = spiciness, y = depth)) + scale_y_reverse() + facet_wrap(~wmo)

a <- ggplot(ts_data |> filter(depth <= 1000)) + geom_point(aes(x=date, y=depth, colour=spiciness), size = 3, shape = 15) +
  scale_y_reverse() +
  #scale_color_viridis() +
  scale_color_cmocean(name = 'balance', limits = c(-0.25,0.25), oob = scales::squish, na.value = 'grey') +
  theme_bw() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = TeX('Spiciness (kg m$^{-3}$)'), x = 'Month', y = 'Depth (m)') +
  facet_wrap(~wmo, ncol = 4)

b <- particles_bis |> filter(depth <= 1000, size == 102) |>
  #group_by(size) |>
  ggplot() + facet_wrap(~wmo, ncol = 4) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=2, shape=15) +
  #scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_color_cmocean(name = 'thermal', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) +
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = 'Particles (#/L)', x = 'Month', y = 'Depth (m)') 

```

## Discussion2

PARLER DU TRADEOFF BETWEEN SMALL FLUX AND BIG PARTICLE FLUX

The latter are however subject to a tradeoff between the likely underestimation of $F_{small}$ and the detection of false jumps (without great impact on the value of $F_{big}$ because false jumps have low $c_{p}$ values) if the threshold is too small and the likely overestimation of $F_{small}$ to the detriment of $F_{big}$ with missed jumps if the threshold is too high.

parler aussi du fait que lesmesures en dérivés, il y a 2h en UVP et OST etc peut-etre que les mesures à 500 m ... ou alors on mets plus de temps à 200 m... voir un peu. faire des profils tous les 10 jours pour une telle apllication ça sert à rien étant donné qu'en plus si on a l'uvp 6 avec l ER embarquée pour éutider la vmmp il faut clairmeent avoir une résolution temporelle plus éleée, ce qui est loin parfait donc voilà.

TO DO - faire le profil de Martin (flux vertical) et faire les données de HPLC pour étudier les communautés et voir si elles sont différentes entre les deux sites de déploiements.

## Conclusion
