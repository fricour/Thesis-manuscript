# Case study in the Labrador Sea {#sec-case-study-labrador-sea}

In the first chapter, I explained the different steps that led to the creation of the embedded zooplankton classification model for the UVP6. However, I did not specify that in spite of the technical and energy constraints, the model was also due for April 2022 for a deployment of BGC-Argo floats in the Labrador Sea a month later. Once deployed, it was only a question of time before I could dig into the UVP6 and the OST data. In the meantime I asked myself, what could be a cool thing to do about those data? I already had some experience with BGC-Argo floats during my master's degree (I published a paper on the study of the deep chlorophyll maximum in the Black Sea, [DOI: 10.5194/bg-18-755-2021](https://bg.copernicus.org/articles/18/755/2021/)) and so while I was waiting for new profiles, I thought it would be useful to have a visualization tool in the form of a web app to investigate incoming data and already prepare the ground for the following study. Here is the link to the [web app](https://fricour.shinyapps.io/CARBON_REFINE/), which is still currently being automatically updated every day with new data (almost) live from the Labrador Sea and hopefully soon with other BGC-Argo floats equipped with the UVP6 and the OST to investigate carbon and particle fluxes in other regions. This study uses preliminary data (not even one year of data) and a new BGC-Argo float configuration, hence more work should be addressed to better understand the dynamics of the system.

## Introduction

The biological carbon pump (BCP) refers to a series of processes that transport biogenic carbon produced in sunlit surface waters to the ocean interior [@Claustre2021-tx]. It is composed of three main pathways [@Claustre2021-tx; @Boyd2019-ez; @Siegel2022-wd]: the biological gravitational pump that is the gravitational settling of particulate organic carbon (POC); the physically mediated particle injection pump that represents physical mechanisms that transport both POC and dissolved organic carbon (DOC) to depth via mixing processes; and the biologically mediated particle injection pump that characterizes the transport of POC by diel and seasonal migration of animals in the water column. Historically, the BCP was mainly studied with sediment traps [@Honjo2008-kn] and Thorium-234 radionuclide disequilibria [@Buesseler2007-ql] focused on the gravitational pump hence the contribution of DOC and migrating animals to the BCP was poorly known. However, the advent of autonomous underwater vehicles (AUVs) equipped with a variety of physical and biogeochemical sensors have demonstrated their ability to better understand them [e.g. @Omand2015-mf; @DallOlmo2016-ex; @Llort2018-pa; @Lacour2019-id; @Haentjens2020-dc; @Stukel2017-lc]. Still, we are missing processes to close the carbon budget [@Giering2014-tj; @Boyd2019-ez] and unknowns remain in the under-sampled mesopelagic (i.e. twilight) zone [@Martin2020-wa]. In order to fill the observational gaps, the Biogeochemical-Argo (BGC-Argo) network aims to develop an operational global array of 1000 profiling floats measuring six core variables from the surface to 2000 m [@Claustre2020-jo]: oxygen, nitrate, pH, chlorophyll a (Chla), suspended particles and downwelling irradiance. In addition to the six core variables, new sensors have been developed to target specific BCP-related questions [@Claustre2020-jo; @Claustre2021-tx; @Chai2020-uq]. In this study, we will focus on BGC-Argo floats equipped with two additional sensors: the Underwater Vision Profiler (UVP6) [@Picheral2021-sx] and a transmissometer used as an optical sediment trap (OST).

The UVP6 is a portable version of the UVP5 [@Picheral2010-to], specifically designed for AUVs [@Picheral2021-sx]. It consists of an underwater camera that counts particles and takes images of objects in, respectively, the 0.1 - 2.5 mm and 0.6 - 2.5 cm size range. In contrast with UVP5 deployments on cabled platforms, UVP6 deployed on long-term drifting AUVs (e.g. BGC-Argo floats) are rarely recovered. Therefore, the latest version of the UVP6 also embarks an embedded image classification algorithm that transmits the abundance of 20 classes (16 biological classes) in near-real time. The OST consists of a vertically mounted transmissometer whose upward-facing optical window intercepts sinking particles to estimate the downward POC flux. The latter have already been used on AUVs [@Bishop2004-at; @Bishop2009-ka; @Estapa2013-gw; @Estapa2017-jr; @Estapa2019-nk] and have demonstrated their usefulness to characterize the gravitational pump.

For the first time, BGC-Argo floats equipped with both the UVP6 and an OST were deployed in the Labrador Sea (LS) to investigate the seasonal and episodic dynamics of particles and carbon fluxes. The LS is a basin of the subpolar North Atlantic (NA) ocean which represents 20% of the global ocean carbon sink [@Takahashi2009-rv]. The efficiency of this sink is explained by both deep winter convection that traps CO$_{2}$ from the atmosphere [@Vage2008-qa; @Yashayaev2009-fe; @Yashayaev2017-mn] and by the largest spring phytoplankton bloom [@Siegel2002-hk] that consumes CO$_{2}$ for the photosynthetic production of organic matter. The triggering mechanism of the NA bloom was initially explained by the 'Critical Depth Hypothesis' of @Sverdrup1953-ki that defines a critical surface mixing depth where phytoplankton growth is equal to phytoplankton losses. According to @Sverdrup1953-ki, a bloom is therefore triggered when the surface mixed layer depth (MLD) is shallower than this critical depth resulting in a positive phytoplankton growth rate. However, this hypothesis was challenged by @Behrenfeld2010-my who suggested that the initiation of the NA bloom begins in winter with a slow phytoplankton biomass increase over the following months (i.e. 'Dilution-Recoupling' hypothesis). These hypotheses have been investigated with BGC-Argo floats but the debate still remains [@Mignot2018-gm; @Yang2020-hw; @Boss2010-do]. Distinct NA bloom phenologies were highlighted by @Lacour2015-da who used a climatology of satellite Chla to divide the NA into 6 bioregions. In their bioregionalization (see also their Figure 2), the LS is characterized by two bioregions with distinct bloom phenologies on both side of the 60°N parallel. The northern part is characterized by a short and potentially more intense spring bloom in late April whereas the bloom in the southern part starts in April-May and slowly develops to reach its maximum in June. @Lacour2015-da concluded that the concurrent influence of surface light and mixing was the first order mechanism controlling the onset of the spring bloom in both bioregions. This is coherent with the results of @Wu2008-vw and @Frajka-Williams2010-ph who found that the early bloom in the northern part is due to a very shallow mixed layer associated with low-salinity waters (i.e. haline stratification) while the southern bloom is related to the development of the seasonal thermocline (i.e. thermal stratification due to increased solar radiation). In late May 2022, two pairs of BGC-Argo floats were deployed in the southern part of the LS at the time of the spring bloom. While BGC-Argo floats typically drift at 1000 m for 10 days between each ascending profile, all floats were configured to drift at three distinct parking depths (200 m, 500 m and 1000 m) to study the downward POC flux and its attenuation derived from beam attenuance [@Estapa2013-gw; @Estapa2019-nk] and to study particle size distributions (PSDs) and concentrations with the UVP6.

The goals of this study are threefold: 1) the comparison of the UVP6 and the OST as two independent measurements of downward carbon fluxes and particle abundance on BGC-Argo floats; 2) the exploration of the seasonality (bloom and non-bloom periods) of downward POC fluxes and particle abundance in the LS; 3) the evaluation of the new drifting configuration and how it could be improved to study specific BCP-related questions.

## Methods

### Deployment and floats configuration

Two pairs of BGC-Argo floats were deployed in the Labrador Sea in late May 2022 (@fig-float-position). The first pair (WMO 6904240 and 4903634) was deployed on the 22$^{nd}$ at coordinates (-49.3°E, 58.9°N) while the second pair (WMO 6904241 and 1902578) was deployed on the 29$^{th}$ at coordinates (-52.4°E, 56.8°N).

All 4 floats used in this study are PROVOR Jumbo floats (NKE Instrumentation, France) equipped with a CTD SBE41-CP (Sea-Bird Scientific, USA), a UVP6-LP (Hydroptic, France), a c-Rover 660 nm beam transmissometer (WETLabs, USA) and a ECO FLBBCD (WETLabs, USA) that includes a Chla fluorometer (excitation at 470 nm, emission at 695 nm), a colored dissolved organic matter (CDOM) fluorometer (excitation at 370 nm, emission at 460 nm) and a 700 nm particle backscatteringmeter ($b_{bp}$). <!-- All floats are equipped with a OCR-504 Multispectral Radiometer (Sea-Bird Scientific, USA) except float 1902578 that is equipped with a RAMSES hyperspectral radiometer (TriOS, Germany) to measure the downwelling irradiance at three wavelengths (380, 412, 490 nm) and of the light available for photosynthesis (PAR). -->

All 4 floats have been configured to drift at 3 distinct parking depths (respectively, 200 m, 500 m and 1000 m for approximately 1, 3 and 5 days) to study the downward carbon flux and its attenuation. During these drift phases, measurements of particle abundance (14 size classes, between 0.1 mm and 2.5 mm) are taken every 20 minutes at 200 m and every 2h at 500 m and 1000 m whereas $c_{p}$ measurements are taken every 30 minutes at all depths. This configuration can be adjusted during the mission of the float via two-way Iridium communication. Finally, the vertical orientation of the transmissometer causes the accumulation of sinking material on the optical window. Therefore, a dedicated pump is used to clean the latter to remove the accumulated material between each drifting phase and subsequently avoid sensor drift.

```{r, load libraries}
library(tidyverse)
library(akima)
library(broom)
library(cmocean)
library(lubridate)
library(scales)
library(patchwork)
library(viridis)
library(ggridges)
library(ggvis)
library(pracma)
library(gbm)
library(MLmetrics)
library(purrr)
library(arrow)
library(plotly)
library(oce)
library(ggplot2)
library(latex2exp)
library(cowplot)
library(RColorBrewer)
library(vroom)
library(castr)
library(reactablefmtr)
library(gt)
#library(gtExtras)
library(spatialrisk)
library(ggOceanMaps)
```

```{r, map of 4 floats, fig.height=5, fig.width=6}
#| label: fig-float-position
#| fig-cap: Trajectory of each float since their deployment (black triangles). Dots represent ascending profiles. The profile that concludes the time series for each float is highlighted by a black contour.

# read floats data
float_6904240 <- vroom('/home/flo/dox/ArgoShine/SAVE_FOR_MANUSCRIPT/6904240_FromNetCDF.csv')
float_6904241 <- vroom('/home/flo/dox/ArgoShine/SAVE_FOR_MANUSCRIPT/6904241_FromNetCDF.csv')
float_4903634 <- vroom('/home/flo/dox/ArgoShine/SAVE_FOR_MANUSCRIPT/4903634_FromNetCDF.csv')
float_1902578 <- vroom('/home/flo/dox/ArgoShine/SAVE_FOR_MANUSCRIPT/1902578_FromNetCDF.csv')

# float_6904240 <- vroom('/home/flo/dox/ArgoShine/6904240_FromNetCDF.csv')
# float_6904241 <- vroom('/home/flo/dox/ArgoShine/6904241_FromNetCDF.csv')
# float_4903634 <- vroom('/home/flo/dox/ArgoShine/4903634_FromNetCDF.csv')
# float_1902578 <- vroom('/home/flo/dox/ArgoShine/1902578_FromNetCDF.csv')

# combine all data
all_floats <- rbind(float_6904240, float_6904241, float_4903634, float_1902578)

# add levels for some field
all_floats <- all_floats |> dplyr::mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)), park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

# summarize localization of floats
argo_map <- all_floats |> select(WMO = wmo, cycle, lat, lon) |> drop_na(lat) |> dplyr::group_by(WMO, cycle) |> dplyr::mutate(WMO = factor(WMO)) |>
  dplyr::summarise(lat = mean(lat), lon = mean(lon))

argo_map_first <- argo_map |> filter(cycle == 1)
argo_map_last <- argo_map |> filter(cycle == max(cycle))

# prepare data for geomapping, see https://mikkovihtakari.github.io/ggOceanMaps/
argo_map_geo <- transform_coord(argo_map) |> dplyr::mutate(cycle = argo_map$cycle, WMO = as.factor(argo_map$WMO))
argo_map_last_geo <- transform_coord(argo_map_last) |> mutate(WMO = as.factor(argo_map_last$WMO))

p <- basemap(limits = c(-55,-40,55,65), bathymetry = T, lat.interval = 2.5, bathy.style = 'poly_blues', bathy.alpha = .5) 

# Make the graticules:
lims <- attributes(p)$limits 
graticule <- sf::st_graticule(
  c(lims[1], lims[3], lims[2], lims[4]), 
  crs = attributes(p)$proj,
  lon = attributes(p)$map.grid$lon.breaks, 
  lat = attributes(p)$map.grid$lat.breaks
)

p <- p + 
  geom_sf(data = graticule, color = "grey", size = LS(1)) + # graticules
  coord_sf(xlim = lims[1:2], ylim = lims[3:4], # redefine limits
           crs = attributes(p)$proj) 

p +
  geom_path(data = argo_map_geo, aes(lon, lat, colour = WMO), linewidth = 1) + 
  geom_point(data = argo_map_geo, aes(x= lon, y = lat, colour = WMO), size = 2) +
  #scale_color_brewer(palette = 'Dark2') + 
  theme_bw() +
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  geom_point(data = transform_coord(argo_map_first), aes(lon, lat), shape = 17, size = 3) +
  geom_point(data = argo_map_last_geo, aes(lon, lat, colour = WMO), size = 4) +
  geom_point(data = argo_map_last_geo, aes(lon, lat), fill = 'black', size = 4, shape = 1) +
  #https://aosmith.rbind.io/2020/07/09/ggplot2-override-aes/#suppress-aesthetics-from-part-of-the-legend
  guides(color = guide_legend(override.aes = list(size = c(1.5,1.5,1.5,1.5),
                                                  shape = c(NA, NA, NA, NA)))) +
  annotation_scale(location = "br") + 
  annotation_north_arrow(location = "tr", which_north = "true") 
```

```{=latex}
\newpage
```
### Data processing

Data were downloaded from the Coriolis data center (*ftp://ftp.ifremer.fr/ifremer/argo/*, with biogeochemical data at *dac/coriolis/* and UVP6 data at *aux/coriolis/*, last accessed: March 6, 2023). Chla data were quality controlled for Non-Photochemical Quenching (NPQ), a photoprotective mechanism that decreases the apparent fluorescence per unit of Chla following the method implemented by @Terrats2020-ef who modified the method of @Xing2018-pn for shallow-mixing cases. Chla data were also divided by a slope factor of 2 following the recommendation of @Roesler2017-me for obtaining unbiased Chla estimates from in situ WETLabs ECO fluorometers. Temperature and salinity data were quality controlled using @Wong2022-hz. The potential density anomaly was computed with the function swSigma0 of the oce R package [@Kelley2021-sw] and the MLD was determined using the 0.03 kg m$^{-3}$ density criterion at a reference depth of 10 m [@De_Boyer_Montegut2004-hh]. $c_{p}$ data were converted from counts to physical units (m$^{-1}$) using the following relationship

$$
c_{p}~[m^{-1}] = -\frac{1}{x}~log \left(~ \frac{c_{p}~[counts]-CSC_{dark}}{CSC_{cal}-CSC_{dark}} \right)
$$

where $x$, $CSC_{dark}$ and $CSC_{cal}$ are respectively the transmissometer pathlength (0.25 m), the sensor output when the beam is blocked (i.e. offset, 0 for each c-Rover of this study) and the sensor output with clean water in the path (12998, 13598, 13115 and 12855 for, respectively, float 4903634, 6904240, 6904241 and 1902578). The latter were obtained from the calibration sheets of each c-Rover. In practice, this calibration is not necessary to use the transmissometer in OST mode for the estimation of small and large sinking particles (see next section).

UVP6 data and in particular the averaged number of particles per size class (i.e. PSD) was divided by the number of images at each depth and the water volume imaged by the UVP (0.7 L) to obtain particle concentrations (units: #/L). Note that in an impending update by the Coriolis data center, it will no longer be necessary to divide by the number of images.

All data were divided into vertical profiles and drifting data using time stamps for profiling and drifting acquisition periods. Finally, data were checked for instrumental anomalies. As a result, $c_{p}$ values from float 6904241 cannot be used from its 20$^{th}$ cycle onwards (\> November 17) due to a likely failure of the pump cleaning the optical window of the transmissometer. In total, the data coverage of this study spans a period of 9 months (end of May 2022 to early March 2023).

### Derivation of carbon fluxes from OST

We applied a similar approach to [@Estapa2013-gw; -@Estapa2017-jr] to derive the continuous component (i.e. continuous deposition of small particles on the upward-facing window of the transmissometer) and discontinuous (i.e. deposition of stochastic large particles) components of the downward POC flux, hereafter referred to as, respectively, $F_{small}$ and $F_{large}$.

First, the drifting $c_{p}$ signal was cleaned from spikes (assumed as transient particles not landing on the window or active swimmers [@Estapa2013-gw]). For this, we computed a 7-point moving median and a 7-point moving median absolute deviation (MAD). Then, we computed the anomaly to the moving median as the difference between the drifting $c_{p}$ signal and the moving median. We considered that a $c_{p}$ measurement was a spike when the anomaly was above $5.2 \times MAD$ following @Leys2013-eu. Despiked $c_{p}$ data were then smoothed with a 7-point moving median filter.

```{r, example of a cp signal analyzed for jumps (with an old method here), fig.width=8, fig.height=5}
#| label: fig-estapa-method_article
#| fig-cap: Example of the jump detection method on a drifting transmissometer where the $c_{p}$ signal is divided into a continuous (dark blue) component and a discontinuous (positive jumps, red) component. Slopes for each segment are depicted in light blue. A negative jump was also detected (yellow). Data from BGC-Argo float WMO 6904240, drifting at 1000 m during its 21$^{st}$ cycle.

clean_cp_data <- function(data, moving_median_order){
  
  data <- data |> arrange(dates)
  
  if(nrow(data) == 0){ # no cp data
    return(data)
  }else{
    
    # # Remove cp > 2 m-1 (don't really know why but eh)
    # if (any(data$cp > 2)){
    #   data <- data[-which(data$cp > 2),]
    # }
    
    # despike cp data
    data$cp <- oce::despike(data$cp, reference = 'median', k = 7, replace = 'NA')
    
    # median average
    data$cp <- pastecs::decmedian(data$cp, order = moving_median_order, ends = "fill") |> pastecs::extract(component = "filtered")
  }
  return(data)
  
}

tmp <- all_floats |> filter(wmo == 6904240, park_depth == '1000 m', cycle == 21, PhaseName == 'PAR') |> drop_na(cp) |> select(dates = juld, everything())

threshold <- 0.08

clean_data <- clean_cp_data(tmp, 3)

# save cleaned Cp signal from spikes to a temporary variable
tmp2 <- clean_data
tmp2$jump <- NA
# compute slope between two adjacent points (except first and last point of drifting time series)
for (i in 2:(nrow(tmp2)-1)){
  #print(i)
  delta_x <- as.numeric(difftime(tmp2$dates[i], tmp2$dates[i-1], units = 'days'))
  delta_y <- tmp2$cp[i]-tmp2$cp[i-1]
  tmp2$jump[i] <- delta_y/delta_x
}
  
# assign a colour for the plot
tmp2$colour <- 'base signal'
tmp2$colour[which(abs(tmp2$jump)>threshold)] <- 'jump' # abs is needed for 'negative jumps'
  
# add group to compute slope (for each subgroups of points, separated from a jump)
tmp2$group <- NA
  
# compute where the jumps are (index value of the array)
jump_index <- which(tmp2$colour == 'jump')
  
# assign group 
for (i in jump_index){
  for (j in 1:nrow(tmp2)){
    if ((j < i) & (is.na(tmp2$group[j]))){
      tmp2$group[j] <- paste0('group_',i)
    }
  }
}
  
# add last group (because it is AFTER the last index so it was not computed before)
tmp2$group[which(is.na(tmp2$group))] <- 'last_group'
  
# compute slope for each subgroups
slope_df <- tmp2 |> filter(colour == 'base signal', jump != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x)
  
# remove negative slope from the mean slope (no physical meaning)
slope_df <- slope_df |> filter(slope > 0)
  
# remove if only one point (cannot fit a slope with one point)
slope_df <- slope_df |> filter(nb_points > 3)
  
# compute mean slope 
#mean_slope <- mean(slope_df$slope)
mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
# estapa relationship (see POSTER)
#poc_flux <- 633*((mean_slope*0.25)**0.77) # *0.25 because ATN = cp*0.25
poc_flux <- 633*((mean_slope)**0.77) # /!\ slope for computed for ATN on y axis (delta_y *0.25 because ATN  = cp*0.25) -> should be OK
  
# build dataframe to plot each subgroup
part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
part_slope <- rbind(part1, part2)
  
# spot negative jump
tmp2$colour[which((tmp2$colour == 'jump') & (tmp2$jump < 0))]  <- 'negative jump'
  
# add large particles flux
rows_to_keep <- c(jump_index, jump_index-1) 
tmp3 <- tmp2[rows_to_keep,] |> select(dates, cp, jump, colour, group) |> dplyr::arrange(dates)
  
# remove negative jump, if any
check_colour <- unique(tmp3$colour)
if(length(check_colour) == 3){
  index_neg_jump <- which(tmp3$colour == 'negative jump')
  tmp3 <- tmp3[-c(index_neg_jump, index_neg_jump+1),]
  tmp3 <- tmp3 |> dplyr::mutate(diff_jump = cp - lag(cp))
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else if(length(check_colour) == 2){
  tmp3 <- tmp3 |> dplyr::mutate(diff_jump = cp - lag(cp))
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else{ # NO jump
  tmp3 <- NULL
  }
  
# large particles flux
if(is.null(tmp3)){ # no jumps
  large_part_poc_flux <- 0
}else{
  tmp4 <- tmp3 |> filter(diff_jump > 0)
  if(nrow(tmp4) == 0){ # no positive jumps
    large_part_poc_flux <- 0
  }else{
    delta_y <- sum(tmp4$diff_jump) *0.25 # to get ATN (= cp*0.25)
    max_time <- max(tmp2$dates)
    min_time <- min(tmp2$dates)
    delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
    slope_large_part <- delta_y/delta_x
    large_part_poc_flux <- 633*(slope_large_part**0.77)
  }
}
  
# compute total drifting time
max_time <- max(tmp2$dates)
min_time <- min(tmp2$dates)
drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))

check <- tmp2 |> select(dates, cp, colour)
check <- check |> filter(colour %in% c('jump', 'negative jump'))

ggplot(tmp2, aes(x = dates, y = cp, colour = colour)) + geom_point(size = 3) +
    scale_colour_manual(values = c('#003366','#E31B23', '#FFC325')) + 
    #scale_shape_manual(values = c(1,1,4)) +
    geom_path(data = part_slope, aes(x = time, y = cp, group = group), colour = '#DCEEF3', linewidth = 1) + theme_bw() + 
    labs(x = 'Time', y = TeX('$c_{p}~(m^{-1})$')) +
  geom_point(data = check, aes(x = dates, y = cp, colour = colour), size = 3) +
  geom_point(data = check, aes(x = dates, y = cp), shape = 1, size = 3, colour = 'black')  +
    theme(legend.position = 'none') + scale_x_datetime(breaks = c(as.POSIXct('2022-11-16'), as.POSIXct('2022-11-18')), date_labels = 'Nov %d')
```

Contrarily to previous works [@Estapa2013-gw; @Estapa2017-jr; @Estapa2019-nk], we chose an empirical statistical approach instead of thresholds to detect the deposition of large particles (hereafter referred to as jumps) in the $c_{p}$ signal. Therefore, we firstly computed the slope between each $c_{p}$ observation for each drifting phase. The detection of jumps consisted in spotting outliers in the slope distribution. For this, we computed the interquartile range ($IQR$) between the first ($Q_{1}$) and third ($Q_{3}$) quartiles. Positive and negative jumps were defined as, respectively, a slope above $Q_{3} + 1.5 \times IQR$ and below $Q_{1} - 1.5 \times IQR$. We found that this method better detected outliers than the one used to despike the $c_{p}$ signal.

In order to compute $F_{small}$, we firstly computed the slope of each segment between jumps (positive and/or negative, see @fig-estapa-method_article). Any segment with less than 4 observations or a negative slope was removed assuming that either the so-called fit failed (leading to undetected jumps) either the transmissometer window was perturbed by factors other than the pump cleaning the optical window (e.g. turbulence or zooplankton consumption of accumulated material). Afterwards, we computed the weighted average slope ($slope_{w}$) of each segment to derive the small particle flux using the linear relationship between the beam attenuance flux and the POC flux (see first panel of Figure 2 in @Estapa2023-oz).

$$
F_{small} = 633(0.25~slope_{w})^{0.77}
$$ {#eq-estapa}

where $slope_{w}$ is multiplied by the transmissometer pathlength because @eq-estapa is an empirical POC-to-attenuance (ATN) relationship where ATN is the product of $c_{p}$ and the transmissometer pathlength.

We used the same relationship to compute $F_{large}$ by replacing $slope_{w}$ as the sum of positive jumps divided by the drifting time.

### Satellite data

Satellite Chla data were downloaded from a daily-merged cloud free GlobColour product (doi: 10.48670/moi-00281) with a spatial resolution of 4 km. MLD data were downloaded from near-real time weekly data from the ARMOR3D product (doi: 10.48670/moi-00052) with a spatial resolution of 0.25°.

```{=latex}
\newpage
```
## Results

### UVP6 particle abundance

#### On profiling mode

Vertical profiles of particle concentrations in the lower (102 - 512 $\mu$m, see @fig-uvp6-profiling-102-406) and upper (0.512 - 2.5 mm, see @fig-uvp6-profiling-512-2500) size range of the UVP6 show similar patterns across all floats. In the lower part of the size range, @fig-uvp6-profiling-102-406 shows the rapid emptying of the water column in November. This process occurred on the entire water column (e.g. size class 406, float 4903634) but also started with more intensity from below the surface (e.g. size class 102, float 6904240). All floats also measured a local particle abundance maximum around 800 - 1000 m whose origin is analyzed in the Discussion. In the upper part of the size range, @fig-uvp6-profiling-512-2500 shows a rapid emptying of the entire water column starting in November. In contrast with small classes (referred to here as below 512 $\mu$m), some particles, especially above 1.5 mm, have almost entirely disappeared from the water column by September. However, the rapid increase and decrease of particles on the entire water column around July-August for floats 6904241 and 1902578 (still in the same geographical zone at that time) could suggest an export event.

```{r, UVP6 profiling mode without sinking speed small classes, fig.height=8, fig.width=8}
#| label: fig-uvp6-profiling-102-406
#| fig-cap: Vertical profiles of UVP6 particle concentrations for size classes in the 102 - 512 $\mu$m size range. Concentrations were normalized for each size class. The colorbar is in log$_{10}$ scale.


lpm_class <- c("NP_Size_102", "NP_Size_128", "NP_Size_161",
                         "NP_Size_203", "NP_Size_256", "NP_Size_323",
                         "NP_Size_406", "NP_Size_512", "NP_Size_645",
                         "NP_Size_813", "NP_Size_1020", "NP_Size_1290",
                        "NP_Size_1630", "NP_Size_2050")


particles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class), MLD) |> drop_na(NP_Size_102) |> dplyr::mutate(juld = as_date(juld))

particles_bis <- particles |> pivot_longer(cols = all_of(lpm_class), names_to = 'size', values_to = 'concentration') |> dplyr::mutate(size = size |> str_remove('NP_Size_') |> as.numeric()) 

# remove buggy data from float 4903634 (negative depth for some reason)
particles_bis <- particles_bis |> filter(depth >= 0)

# rel_conc
particles_bis <- particles_bis |> dplyr::group_by(wmo, size) |> dplyr::mutate(rel_conc = (concentration-min(concentration))/(max(concentration) - min(concentration)))

particles_bis <- particles_bis |> mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)))

facet_names_up <- c(`102` = '102 - 128 µm',
                 `128` = '128 - 161 µm',
                 `161` = '161 - 203 µm',
                 `203` = '203 - 256 µm',
                 `256` = '256 - 323 µm',
                 `323` = '323 - 406 µm',
                 `406` = '406 - 512 µm',
                 `6904240` = '6904240',
                 `6904241` = '6904241',
                 `4903634` = '4903634',
                 `1902578` = '1902578')

up_part <- particles_bis |> filter(depth < 2000, size < 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size, labeller = as_labeller(facet_names_up)) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)') 

up_part
```

```{r, UVP6 profiling mode without sinking speed large classes, fig.height=8, fig.width=8}
#| label: fig-uvp6-profiling-512-2500
#| fig-cap: Vertical profiles of UVP6 particle concentrations for size classes in the 0.512 - 2.5 mm size range. Concentrations were normalized for each size class. The colorbar is in log$_{10}$ scale. The gray background depicts the absence of particle in the specified size class.

facet_names_down <- c(`512` = '512 - 645 µm',
                 `645` = '645 - 813 µm',
                 `813` = '0.81 - 1.02 mm',
                 `1020` = '1.02 - 1.29 mm',
                 `1290` = '1.29 - 1.63 mm',
                 `1630` = '1.63 - 2.05 mm',
                 `2050` = '2.05 - 2.50 mm',
                 `6904240` = '6904240',
                 `6904241` = '6904241',
                 `4903634` = '4903634',
                 `1902578` = '1902578')

down_part <- particles_bis |> filter(depth < 2000, size >= 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size, labeller = as_labeller(facet_names_down)) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_y_reverse() +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)') +
  #geom_vline(xintercept = as_date('2022-01-01')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') 

down_part
```

```{=latex}
\newpage
```
#### On drifting mode

```{r, uvp data}


uvp_data <- all_floats |> filter(PhaseName == 'PAR') |> dplyr::mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date, cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> dplyr::mutate(juld = as_date(juld), WMO = factor(wmo))

uvp_data <- uvp_data |> pivot_longer(cols = lpm_class, names_to = 'size', values_to = 'conc') |> dplyr::mutate(size = size |> str_remove('NP_Size_') |> as.numeric())

# compute daily mean chla for each float at each cycle and at each drifting depth
mean_uvp_data <- uvp_data |> dplyr::group_by(cycle, WMO, size, park_depth, juld) |> dplyr::summarize(mean_conc = mean(conc, na.rm=T))
```

In this study, we focus on 14 size classes, from 102 $\mu$m to 2.5 mm. In @fig-uvp-200m, @fig-uvp-500m and @fig-uvp-1000m, we show the daily averaged concentrations of particles in each size class for each float at their respective drifting depth. At 200 m, it can be observed that particle concentrations in the 102 - 512 $\mu$m range are relatively uniform between June and October before decreasing sharply towards January. Between 512 $\mu$m and 1.63 mm, concentrations are steadily decreasing from June. Above 1.63 mm, the particle abundance is already very low (\< 0.1 particle/L) in June but keeps decreasing until September when they are practically absent (\< 0.01 particle/L). At 500 m, particle concentrations in the 102 - 645 $\mu$m range slowly decrease from July to September with a sharp decrease starting in October. Above 645 $\mu$m, the trend shows a maximum between the end of July and the first half of August though the concentrations of particles in those classes are very low (\< 0.5 particle/L) on the entire time series. At 1000 m, all 4 floats measured a clear maximum in particle abundance in the 102 - 645 $\mu$m in the second half of July. Above 645 $\mu$m, the same trend can still be observed with concentrations of less than 1 particle/L.

```{r, uvp data at 200 m, fig.height=8, fig.width=8}
#| label: fig-uvp-200m
#| fig-cap: Daily averaged concentrations of particle abundance in 14 size classes measured by the UVP6 for each float at 200 m. Black lines and shaded areas represent, respectively, the daily mean particle abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particle abundance is maximum.

facet_all <- c(`102` = '102 - 128 µm',
                 `128` = '128 - 161 µm',
                 `161` = '161 - 203 µm',
                 `203` = '203 - 256 µm',
                 `256` = '256 - 323 µm',
                 `323` = '323 - 406 µm',
                 `406` = '406 - 512 µm',
                 `512` = '512 - 645 µm',
                 `645` = '645 - 813 µm',
                 `813` = '0.81 - 1.02 mm',
                 `1020` = '1.02 - 1.29 mm',
                 `1290` = '1.29 - 1.63 mm',
                 `1630` = '1.63 - 2.05 mm',
                 `2050` = '2.05 - 2.50 mm')

# add blank
blank_data <- tibble(cycle = c(NA, NA), WMO = rep(as.factor(4903634),2), size = c(102,102), park_depth = c('200 m', '500 m'),
                     juld = rep(as.Date('2023-03-05'),2), mean_conc = c(NA,NA))

mean_uvp_data <- rbind(mean_uvp_data, blank_data)

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particle abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y', labeller = as_labeller(facet_all)) #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp200m <- ggplot_build(p200m)

test <- tmp200m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> dplyr::summarize(index_max = which.max(y), max_date = x[index_max]) |> dplyr::mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particle abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y', labeller = as_labeller(facet_all)) +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p200m
```

```{r, uvp data at 500 m, fig.width=8, fig.height=8}
#| label: fig-uvp-500m
#| fig-cap: Daily averaged concentrations of particle abundance in 14 size classes measured by the UVP6 for each float at 500 m. Black lines and shaded areas represent, respectively, the daily mean particle abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particle abundance is maximum.

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particle abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y', labeller = as_labeller(facet_all)) 
  #geom_vline(data = test, aes(xintercept = date_max))

tmp500m <- ggplot_build(p500m)

test <- tmp500m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> dplyr::summarize(index_max = which.max(y), max_date = x[index_max]) |> dplyr::mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particle abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y', labeller = as_labeller(facet_all)) +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p500m
```

```{r, uvp data at 1000 m, fig.width=8, fig.height=8}
#| label: fig-uvp-1000m
#| fig-cap: Daily averaged concentrations of particle abundance in 14 size classes measured by the UVP6 for each float at 1000 m. Black lines and shaded areas represent, respectively, the daily mean particle abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particle abundance is maximum.
#| fig-dpi: 96

#{r, uvp data at 1000 m, out.width="900px", out.height="800px"}

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particle abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y', labeller = as_labeller(facet_all)) #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp1000m <- ggplot_build(p1000m)

test <- tmp1000m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> dplyr::summarize(index_max = which.max(y), max_date = x[index_max]) |> dplyr::mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particle abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y', labeller = as_labeller(facet_all)) +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p1000m
```

```{=latex}
\newpage
```
### OST-derived carbon fluxes

```{r, function to detect and plot jumps}

plot_jumps <- function(data){
  
  # make sure that the cp signal is in chronological order
  tmp <- data |> arrange(dates)
  
  # despike cp data with a 7-point moving window
  tmp$cp <- despike(tmp$cp, k = 3)
  
  # smooth cp data with a 3-point moving median, n time(s)
  tmp$cp <- slide(tmp$cp, fun = median, k = 3, n = 1, na.rm=T)
  
  # compute slope between two adjacent points (except first point) # we could start after 1h to let the float stabilize
  delta_x <- as.numeric(tmp$dates - lag(tmp$dates), units = 'days')
  delta_y <- tmp$cp - lag(tmp$cp)
  tmp$slope <- delta_y/delta_x
  
  # compute a Z score (assuming a normal distribution of the slopes) on the slopes
  tmp <- tmp |> dplyr::mutate(zscore = (slope - mean(slope, na.rm = T))/sd(slope, na.rm = T))
  
  # spot outliers on the Z score signal
  # interquartile range between Q25 and Q75 -> had to used that and not the despike function because slopes are often close (or equal) to 0 so it can miss clear jumps. Q25 and Q75 are more trustworthy in this case than the despike function of Jean-Olivier (see package castr on his github: https://github.com/jiho/castr)
  IQR <- quantile(tmp$zscore, probs = 0.75, na.rm=T) - quantile(tmp$zscore, probs = 0.25, na.rm=T)
  # outliers ('spikes' in the Z score signal)
  spikes_down <- tmp$zscore < quantile(tmp$zscore, 0.25, na.rm=T) - 1.5 *IQR
  spikes_up <-  tmp$zscore > quantile(tmp$zscore, 0.75, na.rm=T) + 1.5 *IQR
  spikes <- as.logical(spikes_down + spikes_up)
  
  # IQR <- quantile(tmp$slope, probs = 0.75, na.rm=T) - quantile(tmp$slope, probs = 0.25, na.rm=T)
  # # outliers ('spikes' in the Z score signal)
  # spikes_down <- tmp$slope < quantile(tmp$slope, 0.25, na.rm=T) - 1.5 *IQR
  # spikes_up <-  tmp$slope > quantile(tmp$slope, 0.75, na.rm=T) + 1.5 *IQR
  # spikes <- as.logical(spikes_down + spikes_up)
  
  # assign spikes
  tmp$spikes <- spikes
  
  # assign colour code to cp signal
  tmp$colour <- 'base signal' # base signal = smoothed despiked cp signal
  tmp[which(tmp$spikes == TRUE),]$colour <- 'jump'
  
  # add group to compute the slope of each group of points, separated by a jump
  tmp$group <- NA
  
  # index of jumps in the array
  jump_index <- which(tmp$colour == 'jump')
  
  # assign group identity to each group of points, separated by a jump (= subgroup)
  for (i in jump_index){
    for (j in 1:nrow(tmp)){
      if ((j < i) & (is.na(tmp$group[j]))){
        tmp$group[j] <- paste0('group_',i)
      }
    }
  }
  tmp$group[which(is.na(tmp$group))] <- 'last_group'
  
  # compute slope for each subgroup
  slope_df <- tmp |> filter(colour == 'base signal', slope != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), 
                                                                                                                  nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],
                                                                                                                  delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),
                                                                                                                  delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x) # *0.25 to convert cp to ATN
  
  # remove negative slope from the mean slope (no physical meaning)
  slope_df <- slope_df |> filter(slope > 0)
  
  # remove if only one point (cannot fit a slope with one point) -> switched to 3 points
  slope_df <- slope_df |> filter(nb_points > 3)
  
  # compute weighted average slope (to take into account the fact that some subgroups might have 2 points and a high slope vs. large group of points with a small slope)
  mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
  # convert cp to POC using Estapa's relationship 
  poc_flux <- 633*(mean_slope**0.77) # /!\ slope computed for ATN on y axis (delta_y *0.25 because ATN = cp*0.25) -> should be OK
  
  # build dataframe to plot each subgroup
  part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
  part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
  part_slope <- rbind(part1, part2)
  
  # spot negative jump
  tmp$colour[which((tmp$colour == 'jump') & (tmp$slope < 0))]  <- 'negative jump'
  
  # add large particles flux to the party
  rows_to_keep <- c(jump_index, jump_index-1)
  tmp2 <- tmp[rows_to_keep,] |> select(dates, cp, slope, colour, group) |> arrange(dates)
  
  # remove negative jumps, if any
  check_colour <- unique(tmp2$colour)
  if(length(check_colour) >= 2){ # there is a least a jump (positive or negative)
    tmp2 <- tmp2 |> dplyr::mutate(diff_jump = cp - lag(cp)) 
    even_indexes <- seq(2,nrow(tmp2),2)
    tmp2 <- tmp2[even_indexes,]
  }else{ # No jump
    tmp2 <- NULL
  }
  
  if(is.null(tmp2)){ # no jump
    large_part_poc_flux <- 0
    tmp3 <- NULL
  }else{
    tmp3 <- tmp2 |> filter(diff_jump > 0)
    if(nrow(tmp3) == 0){ # no positive jumps
      large_part_poc_flux <- 0
    }else{
      delta_y <- sum(tmp3$diff_jump) *0.25 # to get ATN (= cp*0.25)
      max_time <- max(tmp$dates)
      min_time <- min(tmp$dates)
      delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
      slope_large_part <- delta_y/delta_x
      large_part_poc_flux <- 633*(slope_large_part**0.77)
    }
  }
  
  # compute total drifting time
  max_time <- max(tmp$dates)
  min_time <- min(tmp$dates)
  drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))
  
  # to plot subgroups
  part_slope_tmp <- part_slope |> dplyr::mutate(dates = time, colour = 'slope')
  
  # plot
  jump_plot <- plot_ly(tmp, x = ~dates, y = ~cp, type = 'scatter', mode = 'markers', color = ~colour, colors = c('#003366','#E31B23', '#FFC325')) |>
    add_lines(data= part_slope_tmp, x = ~dates, y = ~cp, split = ~group, color = I('#DCEEF3'), showlegend = F) |>
    layout(title= paste0('Drifting time: ', round(drifting_time,3), ' days\n',
                         'Mean ATN slope (light blue): ', round(mean_slope,3), ' day-1\n',
                         'POC flux (small particles): ', round(poc_flux,1), ' mg C m-2 day-1\n',
                         'POC flux (large particles): ', round(large_part_poc_flux,1), ' mg C m-2 day-1'), yaxis = list(title = 'Cp (1/m)'), xaxis = list(title = 'Time'))
  
  #return(jump_plot)
  #return(list('jump_plot' = jump_plot, 'jump_table' = tmp3))
  
    # adapt script to return large and small flux
  df <- tibble('max_time' = max_time, 'min_time' = min_time, 'small_flux' = poc_flux, 'large_flux' = large_part_poc_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],
               cycle = data$cycle[1])
  
  return(df)
  #return(jump_plot)
  
}

# tmp <- all_floats_slope |> filter(wmo == 6904240, park_depth == '1000 m', cycle == 21)
# plot_jumps(tmp)
```

```{r, compute Fsmall and F large}

# remove bad data from float 6904241
float_6904241_slope <- float_6904241 |> filter(cycle < 20)
# create dataframe for all drifting data (for all floats)
all_floats_slope <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
all_floats_slope <- all_floats_slope |> filter(PhaseName == 'PAR') |> drop_na(cp) |> select(depth = pres, cp, dates = juld, park_depth, wmo, cycle) |>
   dplyr::mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
          park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

# start loop for all data
wmo <- c(4903634, 6904240, 6904241, 1902578)
park_depth <- c('200 m', '500 m', '1000 m')

res <- data.frame()
for (i in wmo){
  max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
  for (j in park_depth){
    for (k in seq(1:max_cycle)){
      tmp <- all_floats_slope |> filter(wmo == i, park_depth == j, cycle == k)
      if(nrow(tmp) == 0){
        next
      }else if(nrow(tmp) < 3){ # case where there is not enough data
        next
      }else{
          output <- plot_jumps(tmp)
          res <- rbind(res, output)
      }
    }
  }
}

# keep the good fluxes
cflux <- res |> dplyr::mutate(drifting_time = difftime(max_time, min_time, units = 'days'), WMO = factor(wmo))

cflux_info_table <- cflux |> dplyr::group_by(park_depth) |> summarize(min_smallf = min(small_flux, na.rm=T), max_smallf = max(small_flux, na.rm=T),
                                                                mean_smallf = mean(small_flux, na.rm=T), median_smallf = median(small_flux, na.rm=T),
                                                                std_smallf = sd(small_flux, na.rm=T),
                                                                min_largef = min(large_flux, na.rm=T), max_largef = max(large_flux, na.rm=T),
                                                                mean_largef = mean(large_flux, na.rm=T), median_largef = median(large_flux, na.rm=T),
                                                                std_largef = sd(large_flux, na.rm=T))

cflux$date <- as_date(cflux$min_time)
```

$F_{small}$ and $F_{large}$ were computed at each drifting depth for all floats (@fig-OST-flux). At 200 m, $F_{small}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ (average $\pm$ standard deviation). At 500 m, $F_{small}$ is comprised between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$. For both drifting depths, the trend shows a decrease of $F_{small}$ from June to, respectively, January and February before the drift was stopped at those depths due to the deepening of the winter MLD (@fig-mld) in order to ensure the float stability. At 1000 m, all floats measured an increase of $F_{small}$ from late May to July followed by a steady decrease towards March, with $F_{small}$ ranging between `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ on average.

```{r, Fsmall and Flarge, fig.width=8, fig.height=6}
#| label: fig-OST-flux
#| fig-cap: OST-derived small (top) and large (bottom) particle carbon fluxes for all floats at each drifting depth. Black lines and shaded areas represent, respectively, the monthly mean flux and the 95% confidence interval around the mean. There is no data after January and February at, respectively, 200 m and 500 m because the drifting was stopped at those depths due to the deepening of the MLD associated with convection and the impossibility to stabilize the floats.

# add black data to have the same x axis for all drifting depths (to show that we stopped the drifting at 200 and 500 m)
blank_data <- tibble(max_time = c(NA, NA), min_time = c(NA,NA), small_flux = c(NA,NA), large_flux = c(NA,NA), park_depth = c('200 m', '500 m'),
                     wmo = rep(4903634,2), cycle = c(NA,NA), drifting_time = c(NA,NA), WMO = rep(as.factor(4903634),2), date = rep('2023-03-05',2))

tmp <- rbind(cflux, blank_data)

up <- tmp |> ggplot(aes(x = date, y = small_flux)) +
    geom_point(data = tmp, aes(x = date, y = small_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = tmp, aes(x = date, y = small_flux), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = '', y = TeX('$F_{small}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  scale_y_continuous(trans = 'log10') +
  theme_bw() 

down <- tmp |> ggplot(aes(x = date, y = large_flux)) +
  geom_point(data = tmp, aes(x = date, y = large_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = tmp, aes(x = date, y = large_flux), colour = 'black') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = 'Month', y = TeX('$F_{large}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  theme_bw() +
  scale_y_continuous(trans = 'log10') +
  theme(legend.position = 'none') 

up / down
```

$F_{large}$ was estimated at 200 m between 0 (no deposition of large particles on the transmissometer window) and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_largef, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_largef, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_largef, 1)` mg C m$^{-2}$ day$^{-1}$. High fluxes (\> 100 mg C m$^{-2}$ day$^{-1}$) were measured by 3 out of 4 floats at the end of May before a sharp decrease towards July. At 500 m, $F_{large}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_largef, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_largef, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_largef, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_largef, 1)` mg C m$^{-2}$ day$^{-1}$, decreasing from September to February. At 1000 m, $F_{large}$ shows a similar pattern than the one observed for $F_{small}$ with an increase from June to mid-July, followed by a decrease towards March, with values ranging from 0 to `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_largef, 1)` mg C m$^{-2}$ day$^{-1}$ and an average of `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_largef, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_largef, 1)` mg C m$^{-2}$ day$^{-1}$.

```{r, MLD, fig.height=4, fig.width=8}
#| label: fig-mld
#| fig-cap: MLD measured by all floats from the end of May 2022 to early March 2023. Black dashed lines represent the first two drifting depths.

# add MLD here
mld_data <- vroom('/home/flo/dox/ArgoShine/npq_corrected_data.csv')
#filter date until March
mld_data <- mld_data[mld_data$juld < "2023-03-05",]
# clean and smoothed chla data
mld_data <- mld_data |> select(cycle, MLD, pres, juld, wmo, chla_npq) |> dplyr::mutate(date = as_date(juld), WMO = factor(wmo, levels = c(6904240,4903634,6904241,1902578)))
mld_data <- mld_data |> group_by(date, WMO, pres, cycle) |> summarize(mean_chla = mean(chla_npq, na.rm=T), mld = unique(MLD))

# reverselog_trans <- function(base = exp(1)) {
#     trans <- function(x) -log(x, base)
#     inv <- function(x) base^(-x)
#     trans_new(paste0("reverselog-", format(base)), trans, inv, 
#               log_breaks(base = base), 
#               domain = c(1e-100, Inf))
# }
# 
# mld_plot <- ggplot(mld_data) + geom_line(aes(x = date, y = mld, colour = WMO)) +
#   labs(x = 'Month', y = 'MLD (m)') + scale_y_continuous(trans = reverselog_trans(10), breaks = c(10,50,200,400,750)) +
#   scale_color_brewer(palette = 'Dark2') + 
#   scale_x_date(labels = date_format("%m"), date_breaks = '1 month') + theme_bw()

# reverselog_trans <- function(base = exp(1)) {
#     trans <- function(x) -log(x, base)
#     inv <- function(x) base^(-x)
#     trans_new(paste0("reverselog-", format(base)), trans, inv, 
#               log_breaks(base = base), 
#               domain = c(1e-100, Inf))
# }

mld_plot <- ggplot(mld_data) + geom_line(aes(x = date, y = mld, colour = WMO), size = 1) +
  labs(x = 'Month', y = 'MLD (m)') + scale_y_reverse() +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') + theme_bw() +
  geom_hline(yintercept = 200, colour = 'black', linetype = 'dashed') +
  geom_hline(yintercept = 500, colour = 'black', linetype = 'dashed')

mld_plot
```

```{=latex}
\newpage
```
## Discussion

### Dynamics of the spring bloom in the Labrador Sea

<!-- All floats measured similar particle concentrations at each drifting depth and on the vertical during the entire time series. In addition, except perhaps for the upper part of the UVP6 size spectrum for floats 6904241 and 1902578, the results do not indicate a clear hint of particle export signature. Therefore, we suggest that the 4 BGC-Argo floats mostly measured the dynamics of the seasonal, continuous, particle signal but that they missed the main transient particle export events associated with the LS spring bloom. -->

To estimate the time of the bloom in the deployment area, we compared MLD and surface (i.e. upper 20 m) Chla concentrations from both satellite and in situ data in a 100 km radius around each deployment. @fig-chla-mld-satellite shows that satellite and in situ Chla and MLD data concur. Therefore we assume that the former can be used to study environmental conditions before the floats deployment. Taking this into consideration, @fig-chla-mld-satellite indicates that the 6904240-4903634 pair likely missed a bloom that occurred in early May whereas the 6904241-1902578 pair missed a first (satellite) Chla increase at the end of May days before their deployment. However, a second (satellite and in situ) Chla increase reached a peak at the end of June before it decreased again towards July.

The Chla satellite pattern for the 6904240-4903634 pair typically describes the dynamics of the spring bloom in the northern part of the LS [@Lacour2015-da]. This is not surprising given that this pair was deployed one degree southward to the so-called 60°N limit [@Lacour2015-da]. @fig-chla-mld-satellite shows that the satellite MLD in that region had already shoaled below 100 m since March and that the water column was stable and relatively well stratified in May which might explain the intensity of surface Chla concentrations in contrast with those measured at the second deployment site. The latter is located in the bioregion where the phytoplankton biomass typically develops slowly in May to reach its maximum in June [@Lacour2015-da]. However, the Chla pattern observed in @fig-chla-mld-satellite could suggest that the development of the phytoplankton biomass was stopped by a mixing event at the end of May and that it started again in mid-June when the water column was well stratified.

```{r, chla and MLD from satellite, fig.height=4, fig.width=7}
#| label: fig-chla-mld-satellite
#| fig-cap: Comparison of MLD and Chla concentrations measured from satellites (dark blue) and BGC-Argo floats (red) following their deployment (in pairs). Satellite error bars represent the first and third quartiles around the median in a 100 km radius around each deployment coordinates. In situ Chla represents the Chla median in the upper 20 m of the water column.

# fetch data
npq_data <- vroom('/home/flo/dox/ArgoShine/npq_corrected_data.csv')

#filter date until March
npq_data <- npq_data[npq_data$juld < "2023-03-05",]

# clean and smoothed chla data
d <- npq_data |> select(cycle, MLD, pres, juld, wmo, chla_npq) |> dplyr::mutate(date = as_date(juld))
d <- d |> group_by(date, wmo, pres, cycle) |> summarize(mean_chla = mean(chla_npq, na.rm=T), mld = unique(MLD))
d <- d |> dplyr::mutate(smoothed_chla = smooth(mean_chla)/2) 

# remove NaN in smoothed chla
d <- d |> drop_na(smoothed_chla)

d$wmo <- factor(d$wmo, levels = c(6904240,4903634,6904241,1902578))

# compute chla integration in the MLD and in the 0-100 m zone
d_chla <- d |> group_by(wmo, date) |> summarize(chla_0_100m = integrate(smoothed_chla, pres, from=0, to=100),
                                       mld = unique(mld),
                                       chla_0_mld = integrate(smoothed_chla, pres, from = 0, to=mld),
                                       median_chla_0_20m = median(smoothed_chla[pres<=20], na.rm=T))
#
# ggplot(d_chla) + geom_point(aes(date, chla_0_100m)) + facet_wrap(~wmo, scales = 'free') + theme_bw() + labs(x = 'Month', y = 'Integrated Chla 0-100 m (CHLA UNITS)')
#ggplot(d_chla) + geom_point(aes(date, chla_0_mld)) + facet_wrap(~wmo, scales = 'free')

# add couple info
d_chla <- d_chla |> dplyr::mutate(group = if_else(wmo %in% c(6904240,4903634), '6904240 - 4903634', '6904241 - 1902578'))

#chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_with_clouds_sat.feather') |> select(time, lat, lon, chla = CHL)
chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_without_clouds_sat.feather') |> select(time, lat, lon, chla = CHL)
mld_sat <- arrow::read_feather('DATA/SATELLITE/mld_sat.feather') |> dplyr::mutate(longitude = longitude - 360) |> select(time, lat = latitude, lon = longitude, mld = mlotst)
  
# CHLA
chla_cover_deployment1 <- points_in_circle(chla_sat, lon_center = -49.2, lat_center = 58.9, radius = 100000) # 50 km around center
chla_cover_deployment1 <- chla_cover_deployment1 |> dplyr::mutate(date = as_date(time), group = '6904240 - 4903634') |> dplyr::mutate(deploy = as_date('2022-05-22'))

chla_cover_deployment2 <- points_in_circle(chla_sat, lon_center = -52.3, lat_center = 56.7, radius = 10000) # 50 km around center
chla_cover_deployment2 <- chla_cover_deployment2 |> dplyr::mutate(date = as_date(time), group = '6904241 - 1902578') |> dplyr::mutate(deploy = as_date('2022-05-29'))

# all deploy
chla_cover <- rbind(chla_cover_deployment1, chla_cover_deployment2)

# HPLC data
# chla_hplc <- vroom('DATA/ARGO/CE22009_pigments_031122_Travail_Herve_FŔ.csv') |> filter(Station %in% c(29,9)) |> select(depth = `Depth_(m)`, chla = Total_Chlorophyll_a, station = Station) |> filter(depth < 10) |> mutate(group = if_else(station == 9, '6904240 - 4903634', '6904241 - 1902578')

chla_plot <- ggplot(chla_cover |> filter(date > '2022-03-01')) + geom_errorbar(aes(x = date, y = chla),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = '#003366', width = .5) + theme_bw() +
  labs(x = 'Month', y = TeX('Chla (mg m$^{-3}$)'))+ facet_wrap(~group) +
  geom_vline(data = chla_cover, aes(xintercept = deploy), colour = "black", linetype = 'dashed') +
  geom_point(data = d_chla |> filter(date < '2022-08-01'), aes(x = date, y = median_chla_0_20m), colour = "#E31B23", size = 1) +
   scale_x_date(labels = date_format("%m"), date_breaks = '1 month')
                
# MLD
mld_cover_deployment1 <- points_in_circle(mld_sat, lon_center = -49.2, lat_center = 58.9, radius = 100000) # 50 km around center
mld_cover_deployment1 <- mld_cover_deployment1 |> dplyr::mutate(date = as_date(time), group = '6904240 - 4903634') |> dplyr::mutate(deploy = as_date('2022-05-22'))

mld_cover_deployment2 <- points_in_circle(mld_sat, lon_center = -52.3, lat_center = 56.7, radius = 100000) # 50 km around center
mld_cover_deployment2 <- mld_cover_deployment2 |> dplyr::mutate(date = as_date(time), group = '6904241 - 1902578') |> dplyr::mutate(deploy = as_date('2022-05-30'))

mld_cover <- rbind(mld_cover_deployment1, mld_cover_deployment2)

mld_plot <- ggplot(mld_cover |> filter(date > '2022-03-01')) + geom_errorbar(aes(x = date, y = mld),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = '#003366', width = 5) +
  labs(x = 'Month', y = 'MLD (m)') + facet_wrap(~group) + theme_bw() +
  geom_vline(data = chla_cover, aes(xintercept = deploy), colour = "black", linetype = 'dashed') +
  geom_point(data = d_chla |> filter(date < '2022-08-01') |> dplyr::mutate(week = week(date), DOY = yday(date)) |> group_by(week, group) |> summarize(median_mld = median(mld), mean_date = as_date(mean(DOY), origin = '2022-01-01')), aes(mean_date, median_mld), colour = "#E31B23", size = 1) +
   scale_x_date(labels = date_format("%m"), date_breaks = '1 month') + scale_y_continuous(trans = 'log10')

chla_plot / mld_plot
```

### Phytoplankton communities at deployment sites

High Performance Liquid Chromatography (HPLC) analyses were performed on samples taken during the two deployments. The selected pigments, their taxonomic significance and their size class were taken from Table 2 in @Petit2022-de. The pair 6904240-4903634 showed Chla concentrations (TChla pigment in @fig-hplc) two times higher than the pair 6904241-1902578 in the first 100 m as well as higher concentrations of diatoms (Fucoxanthin). The pair 6904241-1902578 showed higher concentrations of cryptophytes (Alloxanthin), dinoflagellates (Peridin) and green flagellates and prochlorophytes (TChlb). However, their absolute concentrations are quite low (usually less than 0.05 mg m$^{-3}$). Taking that into consideration, it seems that, at the time of float deployment, the northern site was located in a more productive area than the southern site and that it was dominated by diatoms (i.e. microphytoplankton) whereas the latter was composed of a majority of smaller phytoplankton species (mostly nano and picophytoplankton). This supports the idea of two distinct biological dynamics as inferred from float and satellite data.

```{r, hplc, fig.width=8, fig.height=6}
#| label: fig-hplc
#| fig-cap: Pigments derived from HPLC analyses in the northern (floats 6904240 and 4903634) and southern (floats 6904241 and 1902578) deployment sites.

hplc <- vroom('DATA/ARGO/CE22009_pigments_031122_Travail_Herve_FŔ.csv') |> filter(Station %in% c(29,9))

# subset (all QCs are GOOD so OK)
# hplc <- hplc |> select(lat = Latitude, lon = Longitude, depth = `Depth_(m)`,
#                        Zea = Zeaxanthin, TChlb, HF_19 = `19'-Hexanoyloxyfucoxanthin`,
#                        BF_19 = `19'-Butanoyloxyfucoxanthin`, Allo = Alloxanthin,
#                        Fuco = Fucoxanthin, Peri = Peridinin, TChla = Total_Chlorophyll_a) |> mutate(station = c(rep('station A', 10), rep('station B', 9)))

hplc <- hplc |> select(lat = Latitude, lon = Longitude, depth = `Depth_(m)`,
                       Zeaxanthin, TChlb, `19'-Hexanoyloxyfucoxanthin`,
                       `19'-Butanoyloxyfucoxanthin`, Alloxanthin,
                       Fucoxanthin, Peridinin, TChla = Total_Chlorophyll_a) |> mutate(station = c(rep('6904240 - 4903634', 10), rep('6904241 - 1902578', 9)))

# format
# hplc <- hplc |> pivot_longer(cols = c(Zea, TChlb, HF_19, BF_19, Allo, Fuco, Peri, TChla), names_to = 'pigment', values_to = 'concentration') |> drop_na(concentration) 

hplc <- hplc |> pivot_longer(cols = c(Zeaxanthin, TChlb, `19'-Hexanoyloxyfucoxanthin`,
                       `19'-Butanoyloxyfucoxanthin`, Alloxanthin,
                       Fucoxanthin, Peridinin, TChla), names_to = 'pigment', values_to = 'concentration') |> drop_na(concentration)

ggplot(hplc) + geom_path(aes(y = depth, x = concentration, colour = station)) + 
  facet_wrap(~pigment, scales = 'free') + scale_y_reverse() +
  labs(colour = '', x = TeX('Concentration (mg m$^{-3}$)'), y = 'Depth (m)') +
  theme_bw() + scale_color_manual(values = c('#E31B23', '#003366'))
```

### Unclear particle export signature

Even though we suggested that the two pairs of floats were deployed in two different bloom regimes, they all measured an increase in particle concentrations in the 102 - 512 $\mu$m range (@fig-uvp6-profiling-102-406) around 800 - 1000 m as well as an increase in the abundance of large particles (@fig-uvp6-profiling-512-2500) between June and August. This increase raises the question of a possible particle export event. For classes \> 512 $\mu$m, the vertical concentration profiles highlight a larger particle plume for the 6904241-1902578 pair than for the other one. However, apart from the float 6904240, all floats measured similar integrated Chla content in the upper 100 m and in the mixed layer (@fig-chla-content). @fig-OST-flux also shows that $F_{small}$ is similar across all floats at all parking depths and that $F_{large}$ is also similar across all floats at 1000 m with an increase from June to August as observed in F$_{small}$. The same behavior is observed for UVP6 particles in the lower size range at 1000 m (@fig-uvp-1000m). Observing similar properties across a wide geographical range around deployments suggests that we observe the latent dynamics of the system rather than episodic export events.

```{r, mld + chla, fig.height=6, fig.width=8}
#| label: fig-chla-content
#| fig-cap: Integrated Chla content in the mixed layer (up) and in the upper 100 m (down) for each float.

chla_mld <- d_chla |> drop_na(chla_0_100m)

chla_plot_mld <- ggplot(chla_mld) + geom_line(aes(x = date, y = chla_0_mld, colour = as.factor(wmo)), linewidth = 1) + labs(x = 'Month', y = TeX('Integrated Chla content (mg m$^{-2}$)'), colour = 'WMO') +
  theme_bw() + scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  #scale_colour_manual(values = c('#003366', '#E31B23'))
  #scale_color_brewer(palette = 'Dark2') 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) 

chla_plot_100m <- ggplot(chla_mld) + geom_line(aes(x = date, y = chla_0_100m, colour = as.factor(wmo)), linewidth = 1) + labs(x = 'Month', y = TeX('Integrated Chla content (mg m$^{-2}$)'), colour = 'WMO') +
  theme_bw() + scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  #scale_color_brewer(palette = 'Dark2') + 
  scale_colour_manual(values = c('#D95F02', '#7570B3', '#E6AB02', '#1B9E77')) +
  theme(legend.position = 'none')

chla_plot_mld / chla_plot_100m
```

```{=latex}
\newpage
```
```{r, spectral slope}

mid_DSE <- c(0.1147968,0.1446349,0.1822286,0.2295937,0.2892699,0.3644572,0.4591873,0.5785398,0.7289145,0.9183747,1.1570796,1.4578289,1.83674934,2.31415916)
sizebin <- c(0.02640633,0.03326989,0.04191744,0.05281267,0.06653979,0.08383488,0.10562533,0.13307958,0.16766976,0.21125066,0.26615915,0.33533952,0.422501323,0.532318310)

vertical_profiles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class)) |> drop_na(NP_Size_102) |> mutate(juld = as_date(juld)) |>
  filter(depth > 0) # remove buggy data from 4903634

compute_slope <- function(i, data_spectra){
  
  spectrum <- data_spectra[i,]
  
  spectrum_norm <- spectrum/sizebin
  
  # prepare data for linear regression
  Y <- log(spectrum_norm)
  X <- log(mid_DSE)
  
  # check for finite value
  h <- is.finite(Y)
  Y <- Y[h]
  X <- X[h]
  
  data_slope <- tibble(X=X, Y=Y)
  
  model <- lm(formula = Y ~ X, data = data_slope)
  slope <- model$coefficients[2]
  
  return(slope)
}

# compute slope on vertical profiles
vertical_spectra <- as.matrix(data.frame(vertical_profiles[,5:18]))
index <- seq(from = 1, to = nrow(vertical_spectra), by = 1)
slopes <- map_dbl(index, compute_slope, data_spectra = vertical_spectra)
vertical_profiles$spectral_slope <- slopes
```

```{r, taxo}

# read vertical (NPAR) taxo data
taxo_1902578 <- vroom::vroom('DATA/ARGO/1902578_TAXO_FromNetCDF.csv') |> mutate(wmo = 1902578, date = argoJuldToTime(juld)) |> filter(PhaseName == 'NPAR')
taxo_4903634 <- vroom::vroom('DATA/ARGO/4903634_TAXO_FromNetCDF.csv') |> mutate(wmo = 4903634, date = argoJuldToTime(juld)) |> filter(PhaseName == 'NPAR')
taxo_6904240 <- vroom::vroom('DATA/ARGO/6904240_TAXO_FromNetCDF.csv') |> mutate(wmo = 6904240, date = argoJuldToTime(juld)) |> filter(PhaseName == 'NPAR')
taxo_6904241 <- vroom::vroom('DATA/ARGO/6904241_TAXO_FromNetCDF.csv') |> mutate(wmo = 6904241, date = argoJuldToTime(juld)) |> filter(PhaseName == 'NPAR')

all_taxo <- rbind(taxo_1902578, taxo_4903634, taxo_6904240, taxo_6904241) |> mutate(wmo = factor(wmo, levels = c('6904240','4903634','6904241','1902578')))

# test taxo -> pas forcément concluant -> à creuser..
#ggplot(taxo_6904241 |> filter(cycle == 3)) + geom_point(aes(x = conc_calanoid, y = -depth))
```

```{r, spiciness, fig.width=6, fig.height=10}
#| label: fig-spice
#| fig-cap: Potential density anomaly, spiciness, *Calanoida* concentrations (log$_{10}$ colorscale), spectral slope and particle concentrations for the 102-128 $\mu$m size class (log$_{10}$ colorscale) for each float. 

ts_data <- all_floats |> filter(PhaseName == 'NPAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth, lon, lat) |> drop_na(temp) |> filter(sigma > 26) |> 
  dplyr::mutate(date = as_date(date))

ts_data <- ts_data |> mutate(spiciness = swSpice(ts_data$psal, ts_data$temp, ts_data$depth, ts_data$lon, ts_data$lat))

density <- ggplot(ts_data |> filter(depth <= 1000)) + geom_point(aes(x=date, y=depth, colour=sigma), size = 3, shape = 15) +
  scale_y_reverse() +
  #scale_color_viridis() +
  scale_color_cmocean(name = 'dense', limits = c(27,28), oob = scales::squish, na.value = 'grey') +
  theme_bw() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = TeX('Sigma (kg m$^{-3}$)    '), x = 'Month', y = 'Depth (m)') +
  facet_wrap(~wmo, ncol = 4) + theme(axis.title.x = element_blank())

spiciness <- ggplot(ts_data |> filter(depth <= 1000)) + geom_point(aes(x=date, y=depth, colour=spiciness), size = 3, shape = 15) +
  scale_y_reverse() +
  #scale_color_viridis() +
  scale_color_cmocean(name = 'balance', limits = c(-0.5,0.5), oob = scales::squish, na.value = 'grey') +
  theme_bw() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = TeX('Spiciness (kg m$^{-3}$)'), x = 'Month', y = 'Depth (m)') +
  facet_wrap(~wmo, ncol = 4) + theme(axis.title.x = element_blank())

part102 <- particles_bis |> filter(depth <= 1000, size == 102) |>
  #group_by(size) |>
  ggplot() + facet_wrap(~wmo, ncol = 4) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=2, shape=15) +
  #scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_color_cmocean(name = 'thermal', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) +
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = 'Particles (#/L)       ', x = 'Month', y = 'Depth (m)') 

taxo_plot <- all_taxo |> filter(depth <= 1000) |> mutate(date = as.Date(date)) |> filter(date < as.Date("2023-06-03")) |>
  #group_by(size) |>
  ggplot() + facet_wrap(~wmo, ncol = 4) + theme_bw() +
  geom_point(aes(x=date, y=depth, colour=log10(conc_calanoid)), size=1, shape=15) +
  scale_color_cmocean(name = 'thermal', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = 1) +
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = 'Calanoida (#/L)    ', x = 'Month', y = 'Depth (m)') + theme(axis.title.x = element_blank())

spectral_slope_plot <- vertical_profiles |> filter(depth <= 1000) |> mutate(wmo = factor(wmo, levels = c('6904240','4903634','6904241','1902578'))) |>
  ggplot() + facet_wrap(~wmo, ncol = 4) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=spectral_slope), size=2, shape=15) +
  scale_color_cmocean(name = 'thermal', limits = c(-4,-3), oob = scales::squish, na.value = 'grey', direction = 1) +
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = 'Spectral slope     ', x = 'Month', y = 'Depth (m)') + theme(axis.title.x = element_blank())

density / spiciness / taxo_plot / spectral_slope_plot / part102
```

Still, it is unclear why we observe a progressive emptying of the water column starting below 500 m for size classes below 323 $\mu$m (@fig-uvp6-profiling-102-406). This pattern could result from several causes. It could either be the signature of a subduction event [@Llort2018-pa] either the end of a slow sinking export (i.e seasonal dynamics of the system) or a sign of fragmentation processes by zooplankton (i.e. zooplankton feeding). Therefore, we looked at physical properties (density anomaly and spiciness) and well as particle characteristics (zooplankton concentration and spectral slope) to investigate this particularity (@fig-spice). Spiciness is a variable that allows the differentiation of water masses with distinct thermohaline properties but similar density [@Flament2002-zm]. @fig-spice shows that both the density and the spiciness fields are uniform below 500 m hence it is unlikely that a subduction event would be the reason for this decrease in particle abundance below 500 m. Using the embedded zooplankton classification results from the UVP6, we looked at concentrations of *Calanoida*, a species that is abundant in the NA [@Corkett1979-ha]. @fig-spice shows that the concentration of *Calanoida* is higher from the end of May to July in the surface layer. There also seems to be a lower patch between 250 and 500 m from August to November however it is difficult to estimate their impact on particle concentrations at this stage because more data are needed. Finally, we computed the spectral slope to look for aggregation/fragmentation processes. The spectral slope defines the slope of the PSD where a high (in absolute values) spectral slope indicates that small particules dominate the PSD. Here, the panel on spectral slopes in @fig-spice shows a lower (in absolute values) spectral slope (i.e. large particles dominate) at the beginning of the time series but it is also difficult to relate it to biological processes such as zooplankton feeding without any additional data.

All things considered, it is not straightforward to conclude for the presence of a carbon export event. First, the 10-day profiling configuration hinders the characterization of particle export events that are transient processes. Second, the plume of large particles can be misleading because the abundance measured by the UVP6 are very low (usually \< 0.1 #/L). However, the plume could be the result of aggregation (repackaging) by zooplankton feeding on small particles. Third, all floats measured the same particle dynamics in distinct zones which suggests that if the two systems were initially different (i.e. different bloom dynamics), they merged back after the bloom of May. Therefore, we suggest that we mostly observe the latent dynamics of the system rather than clear export signatures that would likely be spotted with both higher bloom intensities (e.g. the May bloom that was missed) and a higher profiling temporal resolution.

```{=latex}
\newpage
```
### Comparing OST and UVP6 data

#### Carbon flux trends and estimations

For the first time, BGC-Argo floats equipped with both an OST and the UVP6 have been deployed to better characterize the downward carbon flux and the nature of sinking particles from a multi-instruments approach [@Claustre2021-tx]. The quantification of the POC flux using the OST method [@Bishop2004-at; @Estapa2013-gw; @Estapa2017-jr; @Estapa2019-nk; @Estapa2023-oz] is based on an empirical POC:ATN ratio [@Estapa2023-oz] whereas UVP6 PSD measurements are used to compute POC fluxes using @eq-guidi-article3:

$$
F = \sum_{i}^{N}C_{i}Ad_{i}^{B}
$$ {#eq-guidi-article3}

where $C_{i}$, A and B represent, respectively, the concentration of particles (#/L) for the $i^{th}$ class of mean diameter $d_{i}$ (in mm) and 2 constants with $B$ being related to the fractal dimension ($D$) of an aggregate by $D = (B +1)/2$ [@Guidi2008-vp].

The use of @eq-guidi-article3 to derive carbon fluxes from PSD was firstly implemented by @Guidi2008-vp who used a minimization procedure between sediment traps data and estimated fluxes using PSD from several previous versions of the UVP to find the optimal value of $A$ and $B$ (i.e. 12.5 and 3.81). @Iversen2010-an and @Fender2019-ou followed the same procedure and concluded that the couple of parameters found by [@Guidi2008-vp] could not be used globally at the expense of greatly over- or underestimating POC fluxes, depending on the sampling region. In addition, the formulation of such a model implies that $A$ and $B$ would be valid at global scale, at all depths, for all UVP models and across all size classes which is arguable [@Bisson2022-bt]. @eq-guidi-article3 also assumes that the flux is only size-dependent though the main driver for the settling of aggregates is still debated [@Iversen2020-kb; @Iversen2023-qj]. As a result, the direct comparison of carbon fluxes derived from both OST and UVP data with @eq-guidi-article3 should be focused on the trend rather than on absolute carbon flux values.

```{r, compute guidi fluxes at parking depth, fig.height=6, fig.width=8}
#| label: fig-comparison-ost-uvp
#| fig-cap: Comparison of total carbon fluxes (F$_{small}$ + F$_{large}$) derived from the OST (black) and the UVP6 using the $A$ and $B$ values of @Guidi2008-vp (red) and @Iversen2010-an (yellow) in @eq-guidi-article3. OST data from float 6904241 stops in November due to a transmissometer failure.

compute_flux <- function(A, b, PSD){ # PSD = Particle Size Distribution
  # for classes sizes between 0.25 mm and 1.5 mm
  mid_ESD <- c(0.29042685,
               0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
               1.16170742, 1.46365963) # With Lionel's script
  exp <- A * mid_ESD ** b
  estimated_flux <- rowSums(t(t(PSD)*exp))
  return(estimated_flux)
}

parking_data <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
parking_data <- parking_data |> filter(PhaseName == 'PAR') |> drop_na(NP_Size_102) |> select(park_depth, all_of(lpm_class), dates = juld, park_depth, wmo, cycle) |> dplyr::mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
                                   park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))
parking_spectra <- parking_data |> dplyr::select(NP_Size_256:NP_Size_1290)

parking_data <- parking_data |> dplyr::mutate(guidi_flux = compute_flux(12.5, 3.81, parking_spectra)) #|> group_by(park_depth, cycle, wmo) 

# add rainer computation
compute_flux_rainer <- function(A, b, PSD){
  
  # Rainer uses an ESD in cm ..
  mid_ESD <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685,
       0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
       1.16170742, 1.46365963, 1.84409558, 2.32341484) / 10
  exp <- A * mid_ESD ** b # because Rainer uses the units of particles/m3 and not particles/L
  estimated_flux <- rowSums(t(t(PSD)*exp))
  return(estimated_flux)
}

parking_spectra_rainer <- parking_data |> dplyr::select(lpm_class)
parking_data <- parking_data |> dplyr::mutate(rainer_flux = compute_flux_rainer(2.8649, 2.24, parking_spectra_rainer)) 
parking_data <- parking_data |> dplyr::mutate(iversen_flux = compute_flux(273, 4.27, parking_spectra)) 

# plot it
comparison_flux_ost_uvp <- merge(parking_data, cflux) |> dplyr::mutate(total_flux = small_flux + large_flux)

# add guidi flux obtained with fminsearch
# mean_d <- c(0.29042685, 0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779, 1.16170742, 1.46365963)
# comparison_flux_ost_uvp <-  comparison_flux_ost_uvp[-c(bad_index),]
# ost_flux <- comparison_flux_ost_uvp$total_flux
# spectra <- comparison_flux_ost_uvp |> select(NP_Size_256:NP_Size_1290)
# # some spectra gives 0 for all classes - makes a buggy fminsearch -> remove those lines
# bad_index <- which((rowSums(t(t(spectra)))) == 0)
# spectra <- spectra[-c(bad_index),]
# compute_flux <- function(x) sum((log10(rowSums(t(t(spectra)*x[1]*mean_d ** x[2]))) - log10(ost_flux))**2)
# x0 <- c(12.5, 2.2)
# fminsearch(compute_flux, x0 = x0)
# fmincon(x0 = x0, compute_flux, lb = c(1, 1.5), ub = c(1000, 3))

# add guidi flux using fminsearch using the optimize function of scipy in python (see a paper of someoe who did it, see Giering 2020 ou willimaoson et giering 2022)
comparison_flux_ost_uvp <- comparison_flux_ost_uvp |> dplyr::mutate(guidi_flux_fminsearch = compute_flux(21.98, 1.18, parking_spectra))

# https://stackoverflow.com/questions/41077199/how-to-use-r-ggplot-stat-summary-to-plot-median-and-quartiles
ggplot(comparison_flux_ost_uvp) + geom_line(aes(x = date, y = total_flux), colour = 'black') + facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) +
  geom_errorbar(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      #fun.min = function(z) {quantile(z, 0.25)},
                      #fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = '#E31B23', width = 5) +
    geom_line(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      fun = median, colour = '#E31B23') +
    geom_errorbar(aes(x = date, y = iversen_flux),
                      stat = 'summary',
                      #fun.min = function(z) {quantile(z, 0.25)},
                      #fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = '#FFC325', width = 5) + # #003366
    geom_line(aes(x = date, y = iversen_flux),
                      stat = 'summary',
                      fun = median, colour = '#FFC325') + 
  scale_y_continuous(trans = 'log10') + theme_bw() + labs(x = 'Month', y = TeX('$F_{total}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month')
```

```{r, correlations between OST and UVP carbon fluxes}
#| label: tbl-correlation
#| tbl-cap: Pearson correlations (all statistically significant with a p-value < 0.01) between UVP6 PSD-derived carbon fluxes and total, large and small particle carbon fluxes measured by the OST at 3 drifting depths.
correlations <- comparison_flux_ost_uvp |> dplyr::select(park_depth, wmo, cycle, guidi_flux, small_flux, large_flux, date, total_flux) |> dplyr::group_by(wmo, park_depth, cycle) |> dplyr::mutate(median_guidi_flux = median(guidi_flux)) |> dplyr::group_by(park_depth, wmo) |> dplyr::summarize(cor_small = round(cor(median_guidi_flux, small_flux),2), cor_large = round(cor(median_guidi_flux, large_flux),2), cor_total = round(cor(median_guidi_flux, total_flux),2))

correlations <- correlations |> pivot_wider(names_from = park_depth, values_from = c(cor_small, cor_large, cor_total))

gt(correlations) |> tab_spanner(label = "Small flux", columns = 2:4) |> tab_spanner(label = "Large flux", columns = 5:7) |> tab_spanner(label = 'Total flux', columns = 8:10) |>
  cols_label(wmo = 'WMO', `cor_small_200 m` = '200 m',
             `cor_small_500 m` = '500 m',
             `cor_small_1000 m` = '1000 m',
             `cor_large_200 m` = '200 m',
             `cor_large_500 m` = '500 m',
             `cor_large_1000 m` = '1000 m',
             `cor_total_200 m` = '200 m',
             `cor_total_500 m` = '500 m',
             `cor_total_1000 m` = '1000 m')
```

@fig-comparison-ost-uvp shows the comparison between the total carbon flux measured by the OST ($F_{total} = F_{small} + F_{large}$) and the UVP6 (median of all UVP measurements while the float was drifting) at all drifting depths. Overall, carbon fluxes estimated with the $A$ and $B$ coefficients of @Guidi2008-vp are lower than the ones measured by the OST whereas the same estimation using the coefficient of @Iversen2010-an generally overestimates the OST at 200 m but is of the same order at 500 m and 1000 m. Without additional measurements (e.g. co-deployed sediment traps), it is however impossible to decipher which measurement is closest to the truth. Nonetheless, carbon fluxes measured at 1000 m by the OST and the UVP6 with the coefficients of @Iversen2010-an are both well correlated (\> 0.6 for all floats) and of similar magnitudes while carbon fluxes using @Guidi2008-vp are one order of magnitude lower. In their paper, @Iversen2010-an proposed that the carbon fluxes discrepancy could be due to different biogeochemical regimes where the relationship of @Guidi2008-vp was rather based on sediment traps at open ocean sites with lower carbon fluxes [@Honjo2008-kn] while the study site of @Iversen2010-an was located in a highly productive system at the continental margin off Cape Blanc in Mauritania. The LS is productive during the spring bloom [@Lacour2015-da] therefore it is coherent to find a closer relationship with estimations based on a highly productive system.

The trends of total flux measured by both instruments are very well correlated at all drifting depths for floats 6904240 and 4903634 with correlations (see @tbl-correlation) ranging from `r min(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,8:10])` to `r max(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,8:10])`. @tbl-correlation also shows that the correlation between UVP-derived carbon fluxes and $F_{small}$ and $F_{large}$ for that pair is `r round(mean(as.vector(as.matrix(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,2:7]))),2)` on average. We calculated lower correlations with the other two floats, especially the float 1902578 at 200 m, where correlations between the UVP-derived carbon fluxes and all fluxes (small, large and total) are close to 0. Overall, it nevertheless suggests that there is a possibility to get more accurate estimations of carbon fluxes using only the PSD hence using only the particle size as an estimator of carbon flux, contrarily to what other studies have argued [@Iversen2020-kb; @McDonnell2010-un; @Iversen2023-qj].

#### Nature of particles seen by the OST and the UVP6

F$_{small}$ represents the continuous accumulation of small particles hence it could be related to the accumulated biovolume of particles seen by the UVP6. Similarly, $F_{large}$ could be related to the biovolume seen by the UVP6 when the OST detects jumps in the attenuation signal. Consequently, we firstly computed the slope of each segment (see lightblue straight lines in @fig-estapa-method_article) and all the jumps at all drifting depths as well as the total biovolume seen by the UVP6 while the float was drifting. Then, we computed the average biovolume seen during each segment duration and we associated the closest total biovolume (in time) with a jump.

```{r, get jump info}

# get_jump_info <- function(data){
#   
#   # make sure that the cp signal is in chronological order
#   tmp <- data |> arrange(dates)
#   
#   # despike cp data with a 7-point moving window
#   tmp$cp <- despike(tmp$cp, k = 3)
#   
#   # smooth cp data with a 3-point moving median, n time(s)
#   tmp$cp <- slide(tmp$cp, fun = median, k = 3, n = 1, na.rm=T)
#   
#   # compute slope between two adjacent points (except first point) # we could start after 1h to let the float stabilize
#   delta_x <- as.numeric(tmp$dates - lag(tmp$dates), units = 'days')
#   delta_y <- tmp$cp - lag(tmp$cp)
#   tmp$slope <- delta_y/delta_x
#   
#   # compute a Z score (assuming a normal distribution of the slopes) on the slopes
#   tmp <- tmp |> dplyr::mutate(zscore = (slope - mean(slope, na.rm = T))/sd(slope, na.rm = T))
#   
#   # spot outliers on the Z score signal
#   # interquartile range between Q25 and Q75 -> had to used that and not the despike function because slopes are often close (or equal) to 0 so it can miss clear jumps. Q25 and Q75 are more trustworthy in this case than the despike function of Jean-Olivier (see package castr on his github: https://github.com/jiho/castr)
#   IQR <- quantile(tmp$zscore, probs = 0.75, na.rm=T) - quantile(tmp$zscore, probs = 0.25, na.rm=T)
#   # outliers ('spikes' in the Z score signal)
#   spikes_down <- tmp$zscore < quantile(tmp$zscore, 0.25, na.rm=T) - 1.5 *IQR
#   spikes_up <-  tmp$zscore > quantile(tmp$zscore, 0.75, na.rm=T) + 1.5 *IQR
#   spikes <- as.logical(spikes_down + spikes_up)
#   
#   # assign spikes
#   tmp$spikes <- spikes
#   
#   # assign colour code to cp signal
#   tmp$colour <- 'base signal' # base signal = smoothed despiked cp signal
#   tmp[which(tmp$spikes == TRUE),]$colour <- 'jump'
#   
#   # add group to compute the slope of each group of points, separated by a jump
#   tmp$group <- NA
#   
#   # index of jumps in the array
#   jump_index <- which(tmp$colour == 'jump')
#   
#   # assign group identity to each group of points, separated by a jump (= subgroup)
#   for (i in jump_index){
#     for (j in 1:nrow(tmp)){
#       if ((j < i) & (is.na(tmp$group[j]))){
#         tmp$group[j] <- paste0('group_',i)
#       }
#     }
#   }
#   tmp$group[which(is.na(tmp$group))] <- 'last_group'
#   
#   # compute slope for each subgroup
#   slope_df <- tmp |> filter(colour == 'base signal', slope != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), 
#                                                                                                                   nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],
#                                                                                                                   delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),
#                                                                                                                   delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x) # *0.25 to convert cp to ATN
#   
#   # remove negative slope from the mean slope (no physical meaning)
#   slope_df <- slope_df |> filter(slope > 0)
#   
#   # remove if only one point (cannot fit a slope with one point)
#   slope_df <- slope_df |> filter(nb_points > 1)
#   
#   # compute weighted average slope (to take into account the fact that some subgroups might have 2 points and a high slope vs. large group of points with a small slope)
#   mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
#   
#   # convert cp to POC using Estapa's relationship 
#   poc_flux <- 633*(mean_slope**0.77) # /!\ slope computed for ATN on y axis (delta_y *0.25 because ATN = cp*0.25) -> should be OK
#   
#   # build dataframe to plot each subgroup
#   part1 <- slope_df |> dplyr::select(group, time = min_time, cp = first_cp)
#   part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
#   part_slope <- rbind(part1, part2)
#   
#   # spot negative jump
#   tmp$colour[which((tmp$colour == 'jump') & (tmp$slope < 0))]  <- 'negative jump'
#   
#   # add large particles flux to the party
#   rows_to_keep <- c(jump_index, jump_index-1)
#   tmp2 <- tmp[rows_to_keep,] |> select(dates, cp, slope, colour, group) |> arrange(dates)
#   
#   # remove negative jumps, if any
#   check_colour <- unique(tmp2$colour)
#   if(length(check_colour) >= 2){ # there is a least a jump (positive or negative)
#     tmp2 <- tmp2 |> dplyr::mutate(diff_jump = cp - lag(cp)) 
#     even_indexes <- seq(2,nrow(tmp2),2)
#     tmp2 <- tmp2[even_indexes,]
#   }else{ # No jump
#     tmp2 <- NULL
#   }
#   
#   if(is.null(tmp2)){ # no jump
#     large_part_poc_flux <- 0
#     tmp3 <- NULL
#   }else{
#     tmp3 <- tmp2 |> filter(diff_jump > 0) # remove negative
#     if(nrow(tmp3) == 0){ # no positive jumps
#       large_part_poc_flux <- 0
#     }else{
#       delta_y <- sum(tmp3$diff_jump) *0.25 # to get ATN (= cp*0.25)
#       max_time <- max(tmp$dates)
#       min_time <- min(tmp$dates)
#       delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
#       slope_large_part <- delta_y/delta_x
#       large_part_poc_flux <- 633*(slope_large_part**0.77)
#     }
#   }
#   
#   # compute total drifting time
#   max_time <- max(tmp$dates)
#   min_time <- min(tmp$dates)
#   drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))
#   
#   # to plot subgroups
#   part_slope_tmp <- part_slope |> dplyr::mutate(dates = time, colour = 'slope')
#   
#   # adapt script to return large and small flux
#   df <- tibble('max_time' = max_time, 'min_time' = min_time, 'small_flux' = poc_flux, 'large_flux' = large_part_poc_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],
#                cycle = data$cycle[1])
#   
#   # clean data to send
#   slope_df <- slope_df |> arrange(min_time) |> dplyr::mutate(min_drifting_time = df$min_time, max_drifting_time = df$max_time, small_flux = df$small_flux, large_flux = df$large_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],cycle = data$cycle[1])
#   if(is.null(tmp3)){
#     large_jump <- tmp3
#   }else{
#      large_jump <- tmp3 |> dplyr::mutate(park_depth = data$park_depth[1], wmo = data$wmo[1],cycle = data$cycle[1]) 
#   }
#   
#   return(list(slope_info = slope_df, jump_info = large_jump))
# }
```

```{r, compute slope and jump info}

# start loop for all data
wmo <- c(4903634, 6904240, 6904241, 1902578)
park_depth <- c('200 m', '500 m', '1000 m')

# res_slope <- data.frame()
# res_jumps <- data.frame()
# for (i in wmo){
#   max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
#   for (j in park_depth){
#     for (k in seq(1:max_cycle)){
#       tmp <- all_floats_slope |> filter(wmo == i, park_depth == j, cycle == k)
#       if(nrow(tmp) == 0){
#         next
#       }else if(nrow(tmp) < 3){ # case where there is not enough data
#         next
#       }else{
#           output <- get_jump_info(tmp)
#           res_slope <- rbind(res_slope, output$slope_info)
#           res_jumps <- rbind(res_jumps, output$jump_info)
#       }
#     }
#   }
# }

# write data because it's long
# vroom_write(res_slope, file = '/home/flo/dox/THESIS/DATA/ARGO/res_slope.csv')
# vroom_write(res_jumps, file = '/home/flo/dox/THESIS/DATA/ARGO/res_jumps.csv')
res_slope <- vroom::vroom(file = '/home/flo/dox/THESIS/DATA/ARGO/res_slope.csv')
res_jumps <- vroom::vroom(file = '/home/flo/dox/THESIS/DATA/ARGO/res_jumps.csv')
```

```{r, check total biovolume with slope value, fig.width=10, fig.height=6}
#| label: fig-biovolume-slope
#| fig-cap: Average biovolume seen by the UVP6 as a function of the attenuance slope (top) and total biovolume seen by the UVP6 as a function of the intensity of the jump seen by the OST (bottom) at each drifting depth.

# uvp_drifting <- all_floats |> filter(PhaseName == 'PAR') |> select(juld, cycle, wmo, park_depth, all_of(lpm_class)) |> 
#   drop_na(NP_Size_102) |> dplyr::mutate(date = as_date(juld))
# 
# # DSE for computing biovolumes
# #DSE <- c(0.29042685,0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779, 1.16170742, 1.46365963)
# DSE_long <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685, 0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
#                 1.16170742, 1.46365963, 1.84409558, 2.32341484)
# 
# # compute biovolume
# get_biovolume <- function(DSE){
#   r <- DSE/2
#   biovolume <- (4*pi/3)*r**3
#   return(biovolume)
# }
# 
# # get biovolume for each DSE (equivalent to say for each size bin)
# biovolumes <- map_dbl(DSE_long, get_biovolume)  # unit = mm^3
# 
# # compute biovolume and add them to the dataframe
# spectra_biovolume <- tibble(as.data.frame(t(t(uvp_drifting[,all_of(lpm_class)])*biovolumes)))
# colnames(spectra_biovolume) <- colnames(spectra_biovolume)|> str_replace('NP_Size', 'Biovolume')
# biovolumes_names <- colnames(spectra_biovolume)
# uvp_drifting <- cbind(uvp_drifting, spectra_biovolume) |> rowwise() |> dplyr::mutate(total_biovolume = sum(across(all_of(biovolumes_names))))
# 
# assign_time_group <- function(d_uvp, d_ost){
#   d_uvp$group <- NA
#   for (i in 1:nrow(d_ost)){
#     low_time <- d_ost$min_time[i]
#     high_time <- d_ost$max_time[i]
#     #print(c(low_time, high_time))
#     group <- d_ost$group[i]
#     time_index <- which(d_uvp$juld >= low_time & d_uvp$juld <= high_time)
#     d_uvp$group[time_index] <- group
#   }
#   return(d_uvp)
# }
# 
# # loop for it
# # start loop for all data
# wmo <- c(4903634, 6904240, 6904241, 1902578)
# park_depth <- c('200 m', '500 m', '1000 m')

# test <- data.frame()
# for (i in wmo){
#   max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
#   for (j in park_depth){
#     for (k in seq(1:max_cycle)){
#       tmp_uvp <- filter(uvp_drifting, wmo == i, cycle == k, park_depth == j) |> arrange(juld) |> select(juld, cycle, wmo, park_depth, total_biovolume) 
#       tmp_ost <- res_slope |> filter(wmo == i, cycle == k, park_depth == j)
#       if(nrow(tmp_uvp) == 0 | nrow(tmp_ost) == 0){
#         next
#       }else{
#       tmp <- assign_time_group(tmp_uvp, tmp_ost)
#       #tmp2 <- tmp |> filter(!is.na(group)) |> dplyr::group_by(group) |> dplyr::mutate(uvp_points = n(), min_date = min(juld), max_date = max(juld), mean_date = mean(floor_date(juld))) # https://stackoverflow.com/questions/44814435/calculate-average-daily-value-from-large-data-set-with-r-standard-format-date-ti
#       tmp2 <- tmp |> filter(!is.na(group)) |> group_by(group) |> dplyr::mutate(uvp_points = n(), mean_date = mean(floor_date(juld)))
#       tmp3 <- merge(tmp2, tmp_ost) |> dplyr::group_by(group, cycle, wmo, park_depth) |> dplyr::summarize(mean_date = unique(mean_date), uvp_points = unique(uvp_points), ost_points = unique(nb_points), group_total_biovolume = sum(total_biovolume), slope = unique(slope), weighted_group_biovolume = group_total_biovolume/uvp_points)
#       test <- rbind(test, tmp3)
#       }
#     }
#   }
# }

# write data because it's long
# vroom_write(test, file = '/home/flo/dox/THESIS/DATA/ARGO/slope_vs_biovolume.csv')

# load data computed with the above commented code
slope_vs_biovolume <- vroom::vroom('DATA/ARGO/slope_vs_biovolume.csv')
slope_vs_biovolume <- slope_vs_biovolume |> mutate(park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

slope_vs_biovolume <- slope_vs_biovolume |> filter(slope > 0.00001) |> mutate(slope = slope*0.25, logslope = log10(slope), logweight = log10(weighted_group_biovolume)) 

# ggplot(slope_vs_biovolume, aes(x = logslope, y = logweight, group = park_depth)) + geom_bin2d(bins = 30) + scale_fill_continuous(type = 'viridis') + facet_wrap(~park_depth, scales = 'free') + theme_bw() +
#    stat_smooth(aes(x = logslope, y = logweight), method = "lm", se = FALSE, colour = "red") + labs(x = TeX('$ATN~slope~(day^{-1})$'), y = TeX('$UVP~biovolume~(mm^{3})$'))

uvp_ost_up <- ggplot(slope_vs_biovolume, aes(x = slope, y = weighted_group_biovolume, group = park_depth)) + geom_bin2d(bins = 30) + scale_fill_continuous(type = 'viridis') + facet_wrap(~park_depth, scales = 'free') + theme_bw() +
    labs(x = TeX('$ATN~slope~(day^{-1})$'), y = TeX('$UVP~biovolume~(mm^{3})$')) + scale_x_continuous(trans = 'log10') + scale_y_continuous(trans = 'log10') 

# what about jump values with uvp measurements at jump
# uvp_drifting <- uvp_drifting |> dplyr::mutate(num_date = as.numeric(juld))
# res_jumps <- res_jumps |> dplyr::mutate(num_date = as.numeric(dates))
# 
# assign_uvp_to_jump <- function(d_uvp, d_jump){
#   d_jump$biovolume <- NA
#   for (i in 1:nrow(d_jump)){
#     index_jump_for_uvp <- which.min(abs(tmp_uvp$num_date - tmp_ost_jump$num_date[i]))
#     d_jump$biovolume[i] <- tmp_uvp$total_biovolume[index_jump_for_uvp]
#   }
#   return(d_jump)
# }

# test_jump <- data.frame()
# for (i in wmo){
#   max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
#   for (j in park_depth){
#     for (k in seq(1:max_cycle)){
#       tmp_uvp <- filter(uvp_drifting, wmo == i, cycle == k, park_depth == j) |> arrange(juld) |> select(juld, cycle, wmo, park_depth, total_biovolume, num_date) 
#       tmp_ost_jump <- res_jumps |> filter(wmo == i, cycle == k, park_depth == j)
#       if(nrow(tmp_ost_jump) == 0 | nrow(tmp_uvp) == 0){
#         next
#       }else{
#         tmp <- assign_uvp_to_jump(tmp_uvp, tmp_ost_jump)
#       }
#       test_jump <- rbind(test_jump, tmp)
#     }
#   }
# }

#vroom_write(test_jump, file = '/home/flo/dox/THESIS/DATA/ARGO/diff_jump_vs_biovolume.csv')

# load data computed with the above commented code
diff_jump_vs_biovolume <- vroom::vroom('DATA/ARGO/diff_jump_vs_biovolume.csv')
diff_jump_vs_biovolume <- diff_jump_vs_biovolume|> mutate(park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')), diff_jump = diff_jump*0.25, log_diff_jump = log10(diff_jump), log_biovolume = log10(biovolume))

# ggplot(diff_jump_vs_biovolume, aes(y = log_biovolume, x = log_diff_jump, group = park_depth)) + geom_bin2d(bins = 30) + scale_fill_continuous(type = 'viridis') + facet_wrap(~park_depth, scales = 'free') + theme_bw() +
#    stat_smooth(aes(y = log_biovolume, x = log_diff_jump), method = "lm", se = FALSE, colour = "red") + labs(x = TeX('$ATN~jump~difference$'), y = TeX('$UVP~biovolume~(mm^{3})$'))

uvp_ost_down <- ggplot(diff_jump_vs_biovolume, aes(y = biovolume, x = diff_jump, group = park_depth)) + geom_bin2d(bins = 30) + scale_fill_continuous(type = 'viridis') + facet_wrap(~park_depth, scales = 'free') + theme_bw() +
    scale_x_continuous(trans = 'log10') + scale_y_continuous(trans = 'log10') + labs(x = TeX('$ATN~jump~difference$'), y = TeX('$UVP~biovolume~(mm^{3})$'))

uvp_ost_up / uvp_ost_down
```

@fig-biovolume-slope shows that there is no clear relationship between the biovolume seen by the UVP6 and the attenuation (continuous or stochastic) measured by the OST. For small particles, the relationship between the ATN slope and the average biovolume shows a weak correlation (0.30 at 500 m and 1000 m). For large particles, the OST misses particles that are seen by the UVP6 (see 500 m and 1000 m in the lower panel). It is coherent with the fact that OSTs were not designed to intercept and detect large sinking particles [@Estapa2023-oz]. The current version of the OST has a housing above the optical window which, together with the float itself, can produce a "self-shading" effect [@Estapa2017-jr] that prevents the collection of fast sinking particles (assumed here to be large particles). In addition, the nature of particles is also a source of uncertainty in the optical attenuation signal. Mathematically, the attenuation efficiency ($Q_{c}$) of a spherical particle can be defined [@Mobley2022-de] as

$$
Q_{c} = Q_{a} + Q_{b}
$$ {#eq-optics}

where $Q_{a}$ and $Q_{b}$ are, respectively, the absorption and scattering efficiencies. Consequently, @Mobley2022-de defines the attenuance cross section ($\sigma_{c}$) as $Q_{c}$ multiplied by the geometrical cross section of that particle ($\pi r^{2}$ for a spherical particle). Therefore, if a particle of a given size has a small $\sigma_{c}$ (e.g. mostly transparent) or a high $\sigma_{c}$ (e.g. opaque), the attenuation will be different (lower for a smaller $\sigma_{c}$) but the size measured by the UVP6 will be the same.

Therefore, if the UVP6 and the OST are well correlated at first order (@fig-comparison-ost-uvp, @tbl-correlation), it is more difficult to directly link the attenuance to the biovolume (hence the size) of a particle because of the nature of particles themselves. Overall, further exploration analyses are needed to better understand the particle dynamics and their composition, especially considering a broader range of trophic situations.

## Perspectives

### A complementary variable to investigate POC fluxes - $b_{bp}$

I did not use $b_{bp}$ data, a proxy for suspended particle concentration, that has already been used to derive POC concentrations, POC fluxes and particle size [@Briggs2013-hj; @Briggs2011-du; @Alkire2012-gv; @Cetinic2012-lz; @DallOlmo2014-ot; @Lacour2019-id; @Briggs2020-gz; @Giering2020-mc; @Rasse2017-tg]. In @Briggs2011-du, the authors explain that when a large particle goes through the sampling volume of the $b_{bp}$ sensor, it causes a spike in the $b_{bp}$ signal. However, the height of this spike depends on many factors such as the cross section of the particle (or aggregate), $Q_{b}$, the sampling volume, the fraction of the particle in the sampling volume and the time the particle spends in the sampling volume. Using an empirical relationship between ship-based $b_{bp}$ and POC, they were able to derive POC concentrations (mg C m$^{-3}$) and then POC fluxes (mg C m$^{-2}$ day$^{-1}$) by multiplying those concentrations by a given sinking speed (m day$^{-1}$).

As of today (March 15, 2023), 111361 $b_{bp}$ profiles were acquired by BGC-Argo floats. With the advent of floats equipped with both the UVP6 and a $b_{bp}$ sensor, it would be interesting to investigate the relationship between the size measured by the UVP6 and the size derived by spikes (i.e. spike height) in the $b_{bp}$ signal. Such an empirical relationship could bring information on the abundance of large particles at global scale using the huge, already available, $b_{bp}$ database.

### Configuration adaptation to specific BCP-related processes

The current configuration of all floats in the LS was a 10-day ascending profile acquisition with drifting at 200 m for one day, 500 m for 3 days and 1000 m for 5 days. During drifting, OST measurements were taken every 30 minutes at all depths whereas UVP measurements were taken every 20 minutes at 200 m and every 2h at 500 m and 1000 m. This initial configuration was based on a compromise between science targets and energy consumption limitations for long-term deployment (years). However, I think this configuration could be adapted depending on the period of the year to target specific processes. For instance, I tried to evaluate sinking speeds from particle concentrations using a gaussian fit in different water layers to follow the gaussian peak with time and depth for each size class. I managed to compute realistic sinking speeds but the uncertainty was huge and potentially biased because I was just trying to fit something I saw in the data. Therefore, I did not use those estimations here but I think it would be possible to compute them if the float was sampling every day when the abundance of particles is at its maximum (e.g. during the spring bloom). Similarly, to address the question of the vertical migration, a 10-day temporal resolution is way to low because plankton images are rare compared to detritus so the sampling volume should be greatly increased to have a better estimation of the presence (or absence) of zooplankton. Finally, it would perhaps be interesting to increase the time spent at 200 m (to 3 days) and decrease the time spent at 1000 m (to 3 days instead of 5) where the probability of capturing big particles is lower than at 200 m. There could also be UVP6 and OST measurements closer in time at all drifting depths to have a better comparison of attenuation slopes/jumps and biovolumes.

```{=html}
<!-- ### A statistical method to detect jumps instead of using a threshold

One fundamental difference between our jump detection method and the initial method based on an attenuance jump threshold [@Estapa2013-gw; @Estapa2017-jr; @Estapa2019-nk; @Estapa2023-oz] is that we use a statistical method to generalize the jump detection to new environments and new drifting depths. Initially, I used slope thresholds based on the 90$^{th}$ and 95$^{th}$ percentiles of the $c_{p}$ slope distribution. Therefore, I had three distinct slope thresholds for each drifting depth. We also thought that with time, this threshold should stabilize (i.e. having enough data points in the slope distribution to have stables percentiles). However, the rapid decrease in particle concentrations starting in November kept decreasing the threshold because the 90$^{th}$ and 95$^{th}$ percentiles were decreasing due the vanishing of large particles (@fig-uvp6-profiling-512-2500). One option was to stop the slope distribution in October, before large particles virtually disappeared. At that time, we therefore looked for a method that could derive a slope threshold for each drifting profile. I think it makes sense because it solved the issue of getting the best threshold that would be a unique one for all profiles and because the method is also relatively objective, easy to implement and resilient. By resilient, I mean that if the method is not perfect, it does not miss big jumps which are important for the correct estimation of $F_{large}$ and for data points wrongly identified as jumps (as outliers in the slope distribution), they do not contribute to $F_{large}$ because they have a small $c_{p}$ contribution and $F_{small}$ is generally not so impacted either because the contribution loss of that false jump is negligible to the slow accumulation on the transmissometer window. This can be checked on the web app mentonned earlier in the *drifting OST* tab. -->
```
