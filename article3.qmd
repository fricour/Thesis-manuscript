# Case study in the Labrador Sea {#case-study-labrador-sea}

## Introduction

introduire la notion de cycle de flotteurs BGC-Argo etc (+ parking depth)

-   dire que le but du papier est de faire une comparaison OST UVP qui sont deux instruments qui permettent de mesurer la meme chose (le flux des particules, l'abondance de particules) et donc le flux de carbone de manière indirecte papier GRL Lacour de 2015

## Methods

### Deployment and floats configuration

Two couples of BGC-Argo floats were deployed in the Labrador Sea in late May 2022. The first couple (WMO 6904240 and 4903634) was deployed on the 22$^{nd}$ at coordinates (-49.3°E, 58.9°N) while the second (WMO 6904241 and 1902578) was deployed on the 30$^{th}$ at coordinates (-52.4°E, 56.8°N).

All 4 floats used in this study are PROVOR Jumbo floats (NKE Instrumentation, France) equipped with a CTD SBE41-CP (Sea-Bird Scientific, USA), a UVP6-LP (Hydroptic, France), a c-Rover 660 nm beam transmissometer (WETLabs, USA) and a ECO FLBBCD (WETLabs, USA) that includes a chlorophyll-a (Chla) fluorometer (excitation at 470 nm, emission at 695 nm), a colored dissolved organic matter (CDOM) fluorometer (excitation at 370 nm, emission at 460 nm) and a 700 nm particle backscatteringmeter ($b_{bp}$). All floats but one (1902578) were equipped with a OCR-504 Multispectral Radiometer to measure the photosynthetic available radiation.

All 4 floats have been configured to drift at 3 distinct parking depths (respectively, 200 m, 500 m and 1000 m for approximately 1, 3 and 5 days) to study the downward carbon flux. During these drift phases, measurements of particles abundance (14 size classes, between 0.1 mm and 2.5 mm) are taken every 20 minutes at 200 m and every 2h at 500 m and 1000 m whereas $c_{p}$ measurements are taken every 30 minutes at all depths. This configuration can be adjusted during the mission of the float via two-way Iridium communication.

```{r, load libraries}
library(akima)
library(broom)
library(tidyverse)
library(cmocean)
library(lubridate)
library(scales)
library(patchwork)
library(viridis)
library(ggridges)
library(ggvis)
library(pracma)
library(gbm)
library(MLmetrics)
library(purrr)
library(arrow)
library(plotly)
library(oce)
library(ggplot2)
library(latex2exp)
library(cowplot)
library(RColorBrewer)
library(vroom)
library(castr)
library(reactablefmtr)
library(gt)
library(gtExtras)
```

```{r, map of 4 floats}
#| label: fig-float-position
#| fig-cap: Route of each float since their deployment (black triangles). Dots represent ascending profiles. The profile that concludes the time series for each float is highlighted by a black contour.

# read floats data
float_6904240 <- vroom('/home/flo/dox/ArgoShine/6904240_FromNetCDF.csv')
float_6904241 <- vroom('/home/flo/dox/ArgoShine/6904241_FromNetCDF.csv')
float_4903634 <- vroom('/home/flo/dox/ArgoShine/4903634_FromNetCDF.csv')
float_1902578 <- vroom('/home/flo/dox/ArgoShine/1902578_FromNetCDF.csv')

all_floats <- rbind(float_6904240, float_6904241, float_4903634, float_1902578)

all_floats <- all_floats |> mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
                                   park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))


argo_map <- all_floats |> select(WMO = wmo, cycle, lat, lon) |> drop_na(lat) |> dplyr::group_by(WMO, cycle) |> mutate(WMO = factor(WMO)) |>
  dplyr::summarise(lat = mean(lat), lon = mean(lon))

argo_map_first <- argo_map |> filter(cycle == 1)
argo_map_last <- argo_map |> filter(cycle == max(cycle))

world_data <- map_data("world")
w <- list(geom_polygon(aes(long, lat, group=group), data=world_data, fill="grey70"), coord_quickmap())

ggplot(argo_map) + w + geom_path(aes(lon, lat, colour = WMO), size = 1) +
  geom_point(aes(lon, lat, colour = WMO), size = 1.5) +
  scale_color_brewer(palette = 'Dark2') +
  theme_bw() +
  #scale_colour_manual(values = c('#a6cee3', '#33a02c', '#b2df8a', '#1f78b4')) +
  coord_cartesian(xlim = c(-55, -40), ylim = c(55,65)) + geom_point(data = argo_map_first, aes(lon, lat), shape = 17, size = 3) +
  geom_point(data = argo_map_last, aes(lon, lat, colour = WMO), size = 3) +
  geom_point(data = argo_map_last, aes(lon, lat, fill = WMO), size = 3, shape = 1) +
  labs(x = 'longitude', y = 'latitude') +
  #https://aosmith.rbind.io/2020/07/09/ggplot2-override-aes/#suppress-aesthetics-from-part-of-the-legend
  guides(color = guide_legend(override.aes = list(size = c(1.5,1.5,1.5,1.5),
                                                  shape = c(NA, NA, NA, NA))))
  
```

### Data processing

Data were downloaded from the Coriolis data center (biogeochemical data at ftp://ftp.ifremer.fr/ifremer/argo/dac/coriolis/ and UVP6 data at ftp://ftp.ifremer.fr/ifremer/argo/aux/coriolis/, last access: MONTH DAY, 2023). Chla data were quality controlled for Non-Photochemical Quenching (NPQ), a photoprotective mechanism that decreases the apparent fluorescence per unit of Chla following the method implemented by @Terrats2020 who modified the method of @Xing2018 for shallow-mixing cases. $c_{p}$ data were converted from counts to physical units (m$^{-1}$) using the following relationship

$$
c_{p}~[m^{-1}] = -\frac{1}{x}~log \left(~ \frac{c_{p}~[counts]-CSC_{dark}}{CSC_{cal}-CSC_{dark}} \right)
$$

where $x$, $CSC_{dark}$ and $CSC_{cal}$ are respectively the transmissometer pathlength (0.25 m), the sensor output when the beam is blocked (i.e. offset, 0 for each c-Rover of this study) and the sensor output with clean water in the path (12998, 13598, 13115 and 12855 for, respectively, float 4903634, 6904240, 6904241 and 1902578). The latter were obtained from the calibration sheets of each c-Rover.

UVP6 data and in particular the averaged number of particles per size class (i.e. particle size distribution hereafter referred to as PSD) was divided by the number of images at each depth and the water volume imaged by the UVP (0.7L) to obtain particle concentrations (units: #/L). Note that in an impending update by Coriolis, it will not be necessary to divide by the number of images anymore.

All data were divided into vertical profiles and drifting data using time stamps for drift. Finally, data were checked for instrumental anomalies. As a result, $c_{p}$ values from float 6904241 cannot be used from its 20$^{th}$ cycle onwards (\> November 17) due to a likely failure of the pump cleaning the optical window of the transmissometer.

### Derivation of carbon fluxes from OST

We applied a similar approach to @Estapa2013-gw and @Estapa2017-jr to derive the continuous component (i.e. continuous deposition of small particles on the upward-facing window of the transmissometer) and discontinuous (i.e. deposition of stochastic big particles) components of the downward POC flux, hereafter referred to as, respectively, $F_{small}$ and $F_{big}$.

First, the drifting $c_{p}$ signal was cleaned from spikes (hypothesized as transient particles not landing on the window or active swimmers [@Estapa2013-gw]). For this, we computed a 7-point moving median and a 7-point moving median absolute deviation (MAD). Then, we computed the anomaly to the moving median as the difference between the drifting $c_{p}$ signal and the moving median. We considered that a $c_{p}$ measurement was a spike when the anomaly was above $5.2 \times MAD$ following @Leys2013. Despiked $c_{p}$ data were then smoothed with a 7-point moving median filter.

Contrarily to previous works [@Estapa2013-gw; @Estapa2017-jr], we chose an empirical statistical approach instead of fixed thresholds to detect jumps. Therefore, we firstly computed the slope between each $c_{p}$ observation for each drifting phase and computed the z-score (i.e. standard score) of each of them. The detection of jumps therefore consisted in spotting outliers in the z-score distribution. For this, we computed the interquartile range ($IQR$) between the first ($Q_{1}$) and third ($Q_{3}$) quartiles. Positive and negative jumps were defined as, respectively, a z-score above $Q_{3} + 1.5 \times IQR$ and below $Q_{1} - 1.5 \times IQR$. We found that this method better detected outliers than the one used to despike the $c_{p}$ signal.

In order to compute $F_{small}$, we firstly computed the slope of each group of observations between jumps (positive and negative, see @fig-estapa-method_article). Undefined slopes (a single point between two jumps) were removed as well as any negative slope assuming that either the so-called fit failed (leading to undetected jumps) either the transmissometer window was perturbed by something else than the pump cleaning the optical window. Afterwards, we computed the weighted average slope ($slope_{w}$) using the previously computed slopes of each remaining group in order to derive the small particles flux using the relationship of @Estapa2022

$$
F_{small} = 633(0.25~slope_{w})^{0.77}
$$ {#eq-estapa}

where $slope_{w}$ is multiplied by the transmissometer pathlength due to the fact that @eq-estapa is an empirical POC-to-attenuance (ATN) relationship where ATN is the product of $c_{p}$ and the transmissometer pathlength.

We used the same relationship to compute $F_{big}$ by replacing $slope_{w}$ as the sum of positive jumps divided by the drifting time.

```{r, example of a cp signal analyzed for jumps (with an old method here)}
#| label: fig-estapa-method_article
#| fig-cap: Application of the method of @Estapa2017-jr on a drifting transmissometer to divide the $c_{p}$ signal into a continuous (dark blue) component and a discontinuous (positive jumps, red) component. Slopes for each group are depicted in light blue. A negative jump was also detected (yellow). Data from BGC-ARGO float WMO 6904240, drifting at 1000 m during its 21$^{st}$ cycle.
clean_cp_data <- function(data, moving_median_order){
  
  if(nrow(data) == 0){ # no cp data
    return(data)
  }else{
    
    # # Remove cp > 2 m-1 (don't really know why but eh)
    # if (any(data$cp > 2)){
    #   data <- data[-which(data$cp > 2),]
    # }
    
    # despike cp data
    data$cp <- oce::despike(data$cp, reference = 'median', k = 7, replace = 'NA')
    
    # median average
    data$cp <- pastecs::decmedian(data$cp, order = moving_median_order, ends = "fill") |> pastecs::extract(component = "filtered")
  }
  return(data)
  
}

tmp <- all_floats |> filter(wmo == 6904240, park_depth == '1000 m', cycle == 21, PhaseName == 'PAR') |> drop_na(cp) |> select(dates = juld, everything())

threshold <- 0.08

clean_data <- clean_cp_data(tmp, 3)

# save cleaned Cp signal from spikes to a temporary variable
tmp2 <- clean_data
tmp2$jump <- NA
# compute slope between two adjacent points (except first and last point of drifting time series)
for (i in 2:(nrow(tmp2)-1)){
  #print(i)
  delta_x <- as.numeric(difftime(tmp2$dates[i], tmp2$dates[i-1], units = 'days'))
  delta_y <- tmp2$cp[i]-tmp2$cp[i-1]
  tmp2$jump[i] <- delta_y/delta_x
}
  
# assign a colour for the plot
tmp2$colour <- 'base signal'
tmp2$colour[which(abs(tmp2$jump)>threshold)] <- 'jump' # abs is needed for 'negative jumps'
  
# add group to compute slope (for each subgroups of points, separated from a jump)
tmp2$group <- NA
  
# compute where the jumps are (index value of the array)
jump_index <- which(tmp2$colour == 'jump')
  
# assign group 
for (i in jump_index){
  for (j in 1:nrow(tmp2)){
    if ((j < i) & (is.na(tmp2$group[j]))){
      tmp2$group[j] <- paste0('group_',i)
    }
  }
}
  
# add last group (because it is AFTER the last index so it was not computed before)
tmp2$group[which(is.na(tmp2$group))] <- 'last_group'
  
# compute slope for each subgroups
slope_df <- tmp2 |> filter(colour == 'base signal', jump != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x)
  
# remove negative slope from the mean slope (no physical meaning)
slope_df <- slope_df |> filter(slope > 0)
  
# remove if only one point (cannot fit a slope with one point)
slope_df <- slope_df |> filter(nb_points > 1)
  
# compute mean slope 
#mean_slope <- mean(slope_df$slope)
mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
# estapa relationship (see POSTER)
#poc_flux <- 633*((mean_slope*0.25)**0.77) # *0.25 because ATN = cp*0.25
poc_flux <- 633*((mean_slope)**0.77) # /!\ slope for computed for ATN on y axis (delta_y *0.25 because ATN  = cp*0.25) -> should be OK
  
# build dataframe to plot each subgroup
part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
part_slope <- rbind(part1, part2)
  
# spot negative jump
tmp2$colour[which((tmp2$colour == 'jump') & (tmp2$jump < 0))]  <- 'negative jump'
  
# add big particles flux
rows_to_keep <- c(jump_index, jump_index-1) 
tmp3 <- tmp2[rows_to_keep,] |> select(dates, cp, jump, colour, group) |> dplyr::arrange(dates)
  
# remove negative jump, if any
check_colour <- unique(tmp3$colour)
if(length(check_colour) == 3){
  index_neg_jump <- which(tmp3$colour == 'negative jump')
  tmp3 <- tmp3[-c(index_neg_jump, index_neg_jump+1),]
  tmp3 <- tmp3 |> mutate(diff_jump = cp - lag(cp)) 
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else if(length(check_colour) == 2){
  tmp3 <- tmp3 |> mutate(diff_jump = cp - lag(cp)) 
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else{ # NO jump
  tmp3 <- NULL
  }
  
# big particles flux
if(is.null(tmp3)){ # no jumps
  big_part_poc_flux <- 0
}else{
  tmp4 <- tmp3 |> filter(diff_jump > 0)
  if(nrow(tmp4) == 0){ # no positive jumps
    big_part_poc_flux <- 0
  }else{
    delta_y <- sum(tmp4$diff_jump) *0.25 # to get ATN (= cp*0.25)
    max_time <- max(tmp2$dates)
    min_time <- min(tmp2$dates)
    delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
    slope_big_part <- delta_y/delta_x
    big_part_poc_flux <- 633*(slope_big_part**0.77)
  }
}
  
# compute total drifting time
max_time <- max(tmp2$dates)
min_time <- min(tmp2$dates)
drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))

check <- tmp2 |> select(dates, cp, colour)
check <- check |> filter(colour %in% c('jump', 'negative jump'))

# ggplot(clean_data, aes(x = dates, y = cp)) + geom_point(size= 3) + geom_point(data = tmp2, aes(x = dates, y = cp, colour = colour), size = 2) + 
#     scale_colour_manual(values = c('#003366','#E31B23', '#FFC325')) + 
#     scale_shape_manual(values = c(1,1,4)) +
#     geom_path(data = part_slope, aes(x = time, y = cp, group = group), colour = '#DCEEF3', size = 1) + theme_bw() + 
#     labs(x = 'Time', y = 'Cp (1/m)') +
#   geom_point(data = check, aes(x = dates, y = cp, colour = colour))

ggplot(tmp2, aes(x = dates, y = cp, colour = colour)) + geom_point(size = 3) +
    scale_colour_manual(values = c('#003366','#E31B23', '#FFC325')) + 
    #scale_shape_manual(values = c(1,1,4)) +
    geom_path(data = part_slope, aes(x = time, y = cp, group = group), colour = '#DCEEF3', size = 2) + theme_bw() + 
    labs(x = 'Time', y = TeX('$c_{p}~(m^{-1})$')) +
  geom_point(data = check, aes(x = dates, y = cp, colour = colour), size = 3) +
  geom_point(data = check, aes(x = dates, y = cp), shape = 1, size = 3, colour = 'black')  +
  #https://r-graph-gallery.com/239-custom-layout-legend-ggplot2.html
  # theme(legend.position = c(0.02,0.99), legend.justification = c('left', 'top'), 
  #       legend.title = element_blank())
    theme(legend.position = 'none')
```

## Results

### OST-derived carbon fluxes

```{r, function to detect and plot jumps}

plot_jumps <- function(data){
  
  # make sure that the cp signal is in chronological order
  tmp <- data |> arrange(dates)
  
  # despike cp data with a 7-point moving window
  tmp$cp <- despike(tmp$cp, k = 3)
  
  # smooth cp data with a 3-point moving median, n time(s)
  tmp$cp <- slide(tmp$cp, fun = median, k = 3, n = 1, na.rm=T)
  
  # compute slope between two adjacent points (except first point) # we could start after 1h to let the float stabilize
  delta_x <- as.numeric(tmp$dates - lag(tmp$dates), units = 'days')
  delta_y <- tmp$cp - lag(tmp$cp)
  tmp$slope <- delta_y/delta_x
  
  # compute a Z score (assuming a normal distribution of the slopes) on the slopes
  tmp <- tmp |> mutate(zscore = (slope - mean(slope, na.rm = T))/sd(slope, na.rm = T))
  
  # spot outliers on the Z score signal
  # interquartile range between Q25 and Q75 -> had to used that and not the despike function because slopes are often close (or equal) to 0 so it can miss clear jumps. Q25 and Q75 are more trustworthy in this case than the despike function of Jean-Olivier (see package castr on his github: https://github.com/jiho/castr)
  IQR <- quantile(tmp$zscore, probs = 0.75, na.rm=T) - quantile(tmp$zscore, probs = 0.25, na.rm=T)
  # outliers ('spikes' in the Z score signal)
  spikes_down <- tmp$zscore < quantile(tmp$zscore, 0.25, na.rm=T) - 1.5 *IQR
  spikes_up <-  tmp$zscore > quantile(tmp$zscore, 0.75, na.rm=T) + 1.5 *IQR
  spikes <- as.logical(spikes_down + spikes_up)
  
  # assign spikes
  tmp$spikes <- spikes
  
  # assign colour code to cp signal
  tmp$colour <- 'base signal' # base signal = smoothed despiked cp signal
  tmp[which(tmp$spikes == TRUE),]$colour <- 'jump'
  
  # add group to compute the slope of each group of points, separated by a jump
  tmp$group <- NA
  
  # index of jumps in the array
  jump_index <- which(tmp$colour == 'jump')
  
  # assign group identity to each group of points, separated by a jump (= subgroup)
  for (i in jump_index){
    for (j in 1:nrow(tmp)){
      if ((j < i) & (is.na(tmp$group[j]))){
        tmp$group[j] <- paste0('group_',i)
      }
    }
  }
  tmp$group[which(is.na(tmp$group))] <- 'last_group'
  
  # compute slope for each subgroup
  slope_df <- tmp |> filter(colour == 'base signal', slope != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), 
                                                                                                                  nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],
                                                                                                                  delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),
                                                                                                                  delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x) # *0.25 to convert cp to ATN
  
  # remove negative slope from the mean slope (no physical meaning)
  slope_df <- slope_df |> filter(slope > 0)
  
  # remove if only one point (cannot fit a slope with one point)
  slope_df <- slope_df |> filter(nb_points > 1)
  
  # compute weighted average slope (to take into account the fact that some subgroups might have 2 points and a high slope vs. large group of points with a small slope)
  mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
  # convert cp to POC using Estapa's relationship 
  poc_flux <- 633*(mean_slope**0.77) # /!\ slope computed for ATN on y axis (delta_y *0.25 because ATN = cp*0.25) -> should be OK
  
  # build dataframe to plot each subgroup
  part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
  part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
  part_slope <- rbind(part1, part2)
  
  # spot negative jump
  tmp$colour[which((tmp$colour == 'jump') & (tmp$slope < 0))]  <- 'negative jump'
  
  # add big particles flux to the party
  rows_to_keep <- c(jump_index, jump_index-1)
  tmp2 <- tmp[rows_to_keep,] |> select(dates, cp, slope, colour, group) |> arrange(dates)
  
  # remove negative jumps, if any
  check_colour <- unique(tmp2$colour)
  if(length(check_colour) == 3){ # there is at least a negative jump
    index_neg_jump <- which(tmp2$colour == 'negative jump')
    tmp2 <- tmp2[-c(index_neg_jump, index_neg_jump+1),]
    tmp2 <- tmp2 |> mutate(diff_jump = cp - lag(cp)) 
    even_indexes <- seq(2,nrow(tmp2),2)
    tmp2 <- tmp2[even_indexes,]
  }else if(length(check_colour) == 2){ # only positive jumps
    tmp2 <- tmp2 |> mutate(diff_jump = cp - lag(cp)) 
    even_indexes <- seq(2,nrow(tmp2),2)
    tmp2 <- tmp2[even_indexes,]
  }else{ # No jump
    tmp2 <- NULL
  }
  
  if(is.null(tmp2)){ # no jump
    big_part_poc_flux <- 0
    tmp3 <- NULL
  }else{
    tmp3 <- tmp2 |> filter(diff_jump > 0)
    if(nrow(tmp3) == 0){ # no positive jumps
      big_part_poc_flux <- 0
    }else{
      delta_y <- sum(tmp3$diff_jump) *0.25 # to get ATN (= cp*0.25)
      max_time <- max(tmp$dates)
      min_time <- min(tmp$dates)
      delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
      slope_big_part <- delta_y/delta_x
      big_part_poc_flux <- 633*(slope_big_part**0.77)
    }
  }
  
  # compute total drifting time
  max_time <- max(tmp$dates)
  min_time <- min(tmp$dates)
  drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))
  
  # to plot subgroups
  part_slope_tmp <- part_slope |> mutate(dates = time, colour = 'slope')
  
  # plot
  jump_plot <- plot_ly(tmp, x = ~dates, y = ~cp, type = 'scatter', mode = 'markers', color = ~colour, colors = c('#003366','#E31B23', '#FFC325')) |>
    add_lines(data= part_slope_tmp, x = ~dates, y = ~cp, split = ~group, color = I('#DCEEF3'), showlegend = F) |>
    layout(title= paste0('Drifting time: ', round(drifting_time,3), ' days\n',
                         'Mean ATN slope (light blue): ', round(mean_slope,3), ' day-1\n',
                         'POC flux (small particles): ', round(poc_flux,1), ' mg C m-2 day-1\n',
                         'POC flux (big particles): ', round(big_part_poc_flux,1), ' mg C m-2 day-1'), yaxis = list(title = 'Cp (1/m)'), xaxis = list(title = 'Time'))
  
  
  #return(jump_plot)
  #return(list('jump_plot' = jump_plot, 'jump_table' = tmp3))
  
    # adapt script to return big and small flux
  df <- tibble('max_time' = max_time, 'min_time' = min_time, 'small_flux' = poc_flux, 'big_flux' = big_part_poc_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],
               cycle = data$cycle[1])
  
  return(df)
  
}
```

```{r, compute Fsmall and F big}

# remove bad data from float 6904241
float_6904241_slope <- float_6904241 |> filter(cycle < 20)
# create dataframe for all drifting data (for all floats)
all_floats_slope <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
all_floats_slope <- all_floats_slope |> filter(PhaseName == 'PAR') |> drop_na(cp) |> select(depth = pres, cp, dates = juld, park_depth, wmo, cycle)

# start loop for all data
wmo <- c(4903634, 6904240, 6904241, 1902578)
park_depth <- c('200 m', '500 m', '1000 m')

res <- data.frame()
for (i in wmo){
  max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
  for (j in park_depth){
    for (k in seq(1:max_cycle)){
      tmp <- all_floats_slope |> filter(wmo == i, park_depth == j, cycle == k)
      if(nrow(tmp) == 0){
        next
      }else if(nrow(tmp) < 3){ # case where there is not enough data
        next
      }else{
          output <- plot_jumps(tmp)
          res <- rbind(res, output)
      }
    }
  }
}

# keep the good fluxes
cflux <- res |> mutate(drifting_time = difftime(max_time, min_time, units = 'days'), WMO = factor(wmo))

cflux_info_table <- cflux |> dplyr::group_by(park_depth) |> summarize(min_smallf = min(small_flux, na.rm=T), max_smallf = max(small_flux, na.rm=T),
                                                                mean_smallf = mean(small_flux, na.rm=T), median_smallf = median(small_flux, na.rm=T),
                                                                std_smallf = sd(small_flux, na.rm=T),
                                                                min_bigf = min(big_flux, na.rm=T), max_bigf = max(big_flux, na.rm=T),
                                                                mean_bigf = mean(big_flux, na.rm=T), median_bigf = median(big_flux, na.rm=T),
                                                                std_bigf = sd(big_flux, na.rm=T))

cflux$date <- as_date(cflux$min_time)

# # add levels
# cflux$park_depth <- factor(cflux$park_depth, levels = c('200 m', '500 m', '1000 m'))

```

$F_{small}$ and $F_{big}$ were computed at each drifting depth for all floats (@fig-OST-flux). At 200 m, $F_{small}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ (average $\pm$ standard deviation). At 500 m, $F_{small}$ is comprised between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_smallf, 1)` mg C m$^{2}$ day$^{-1}$. For both drifting depths, the trend shows a decrease of $F_{small}$ from June to January. At 1000 m, all floats measured an increase of $F_{small}$ from late May to mid-July followed by a steady decrease towards January, with $F_{small}$ ranging between `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ on average.

```{r, plot Fsmall and Fbig}
#| label: fig-OST-flux
#| fig-cap: OST-derived small (top) and big (bottom) particles flux for all floats at each drifting depth. Black lines and shaded areas represent, respectively, the monthly mean flux and the 95% confidence interval around the mean. 

up <- cflux |> ggplot(aes(x = date, y = small_flux)) +
    geom_point(data = cflux, aes(x = date, y = small_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = cflux, aes(x = date, y = small_flux), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = '', y = TeX('$F_{small}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  scale_y_continuous(trans = 'log10') +
  theme_bw() 

down <- cflux |> ggplot(aes(x = date, y = big_flux)) +
  geom_point(data = cflux, aes(x = date, y = big_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = cflux, aes(x = date, y = big_flux), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = 'Month', y = TeX('$F_{big}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  theme_bw() +
  scale_y_continuous(trans = 'log10') +
  theme(legend.position = 'none') 
  
up / down

```

$F_{big}$ was estimated at 200 m between 0 (no deposition of big particles on the transmissometer window) and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$. High fluxes (\> 100 mg C m$^{-2}$ day$^{-1}$) were measured by 3 out of 4 floats at the end of May before a sharp decrease towards July. At 500 m, $F_{big}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$, decreasing from September to January. At 1000 m, $F_{big}$ shows a similar pattern than the one observed for $F_{small}$ with an increase from June to mid-July, followed by a decrease towards January, with values ranging from 0 to `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$.

### UVP6 particle abundances

#### On drifting mode

```{r, uvp data}

lpm_class <- c("NP_Size_102", "NP_Size_128", "NP_Size_161",
                         "NP_Size_203", "NP_Size_256", "NP_Size_323",
                         "NP_Size_406", "NP_Size_512", "NP_Size_645",
                         "NP_Size_813", "NP_Size_1020", "NP_Size_1290",
                        "NP_Size_1630", "NP_Size_2050")

uvp_data <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date, cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))

uvp_data <- uvp_data |> pivot_longer(cols = lpm_class, names_to = 'size', values_to = 'conc') |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric())

# add level for order
#uvp_data$size_class <- factor(uvp_data$size_class, levels = lpm_class)
# uvp_data$park_depth <- factor(uvp_data$park_depth, levels = c('200 m', '500 m', '1000 m'))

# compute daily mean chla for each float at each cycle and at each drifting depth
mean_uvp_data <- uvp_data |> group_by(cycle, WMO, size, park_depth, juld) |> summarize(mean_conc = mean(conc, na.rm=T))
```

In this study, we focus on 14 size classes, from 102 $\mu$m to 2.5 mm. In @fig-uvp-200m, @fig-uvp-500m and @fig-uvp-1000m, we show the daily averaged concentrations of particles in each class size for each float at their respective drifting depth. At 200 m, it can be observed that particles concentrations in the 102 - 512 $\mu$m range are relatively uniform between June and October before decreasing sharply towards January. Between 512 $\mu$m and 1.29 mm, concentrations are steadily decreasing from June. Above 1.29 mm, the abundance of particles is already very low (\< 0.1 particle/L) in June and keeps decreasing over time.

```{r, uvp data at 200 m}
#| label: fig-uvp-200m
#| fig-cap: Daily averaged concentrations of particles abundance in 14 size classes measured by the UVP6 for each float at 200 m. Black lines and shaded areas represent, respectively, the daily mean particles abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particles abundance is maximum.

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp200m <- ggplot_build(p200m)

test <- tmp200m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> summarize(index_max = which.max(y), max_date = x[index_max]) |> mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p200m
```

At 500 m, particles concentrations in the 102 - 645 $\mu$ range slowly decrease from July to September with a sharp decrease starting in October. Above 645 $\mu$m, the trend shows a maximum between the end of July and the first half of August though the concentrations of particles in those classes are very low (\< 0.5 particle/L) on the entire timeserie.

```{r, uvp data at 500 m}
#| label: fig-uvp-500m
#| fig-cap: Daily averaged concentrations of particles abundance in 14 size classes measured by the UVP6 for each float at 500 m. Black lines and shaded areas represent, respectively, the daily mean particles abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particles abundance is maximum.

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp500m <- ggplot_build(p500m)

test <- tmp500m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> summarize(index_max = which.max(y), max_date = x[index_max]) |> mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p500m
```

At 1000 m, all 4 floats measured a clear maximum in particles abundance in the 102 - 645 $\mu$m in the second half of July. Above 645 $\mu$m, the same trend can still be observed with concentrations of less than 1 particle/L.

```{r, uvp data at 1000 m}
#| label: fig-uvp-1000m
#| fig-cap: Daily averaged concentrations of particles abundance in 14 size classes measured by the UVP6 for each float at 1000 m. Black lines and shaded areas represent, respectively, the daily mean particles abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particles abundance is maximum.
#| fig-dpi: 96

#{r, uvp data at 1000 m, out.width="900px", out.height="800px"}

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp1000m <- ggplot_build(p1000m)

test <- tmp1000m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> summarize(index_max = which.max(y), max_date = x[index_max]) |> mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p1000m
```

TODO : It should be noted that all floats measure the same trend -\> logique si on parle de la dynamique du bruit de fond de la zone et pas de la dynamique évènementielle qui serait différentes en fonction des flotteurs vu qu'ils dérivent chacun dans le parcours respectifs. Ça devrait renforcer cet aspect.

## Discussion

### Comparing OST and UVP data

For the first time, BGC-Argo floats equipped with both a transmissometer and a UVP6 have been deployed to better characterize the downward carbon flux and the nature of sinking particles from a multi-instrument approach [@Claustre2021-tx]. The quantification of the POC flux using the OST method [@Bishop2004-at; @Estapa2013-gw; @Estapa2017-jr] is based on an empirical POC:ATN ratio \[REF estapa 2022\] whereas UVP6 PSD measurements are used to compute POC fluxes using @eq-guidi-article3:

$$
F = \sum_{i}^{N}C_{i}Ad_{i}^{b}
$$ {#eq-guidi-article3}

where $C_{i}$, A and b represent, respectively, the concentration of particles (number/L) for the $i^{th}$ class of mean diameter $d_{i}$ (in mm) and 2 constants with $b$ being related to the fractal dimension ($D$) of an aggregate by $D = (b +1)/2$ [@Guidi2008-vp].

The use of @eq-guidi-article3 to derive carbon fluxes from PSD was firstly implemented by @Guidi2008-vp who used a minimization procedure between sediment traps data and estimated fluxes using PSD from several previous versions of the UVP to find the optimal value of $A$ and $b$ (i.e. 12.5 and 3.81). @Iversen2010-an and @Fender2019-ou followed the same procedure and concluded that the couple of parameters found by [@Guidi2008-vp] could not be used globally at the expense of greatly over- or underestimating POC fluxes, depending on the sampling region. In addition, the formulation of such a model implies that $A$ and $b$ would be valid at global scale, at all depths, for all UVP models and across all size classes which is arguable [@Bisson2022-bt]. @eq-guidi-article3 also assumes that the flux is only size-dependent though the main driver for the settling of aggregates is still debated [@Iversen2020-kb; @Iversen2022-qj]. As a result, the direct comparison of carbon fluxes derived from both OST and UVP data with @eq-guidi-article3 should be focused on the trend rather than on absolute carbon flux values.

```{r, compute guidi fluxes at parking depth}
#| label: fig-comparison-ost-uvp
#| fig-cap: Comparison of total carbon fluxes derived from the OST (black) and the UVP6 (red) where the error bars represent the 25$^{th}$ and 75^{th}$ percentiles around the median.

compute_flux <- function(A, b, PSD){ # PSD = Particle Size Distribution
  # for classes sizes between 0.102 mm and 1.5 mm
  #mid_ESD <- c(0.1164512, 0.1463600, 0.1843915, 0.2325200, 0.2933257, 0.3691650, 0.4650400, 0.5860455, 0.7385533, 0.9280422, 1.1705684, 1.4795321) # With Rainer's code (paruvpy)
  # mid_ESD <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685,
  #              0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
  #              1.16170742, 1.46365963) # With Lionel's script
  mid_ESD <- c(0.23051195, 0.29042685,
               0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
               1.16170742, 1.46365963) # With Lionel's script
  exp <- A * mid_ESD ** b
  estimated_flux <- rowSums(t(t(PSD)*exp))
  return(estimated_flux)
}

parking_data <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
parking_data <- parking_data |> filter(PhaseName == 'PAR') |> drop_na(NP_Size_102) |> select(park_depth, all_of(lpm_class), dates = juld, park_depth, wmo, cycle) |> mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
                                   park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))
parking_spectra <- parking_data |> dplyr::select(NP_Size_203:NP_Size_1290)

parking_data <- parking_data |> mutate(guidi_flux = compute_flux(12.5, 3.81, parking_spectra)) |> group_by(park_depth, cycle, wmo) #|> summarize(median_guidi_flux = median(guidi_flux))

# plot it
comparison_flux_ost_uvp <- merge(parking_data, cflux) |> mutate(total_flux = small_flux + big_flux)

# https://stackoverflow.com/questions/41077199/how-to-use-r-ggplot-stat-summary-to-plot-median-and-quartiles
# ggplot(comparison_flux_ost_uvp) + geom_line(aes(x = date, y = small_flux)) + facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) +
#   geom_errorbar(aes(x = date, y = guidi_flux),
#                       stat = 'summary',
#                       fun.min = function(z) {quantile(z, 0.25)},
#                       fun.max = function(z) {quantile(z, 0.75)},
#                       fun = median, colour = 'red', width = 5) +
#   geom_line(aes(x = date, y = guidi_flux),
#                       stat = 'summary',
#                       fun = median, colour = 'red') + scale_y_continuous(trans = 'log10') +
#   geom_line(data = comparison_flux_ost_uvp, aes(x = date, y = big_flux), linetype = 'dotdash') + theme_bw()

ggplot(comparison_flux_ost_uvp) + geom_line(aes(x = date, y = total_flux)) + facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) +
  geom_errorbar(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'red', width = 5) +
  geom_line(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      fun = median, colour = 'red') + scale_y_continuous(trans = 'log10') + theme_bw()


correlations <- comparison_flux_ost_uvp |> select(park_depth, wmo, cycle, guidi_flux, small_flux, big_flux, date, total_flux) |> group_by(wmo, park_depth, cycle) |> mutate(median_guidi_flux = median(guidi_flux)) |> group_by(park_depth, wmo) |> summarise(cor_small = round(cor(median_guidi_flux, small_flux),2), cor_big = round(cor(median_guidi_flux, big_flux),2), cor_total = round(cor(median_guidi_flux, total_flux),2))

correlations <- correlations |> pivot_wider(names_from = park_depth, values_from = c(cor_small, cor_big, cor_total))
gt(correlations) |> tab_spanner(label = "Small flux", columns = 2:4) |> tab_spanner(label = "Big flux", columns = 5:7) |> tab_spanner(label = 'Total flux', columns = 8:10) |>
  cols_label(wmo = 'WMO', `cor_small_200 m` = '200 m',
             `cor_small_500 m` = '500 m',
             `cor_small_1000 m` = '1000 m',
             `cor_big_200 m` = '200 m',
             `cor_big_500 m` = '500 m',
             `cor_big_1000 m` = '1000 m',
             `cor_total_200 m` = '200 m',
             `cor_total_500 m` = '500 m',
             `cor_total_1000 m` = '1000 m')

```

TODO :

-   TS diagram at drifting depth + isopycnals
-   satellite chla + MLD -\> show that conditions for bloom (stable and shallow MLD) + chla happened before deployment in the area
-   faire le modèle de pente avec les biovolues pour prédire quels types de particules explique le plus la pente -\> faire un modèle pour le small flux et un modèle pour le big flux, pas un modèle pour les small + big car les pentes ne sont pas additives
-   discuter des corrélations
-   science: on voit le système latent, on a raté le bloom
-   rajouter les vitesses de sédimentations (n'importe quelle méthode fera l'affaire) -> permet d'utiliser le graphique de l'UVP en vertical -> hop une justification en plus
-   rajouter dans les méthodes les produits satellites qu'on utilise
-   différentes dynamiques post-bloom
-   phénologie du bloom printanier

### Which particles drive the flux

```{r, flux model 2, include = FALSE}

# get spectra
uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))

flux_model <- function(boosting_rounds, which_flux){
    
    # select which flux to build the model
    if(which_flux == 'small'){
      cflux2 <- cflux |> mutate(flux = small_flux) 
    }else if(which_flux == 'big'){
      cflux2 <- cflux |> mutate(flux = big_flux)
    }else{
      cflux2 <- cflux |> mutate(flux = small_flux + big_flux)
    }
    
    # merge data for training
    data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)
    
  # training data
  tmp <- data_model |> select(flux, all_of(lpm_class))
  
  set.seed(101)
  # model
  model <- gbm(flux ~ ., data=tmp, distribution="gaussian",
  cv.folds=5,
  n.trees=boosting_rounds, shrinkage=0.005, interaction.depth=6,
  bag.fraction=0.5,
  n.cores=1
  )
   
  # get best model and thus best round number
  best <- gbm.perf(model)
  #print(best)
  
  # get features importance
  features_importance <- summary(model, n.trees=best, plotit = FALSE)
  
  
  return(list(features_importance = features_importance, model = model, best_tree = best))
   
}
  
# small_model <- flux_model(3000, 'small')
# big_model <- flux_model(3000, 'big')
# total_model <- flux_model(3000, 'total')
# 
# save(small_model, big_model, total_model, file = '/home/flo/dox/THESIS/DATA/MODEL/model_xgboost_data') 
  
#load(file = 'DATA/MODEL/model_xgboost_data')

```

#### On profiling mode

Cette partie devrait venir dans la discussion si on part du principe que la première partie est liée à la méthodologie

@fig-uvp-vertical-profiles shows that all floats measure similar patterns in the vertical profiles of particles concentrations in all size classes. In the 102-512 $\mu$m range, all 4 floats measured a local maximum between 650 m and 1050 m. Above 512 $\mu$m, the relative abundance is maximum along a depth track centered on August. It can be seen among all size classes that the water column is being drained proportionally to the class size.

```{r, uvp_data on the vertical}
#| label: fig-uvp-vertical-profiles
#| fig-cap: Normalized concentrations of particles for all UVP6 vertical profiles. Plots were arranged to respect the deployment of each couple of floats (e.g. the first two rows of plots are related to co-deployed floats 6904240 and 4903634). The colorbar depicts the log$_{10}$ of normalized concentrations. Grey areas indicate that particles were not measured in the size range of interest.

particles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class), MLD) |> drop_na(NP_Size_102) |> mutate(juld = as_date(juld))

particles_bis <- particles |> pivot_longer(cols = all_of(lpm_class), names_to = 'size', values_to = 'concentration') |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric())
#particles_bis$size <- factor(particles_bis$size, levels = lpm_class)

# rel_conc
particles_bis <- particles_bis |> group_by(wmo, size) |> mutate(rel_conc = (concentration-min(concentration))/(max(concentration) - min(concentration)))

# order floats by couple of deployment
# particles_bis$wmo <- factor(particles_bis$wmo, levels = c(6904240,4903634,6904241,1902578))

# stats on the local maximum
# test <- particles_bis |> filter(depth > 500) |> group_by(wmo, cycle, size) |> # find depth of maximum relative abundance below 500 m between June and September
#   summarize(index_max = which.max(rel_conc), max_depth = depth[index_max], date = unique(juld)) |> 
#   filter(size < 512, date < '2022-10-01', date > '2022-05-31') |> # compute mean depth for each class size for each float
#   group_by(wmo, size) |> summarize(mean_depth = mean(max_depth))

# plot concentration
# particles_bis |> filter(depth < 200) |>
#   ggplot() + facet_grid(wmo~size) + theme_bw() +
#   geom_point(aes(x=juld, y=depth, colour=rel_conc), size=1, shape=15) +
#   #scale_colour_viridis(option = 'H', limits = c(-1,1), oob = scales::squish, na.value = 'transparent') +
#   scale_colour_viridis(option = 'H') +
#   scale_y_reverse() +
#   scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
#   labs(x = 'Month', y = 'Depth (m)') +
#   geom_path(data = particles_bis, aes(juld, MLD), colour = "white")

## plot absolute concentrations
up_part <- particles_bis |> filter(depth < 2000, size < 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  #geom_point(aes(x=juld, y=depth, colour=rel_conc), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + # A or F
  #scale_colour_viridis(option = 'A', limits = c(-2,-1), oob = scales::squish, na.value = 'grey', direction = -1) + # A or F
  #scale_colour_viridis(option = 'H') +
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  #labs(colour = 'Log10 normalized abundance)', x = 'Month', y = 'Depth (m)') +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)') #+
  #geom_hline(yintercept = c(600, 1100), linetype = 'dotted')
  #theme(legend.title = element_text(angle = -90))
  #guides(fill = guide_legend(title.theme = element_text(size = 200)))
  #+ theme(legend.position = 'none')
  #geom_path(data = particles_bis, aes(juld, MLD), colour = "black") +
  #theme(legend.position = 'none') #+
  #geom_hline(yintercept = c(200,500,1000), linetype = 'dotted') #+
  #guides(colour = guide_legend(title.position = "bottom"))

#Normalized concentrations (#/L)

up_part

down_part <- particles_bis |> filter(depth < 2000, size >= 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  #geom_point(aes(x=juld, y=depth, colour=rel_conc), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) +
  #scale_colour_viridis(option = 'H') +
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  #labs(colour = 'Log10 normalized abundance)', x = 'Month', y = 'Depth (m)') +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)')

down_part
```

### Environmental conditions prior to bloom

 + aller chercher les produits satellitaires pour montrer que l'on a bien raté le bloom ou que les conditions du bloom n'étaient pas réunies

```{r, chla corrected for NPQ and MLD}

# fetch data
npq_data <- vroom('DATA/ARGO/npq_corrected_data.csv')  

# clean and smoothed chla data
d <- npq_data |> select(cycle, MLD, pres, juld, wmo, chla_npq) |> mutate(date = as_date(juld))
d <- d |> group_by(date, wmo, pres, cycle) |> summarize(mean_chla = mean(chla_npq, na.rm=T), mld = unique(MLD))
d <- d |> mutate(smoothed_chla = smooth(mean_chla))

#d$wmo <- factor(d$wmo, levels = c(6904240,4903634,6904241,1902578))

# plot it
ggplot(d) + geom_point(aes(x = date, y = pres, colour = log10(smoothed_chla))) +
  scale_y_reverse(limits = c(200,0)) +
  #scale_colour_viridis(option = 'A') +
  scale_colour_cmocean(name = 'algae', oob = scales::squish, limits = c(-1,1), na.value = 'grey') +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  labs(x = 'Time', y = 'NPQ-corrected Chla') + facet_wrap(~wmo) + theme_bw() +
  geom_line(aes(x = date, y = mld), colour = 'black', size = 1)

# compute chla integration in the MLD and in the 0-100 m zone
d_chla <- d |> group_by(wmo, date) |> summarize(chla_0_100m = integrate(smoothed_chla, pres, from=0, to=100),
                                       mld = unique(mld),
                                       chla_0_mld = integrate(smoothed_chla, pres, from = 0, to=mld))
# 
# ggplot(d_chla) + geom_point(aes(date, chla_0_100m)) + facet_wrap(~wmo, scales = 'free') + theme_bw() + labs(x = 'Month', y = 'Integrated Chla 0-100 m (CHLA UNITS)')
#ggplot(d_chla) + geom_point(aes(date, chla_0_mld)) + facet_wrap(~wmo, scales = 'free')

```

```{r, chla and MLD from satellite}

chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_with_clouds_sat.feather')
#chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_without_clouds_sat.feather')
mld_sat <- arrow::read_feather('DATA/SATELLITE/mld_sat.feather') |> mutate(lat = latitude, lon = longitude - 360) |> select(time, lat, lon, mld = mlotst)
  
# CHLA
x <- 1
y <- 1
chla_cover_deployment1 <- chla_sat |> filter(lat > 58.9 - y, lat < 58.9 + y, lon > -49.3 - x, lon < -49.3 + x) |> mutate(date = as_date(time), group = 'couple 1') |> mutate(deploy = as_date('2022-05-22'))
chla_cover_deployment2 <- chla_sat |> filter(lat > 56.7 - y, lat < 56.7 + y, lon > -52.3 - x, lon < -52.3 + x) |> mutate(date = as_date(time), group = 'couple 2') |> mutate(deploy = as_date('2022-05-30'))
chla_cover <- rbind(chla_cover_deployment1, chla_cover_deployment2)

ggplot(chla_cover) + geom_errorbar(aes(x = date, y = CHL),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'black', width = .5) + theme_bw() +
  labs(x = 'Time', y = 'Chla')+ facet_wrap(~group) +
  geom_vline(data = chla_cover, aes(xintercept = deploy), colour = "red")

# MLD
mld_cover_deployment1 <- mld_sat |> filter(lat > 58.9 - y, lat < 58.9 + y, lon > -49.3 - x, lon < -49.3 + x) |> mutate(date = as_date(time), group = 'couple 1') 
mld_cover_deployment2 <- mld_sat |> filter(lat > 56.7 - y, lat < 56.7 + y, lon > -52.3 - x, lon < -52.3 + x) |> mutate(date = as_date(time), group = 'couple 2')
mld_cover <- rbind(mld_cover_deployment1, mld_cover_deployment2)

ggplot(mld_cover) + geom_errorbar(aes(x = date, y = mld),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'black', width = 5) +
  labs(x = 'Time', y = 'MLD (m)') + facet_wrap(~group, scales = 'free_y')

```



### Processes at play

```{r, monthly anomaly between drifting at 500 and 1000 m}
#| label: fig-processes-at-play
#| fig-cap: Ratio of monthly mean particles concentrations during drifting (1000 m over 500 m) for size classes below 1.63 mm. 

tmp <- uvp_data |> filter(park_depth != '200 m') |> group_by(wmo, size, month, park_depth) |> 
  summarize(monthly_mean_conc = mean(conc), monthly_median_conc = median(conc)) |>
  # compute concentration anomaly at 500 and 1000 m
  mutate(diff_anomaly = monthly_mean_conc - lead(monthly_mean_conc), div_anomaly = monthly_mean_conc/lead(monthly_mean_conc)) |> drop_na(diff_anomaly) |> #filter(is.finite(div_anomaly)) |>
  select(wmo, size, month, diff_anomaly, div_anomaly) |> mutate(month = month.abb[month]) |> 
  mutate(size = as.character(size)) |>
  mutate(rev_div_anomaly = 1/div_anomaly) |> 
  #filter(is.finite(rev_div_anomaly)) |>
  mutate(diff_anomaly = round(diff_anomaly, 1), rev_div_anomaly = round(rev_div_anomaly, 1), div_anomaly = round(div_anomaly, 1)) |>
  ungroup()

# test en weekly
# tmp <- uvp_data |> filter(park_depth != '200 m', month != 5) |> group_by(wmo, size, week, park_depth) |> 
#   summarize(monthly_mean_conc = mean(conc), monthly_median_conc = median(conc)) |>
#   # compute concentration anomaly at 500 and 1000 m
#   mutate(diff_anomaly = monthly_mean_conc - lead(monthly_mean_conc), div_anomaly = monthly_mean_conc/lead(monthly_mean_conc)) |> drop_na(diff_anomaly) |> #filter(is.finite(div_anomaly)) |>
#   select(wmo, size, week, diff_anomaly, div_anomaly) |> 
#   mutate(size = as.character(size)) |>
#   mutate(rev_div_anomaly = 1/div_anomaly) |> 
#   #filter(is.finite(rev_div_anomaly)) |>
#   mutate(diff_anomaly = round(diff_anomaly, 1), rev_div_anomaly = round(rev_div_anomaly, 1)) |>
#   ungroup()
# 
# create_data_for_heatmap <- function(float){
#   y <- unique(tmp$size)[1:12] # above, data is way too low
#   d <- tmp |> filter(wmo == float, size %in% y, week > 5)
#   x <- unique(d$week)
#   data <- expand.grid(X=x, Y=y)
#   #data$Z <- tt$diff_anomaly
#   data$Z <- d$rev_div_anomaly
#   data$wmo <- float
#   return(data)
# }

# d1 <- create_data_for_heatmap(6904241)
# d2 <- create_data_for_heatmap(1902578)
# d3 <- create_data_for_heatmap(4903634)
# d4 <- create_data_for_heatmap(6904240)
# 
# dd <- rbind(d1, d2, d3, d4)

# ggplot(dd, aes(X, Y, fill= Z)) + 
#   geom_tile() + scale_fill_viridis() + facet_wrap(~wmo)

create_data_for_heatmap <- function(float, df){
  y <- unique(df$size)[1:12] # above, data is way too low
  d <- df |> filter(wmo == float, size %in% y)
  #x <- c('Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan')
  x <- unique(d$month)
  data <- expand.grid(X=x, Y=y)
  #data$Z <- tt$diff_anomaly
  data$Z <- d$rev_div_anomaly
  #data$Z <- d$div_anomaly
  data$wmo <- float
  return(data)
}

# for some reason, there is some bug with the quarto compilation .. so I save the tibble .. and reload it 
# d1 <- create_data_for_heatmap(6904241, tmp)
# d2 <- create_data_for_heatmap(1902578, tmp)
# d3 <- create_data_for_heatmap(4903634, tmp)
# d4 <- create_data_for_heatmap(6904240, tmp)
# dd <- rbind(d1, d2, d3, d4)
# vroom_write(dd, 'heatmap_data.csv')

# reload + redo some annoying stuff that were lost during the writing of the csv
dd <- vroom('heatmap_data.csv') |> mutate(Y = as.character(Y)) 
dd$X <- factor(dd$X, levels = c('May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan'))
dd$Y <- factor(dd$Y, levels = c('102', '128', '161', '203', '256', '323', '406', '512', '645', '813', '1020', '1290'))
dd$Y <- factor(dd$Y, levels = c('1290', '1020', '813', '645', '512', '406', '323', '256', '203', '161', '128', '102'))
# dd$wmo <- factor(dd$wmo, levels = c(6904240,4903634,6904241,1902578))

#dd <- dd |> mutate(Z_binned = cut(Z, breaks = c(0,1,1.5,2.5,max(Z))))
dd <- dd |> mutate(Z_binned = cut(Z, breaks = c(0,.5,1,1.5,2,max(Z))))

# ggplot(dd) +
#   geom_tile(aes(X, Y, fill = Z), alpha = 0.75) + scale_fill_viridis() + theme_bw() + labs(x = 'Month', y = 'Size class') +
#   facet_wrap(~wmo) +
#   geom_text(data = dd, aes(X, Y, label = Z))

ggplot(dd) + 
  geom_tile(aes(X, Y, fill = Z_binned), colour = 'black', alpha = 1) + theme_bw() + labs(fill = 'Ratio', x = 'Month', y = 'Size class') +
  facet_wrap(~wmo) + scale_fill_viridis_d(option = 'A')

# plot_heatmap <- function(float){
#   y <- unique(tmp$size)[1:12]
#   tt <- tmp |> filter(wmo == float, size %in% y) 
#   x <- c('Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan')
#   data <- expand.grid(X=x, Y=y)
#   #data$Z <- tt$anom
#   data$Z <- tt$diff_anomaly
#   data$Z <- tt$rev_div_anomaly
# 
#   p <- ggplot(data, aes(X, Y, fill= Z)) + 
#     geom_tile() + scale_fill_viridis() +
#     geom_text(data = data, aes(X, Y, label = Z)) +
#     labs(title = float)
#   
#   return(p)
# }
# 
# p1 <- plot_heatmap(4903634)
# p2 <- plot_heatmap(6904240)
# p3 <- plot_heatmap(1902578)
# p4 <- plot_heatmap(6904241)
# 
# (p1 | p2) / 
# (p3 | p4)

# # https://kcuilla.github.io/reactablefmtr/articles/reactablefmtr_cookbook.html
# tt <- tt |> pivot_wider(names_from = month, values_from = anom) |> mutate(size = as.character(size))
# reactable(
#   tt,
#   pagination = FALSE,
#   showSortIcon = FALSE,
#   theme = void(
#     centered = TRUE, 
#     cell_padding = 0,
#     header_font_color = 'black',
#     font_color = 'black'
#     ),
#   defaultColDef = colDef(
#     maxWidth = 50,
#     align = 'center',
#     cell = tooltip(),
#     style = color_scales(
#       data = tt,
#       span = 2:8,
#       colors = c('#002347','#003366','#003F7D','#FF8E00','#FD7702','#FF5003'),
#       bias = 1.4,
#       opacity = 0.9,
#       show_text = FALSE
#     )
#   ),
#   columns = list(
#     total = colDef(
#       maxWidth = 225,
#       cell = data_bars(
#         data = tt,
#         fill_color = c('#002347','#003366','#003F7D','#FF8E00','#FD7702','#FF5003'),
#         bias = 1.4,
#         fill_opacity = 0.9,
#         background = 'transparent',
#         bar_height = 40,
#         text_position = 'center'
#         ),
#       style = list(borderLeft = "2px solid #999999")
#     )
#   )
# ) 

```

```{r, test gt table}

monthly_mean <- uvp_data |> mutate(month = month.abb[month(juld)]) |> group_by(size, park_depth, month) |> summarize(mean_conc = round(mean(conc, na.rm=T), 2)) |> ungroup()

# pivot wider
monthly_mean <- monthly_mean |> pivot_wider(names_from = month, values_from = mean_conc) |> select(size, depth = park_depth, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, Jan) 

monthly_mean |> group_by(size) |> gt(rowname_col = 'depth')

#monthly_mean |> gt(rowname_col = c('size')) |> tab_stubhead(label = 'Class') 

```

### Chlorophyll data

```{r, chla data}

# argo data
npq_data <- vroom('DATA/ARGO/npq_corrected_data.csv')  

# clean and smoothed chla data
d <- npq_data |> select(cycle, MLD, pres, juld, wmo, chla_npq) |> mutate(date = as_date(juld))
d <- d |> group_by(date, wmo, pres, cycle) |> summarize(mean_chla = mean(chla_npq, na.rm=T), mld = unique(MLD))
d <- d |> mutate(smoothed_chla = smooth(mean_chla))

d$wmo <- factor(d$wmo, levels = c(6904240,4903634,6904241,1902578))

# plot it
ggplot(d) + geom_point(aes(x = date, y = pres, colour = log10(smoothed_chla))) +
  scale_y_reverse(limits = c(200,0)) +
  #scale_colour_viridis(option = 'A') +
  scale_colour_cmocean(name = 'algae', oob = scales::squish, limits = c(-1,1), na.value = 'grey') +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  labs(x = 'Time', y = 'NPQ-corrected Chla') + facet_wrap(~wmo) + theme_bw()

# compute chla integration in the MLD and in the 0-100 m zone
d_chla <- d |> group_by(wmo, date) |> summarize(chla_0_100m = integrate(smoothed_chla, pres, from=0, to=100),
                                       mld = unique(mld),
                                       chla_0_mld = integrate(smoothed_chla, pres, from = 0, to=mld))

ggplot(d_chla) + geom_point(aes(date, chla_0_100m)) + facet_wrap(~wmo, scales = 'free') + theme_bw() + labs(x = 'Month', y = 'Integrated Chla 0-100 m (CHLA UNITS)')
#ggplot(d_chla) + geom_point(aes(date, chla_0_mld)) + facet_wrap(~wmo, scales = 'free')

```

### Mixed layer depth

```{r, mld data}
ggplot(d_chla) + geom_point(aes(date, mld)) + facet_wrap(~wmo, scales = 'free') + theme_bw() + labs(x = 'Month', y = 'Mixed layer depth (m)')
```

### Flux model(s)

```{r, flux model, include = FALSE}

# get spectra
uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))

flux_model <- function(boosting_rounds, which_flux){
    
    # select which flux to build the model
    if(which_flux == 'small'){
      cflux2 <- cflux |> mutate(flux = small_flux) 
    }else if(which_flux == 'big'){
      cflux2 <- cflux |> mutate(flux = big_flux)
    }else{
      cflux2 <- cflux |> mutate(flux = small_flux + big_flux)
    }
    
    # merge data for training
    data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)
    
  # training data
  tmp <- data_model |> select(flux, all_of(lpm_class))
  
  set.seed(101)
  # model
  model <- gbm(flux ~ ., data=tmp, distribution="gaussian",
  cv.folds=5,
  n.trees=boosting_rounds, shrinkage=0.005, interaction.depth=6,
  bag.fraction=0.5,
  n.cores=1
  )
   
  # get best model and thus best round number
  best <- gbm.perf(model)
  #print(best)
  
  # get features importance
  features_importance <- summary(model, n.trees=best, plotit = FALSE)
  
  
  return(list(features_importance = features_importance, model = model, best_tree = best))
   
}
  
# small_model <- flux_model(3000, 'small')
# big_model <- flux_model(3000, 'big')
# total_model <- flux_model(3000, 'total')
# 
# save(small_model, big_model, total_model, file = '/home/flo/dox/THESIS/DATA/MODEL/model_xgboost_data') 
  
load(file = 'DATA/MODEL/model_xgboost_data')

```

```{r, model predictions}

model_predict <- function(which_flux, mod, best_tree_number){
    
    # get spectra
    uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))
    
    # select which flux to build the model
    if(which_flux == 'small'){
      cflux2 <- cflux |> mutate(flux = small_flux) 
    }else if(which_flux == 'big'){
      cflux2 <- cflux |> mutate(flux = big_flux)
    }else{
      cflux2 <- cflux |> mutate(flux = small_flux + big_flux)
    }
    
    # merge data for training
    data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)

  # predictions on mean spectra values
  uvp_spectra_average <- data_model |> dplyr::group_by(wmo, cycle, depth) |> dplyr::summarize(depth = unique(depth), true_flux = unique(flux),
                                                                                   NP_Size_102 = mean(NP_Size_102),
                                                                                   NP_Size_128 = mean(NP_Size_128),
                                                                                   NP_Size_161 = mean(NP_Size_161),
                                                                                   NP_Size_203 = mean(NP_Size_203),
                                                                                   NP_Size_256 = mean(NP_Size_256),
                                                                                   NP_Size_323 = mean(NP_Size_323),
                                                                                   NP_Size_406 = mean(NP_Size_406),
                                                                                   NP_Size_512 = mean(NP_Size_512),
                                                                                   NP_Size_645 = mean(NP_Size_645),
                                                                                   NP_Size_813 = mean(NP_Size_813),
                                                                                   NP_Size_1020 = mean(NP_Size_1020),
                                                                                   NP_Size_1290 = mean(NP_Size_1290),
                                                                                   NP_Size_1630 = mean(NP_Size_1630),
                                                                                   NP_Size_2050 = mean(NP_Size_2050)) |> ungroup()


  # prediction at 1000 m
  #spectra <- uvp_spectra_average |> select(true_flux, all_of(lpm_class[]))
  
  # prediction 
  pred_flux <- predict(mod, newdata = uvp_spectra_average, n.trees = best_tree_number)
  
  uvp_spectra_average$pred_flux <- pred_flux
  
  return(uvp_spectra_average)
}

# pred_small <- model_predict('small', small_model$model, small_model$best_tree) |> select(wmo, cycle, depth, true_small = true_flux, pred_small = pred_flux)
# pred_big <- model_predict('big', big_model$model, big_model$best_tree) |> select(true_big = true_flux, pred_big = pred_flux)
# pred_total <- model_predict('total', total_model$model, total_model$best_tree) |> select(true_total = true_flux, pred_total = pred_flux)
# 
# all_predictions <- cbind(pred_small, pred_big, pred_total) |> mutate(pred_total2 = pred_small + pred_big, anom = pred_total - pred_total2)
# 
# vroom::vroom_write(all_predictions, file = '/home/flo/dox/THESIS/DATA/MODEL/all_predictions.csv')
all_predictions <- vroom('DATA/MODEL/all_predictions.csv')

```

#### Features importance

```{r, features importance}

features <- small_model$features_importance |> select(size = var, importance = rel.inf) |> mutate(model = 'small') |>
  bind_rows(big_model$features_importance |> select(size = var, importance = rel.inf) |> mutate(model = 'big')) |>
  bind_rows(total_model$features_importance |> select(size = var, importance = rel.inf) |> mutate(model = 'total'))

rownames(features) <- NULL

features <- features |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric())

ggplot(features) + geom_point(aes(x = as.factor(size), y = importance, colour = model), size = 5) + labs(x = 'Size', y = 'Features importance')

```

#### Predictions

```{r, plot predictions}

p_small <- ggplot(all_predictions) + geom_point(aes(true_small, pred_small)) + xlim(c(0,25)) + ylim(c(0,25)) + theme_bw() +

  annotate("text", x = 3, y = 23, label = paste0('R² = ' , round(R2_Score(all_predictions$true_small, all_predictions$pred_small), 2)))

p_big <- ggplot(all_predictions) + geom_point(aes(true_big, pred_big)) + xlim(c(0,50)) + ylim(c(0,50)) + theme_bw() +

  annotate("text", x = 5, y = 45, label = paste0('R² = ' , round(R2_Score(all_predictions$true_big, all_predictions$pred_big), 2)))

p_total <- ggplot(all_predictions) + geom_point(aes(true_total, pred_total)) + xlim(c(0,100)) + ylim(c(0,100)) + theme_bw() +

  annotate("text", x = 3, y = 80, label = paste0('R² = ' , round(R2_Score(all_predictions$true_total, all_predictions$pred_total), 2)))

(p_small + p_big) / p_total

# plot(test$true_flux, y=test$pred_flux, asp=1)

# print(R2_Score(test$pred_flux, test$true_flux))

```

#### Apply the total flux model on all vertical profiles

```{r, apply model to vertical profile}

vertical_profiles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class)) |> drop_na(NP_Size_102) |> mutate(juld = as_date(juld))

# make predictions

small_predicted_flux <- predict(small_model$model, newdata = vertical_profiles, n.trees = small_model$best_tree)

big_predicted_flux <- predict(big_model$model, newdata = vertical_profiles, n.trees = big_model$best_tree)

total_predicted_flux <- predict(total_model$model, newdata = vertical_profiles, n.trees = total_model$best_tree)

# add predictions to vertical data

vertical_profiles <- vertical_profiles |> mutate(small_pred = small_predicted_flux, big_pred = big_predicted_flux, total_pred = total_predicted_flux)

vertical_profiles$wmo <- factor(vertical_profiles$wmo, levels = c(6904240,4903634,6904241,1902578))

# pv_small <- ggplot(vertical_profiles, aes(x = juld, y = depth, colour = log10(small_pred))) + geom_point() + scale_color_viridis(option = 'H') +

#   scale_y_reverse(limits = c(500,0)) +

#   scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

#   labs(x = 'Time', y = 'Predicted Fsmall') + facet_wrap(~wmo)

#

# pv_big <- ggplot(vertical_profiles, aes(x = juld, y = depth, colour = log10(big_pred))) + geom_point() + scale_color_viridis(option = 'H') +

#   scale_y_reverse(limits = c(500,0)) +

#   scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

#   labs(x = 'Time', y = 'Predicted Fbig') + facet_wrap(~wmo)

pv_total <- ggplot(vertical_profiles, aes(x = juld, y = depth, colour = total_pred)) + geom_point() + #scale_color_viridis(option = 'H') +

  scale_y_reverse(limits = c(2000,0)) +

  scale_colour_viridis(option = 'A', limits = c(quantile(all_predictions$true_total, probs = 0.25), quantile(all_predictions$true_total, probs = 0.75)), oob = scales::squish, na.value = 'grey', direction = -1) +

  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

  labs(x = 'Time', y = 'Predicted Ftotal') + facet_wrap(~wmo) + theme_bw()

pv_total

ggplot(vertical_profiles, aes(x = juld, y = depth, colour = total_pred)) + geom_point() + #scale_color_viridis(option = 'H') +

  scale_y_reverse(limits = c(500,0)) +

  scale_colour_viridis(option = 'A', limits = c(20,100), oob = scales::squish, na.value = 'grey', direction = -1) +

  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

  labs(x = 'Time', y = 'Predicted Ftotal') + facet_wrap(~wmo) + theme_bw()

```

```{r, flux distribution}

depth_bin <- c(seq(0,500,50), seq(600, 1000, 100), seq(1250, 2000, 250))

flux_distrib <- vertical_profiles |> filter(depth < 2000) |> select(juld, depth, wmo, total_pred) |> mutate(depth_bin = cut(depth, breaks = depth_bin)) |>

  mutate(month = month.abb[month(juld)]) |> ungroup() #|> filter(month != 'May') # I filtered May because I had some troubles.. I need to fill some cases unseen by the float (eg. when they start profiling until 1000 m then go to 2000 -> ça fout un peu la merde..)

tmp <- flux_distrib |> group_by(wmo, depth_bin, month) |> summarize(mean_flux = mean(total_pred)) |>

  mutate(cut_flux = cut(mean_flux, breaks = c(0,10,20,30,40,50,75,100,+Inf))) |> ungroup()

create_data_for_heatmap_bis <- function(float, df){

  # df <- tmp |> filter(wmo == float)

  # y <- unique(df$depth_bin)

  # if(float == 1902578){

  #   y <- y[1:15]

  # }

  # x <- unique(df$month)

  # data <- expand.grid(X=x, Y=y)

  # data$Z <- df$cut_flux

  # data$wmo <- float

  # data$month <- df$month

  df <- df |> filter(wmo == float) |> select(cut_flux, depth_bin, month)

  y <- unique(df$depth_bin)

  df <- df|> pivot_wider(names_from = depth_bin, values_from = cut_flux)

  x <- unique(df$month)

  data <- expand.grid(X=x, Y=y)

  data$Z <- as.vector(as.matrix(df |> select(-month)))

  data$wmo <- float

  return(data)

}

d1 <- create_data_for_heatmap_bis(6904241, tmp)

d2 <- create_data_for_heatmap_bis(1902578, tmp)

d3 <- create_data_for_heatmap_bis(4903634, tmp)

d4 <- create_data_for_heatmap_bis(6904240, tmp)

dd <- rbind(d1, d2, d3, d4) |> mutate(X = factor(X, levels = c('May','Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan'))) |>

  mutate(Y = factor(Y, levels = sort(unique(Y), decreasing = T))) |>

  mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578))) |>

  mutate(Z = factor(Z, levels = levels(tmp$cut_flux)))

ggplot(dd) +

  geom_tile(aes(X, Y, fill = Z), colour = 'black', alpha = 1) + theme_bw() + labs(fill = 'Flux range', x = 'Month', y = 'Depth bin') +

  facet_wrap(~wmo) + scale_fill_viridis_d(option = 'A')

```

#### same plot mais pour les classes de tailles

```{r, TEST }

depth_bin <- c(seq(0,500,50), seq(600, 1000, 100), seq(1250, 2000, 250))

part_distrib <- vertical_profiles |> filter(depth < 2000) |> select(juld, depth, wmo, NP_Size_256) |> mutate(depth_bin = cut(depth, breaks = depth_bin)) |>

  mutate(month = month.abb[month(juld)]) |> ungroup()

tmp <- part_distrib |> group_by(wmo, depth_bin, month) |> summarize(mean_part = log10(mean(NP_Size_256))) |>

  mutate(cut_part = cut(mean_part, breaks = c(seq(0,100,20),+Inf))) |> ungroup()

create_data_for_heatmap_bis <- function(float, df){

  df <- df |> filter(wmo == float) |> select(mean_part, depth_bin, month)

  y <- unique(df$depth_bin)

  df <- df|> pivot_wider(names_from = depth_bin, values_from = mean_part)

  x <- unique(df$month)

  data <- expand.grid(X=x, Y=y)

  data$Z <- as.vector(as.matrix(df |> select(-month)))

  data$wmo <- float

  return(data)

}

d1 <- create_data_for_heatmap_bis(6904241, tmp)

d2 <- create_data_for_heatmap_bis(1902578, tmp)

d3 <- create_data_for_heatmap_bis(4903634, tmp)

d4 <- create_data_for_heatmap_bis(6904240, tmp)

dd <- rbind(d1, d2, d3, d4) |> mutate(X = factor(X, levels = c('May','Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan'))) |>

  mutate(Y = factor(Y, levels = sort(unique(Y), decreasing = T))) |>

  mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578))) #|>

  #mutate(Z = factor(Z, levels = levels(tmp$cut_part)))

ggplot(dd) +

  geom_tile(aes(X, Y, fill = Z), colour = 'black', alpha = 1) + theme_bw() + labs(fill = 'Log10(abundance) #/L', x = 'Month', y = 'Depth bin', title = 'NP_Size_256') +

  facet_wrap(~wmo) + scale_fill_viridis(option = 'A')

```

#### same plots mais par mois

Il y a une erreur lors de la standardisation -> WHY doesn't it want to group it by month???

```{r, monthly heatmap}

depth_bin <- c(seq(0,500,50), seq(600, 1000, 100), seq(1250, 2000, 250))

part_distrib <- vertical_profiles |> filter(depth < 2000) |> select(juld, depth, wmo, all_of(lpm_class)) |> mutate(depth_bin = cut(depth, breaks = depth_bin)) |> mutate(month = month.abb[month(juld)]) |> ungroup() |> pivot_longer(cols = all_of(lpm_class), names_to = 'size', values_to = 'concentration')

tmp <- part_distrib |> group_by(depth_bin, size, month) |> summarize(mean_part = mean(concentration)) |> mutate(rel_mean_conc = (mean_part-min(mean_part))/(max(mean_part) - min(mean_part))) |> ungroup()

plot_heat_month <- function(m){

  # transform data for heatmap

  #m <- 'May'

  tmp2 <- tmp |> filter(month == m) |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric()) |> select(-c(month, mean_part))

  y <- unique(tmp2$depth_bin)

  x <- unique(tmp2$size)

  test <- tmp2 |> pivot_wider(names_from = depth_bin, values_from = rel_mean_conc)

  #test <- tmp2 |> pivot_wider(names_from = depth_bin, values_from = mean_part)

  data <- expand.grid(X=x, Y=y)

  data$Z <- as.vector(as.matrix(test |> select(-size)))

  data$month <- m

  return(data)

}

m_data <- map_dfr(c('May','Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan'), plot_heat_month)

m_data <- m_data |> mutate(Y = factor(Y, levels = sort(unique(Y), decreasing = T)),

                   month = factor(month, levels = c('May','Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan')))

ggplot(m_data |> mutate(Y = factor(Y, levels = sort(unique(Y), decreasing = T)))) +

  geom_tile(aes(as.factor(X), Y, fill = Z), colour = 'black', alpha = 1) + theme_bw() + scale_fill_viridis(option = 'A') +

  facet_wrap(~month)

```

#### Compute fluxes with Guidi et al. 2008

A = 12.5, b = 3.81 fitted on 250 microns - 1.5 mm.

```{r, guidi 2008}

mid_DSE <- c(0.2892699,0.3644572,0.4591873,0.5785398,0.7289145,0.9183747,1.1570796,1.4578289)

guidi_spectra <- vertical_profiles |> select(NP_Size_203:NP_Size_1290)

# Guidi 2008

A <- 12.5

b <- 3.81

exp <- A * mid_DSE ** b

estimated_flux <- rowSums(t(t(guidi_spectra)*exp))

#estimated_flux

vertical_profiles$guidi_flux <- estimated_flux

ggplot(vertical_profiles, aes(x = juld, y = depth, colour = total_pred)) + geom_point() + #scale_color_viridis(option = 'H') +

  scale_y_reverse(limits = c(2000,0)) +

  scale_colour_viridis(option = 'A', limits = c(quantile(vertical_profiles$guidi_flux, probs = 0.25), quantile(vertical_profiles$guidi_flux, probs = 0.50)), oob = scales::squish, na.value = 'grey', direction = -1) +

  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

  labs(x = 'Time', y = 'Predicted Guidi Flux') + facet_wrap(~wmo) + theme_bw()

```

#### Guidi vs. No Guidi Ha Diguedi

```{r, anomaly guidi}

ggplot(vertical_profiles |> mutate(ratio_flux = guidi_flux/total_pred)) + geom_point(aes(x = juld, y = depth, colour = ratio_flux)) +

  scale_y_reverse(limits = c(2000,0)) +

  scale_colour_viridis(option = 'A', limits = c(1,5), oob = scales::squish, na.value = 'grey') +

  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

  labs(x = 'Time', y = 'Ratio Guidi/Total Model') + facet_wrap(~wmo) + theme_bw()

```

### Modèle basé sur les pentes

```{r, model based on slope}

# compute biovolumes for mid size bin (ESD)

# DSE from 102 microns to 2.05 mm (this class to goes to 2.58 mm) (values obtained from Guidi's Matlab code converted on python)

DSE <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685,

              0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,

              1.16170742, 1.46365963, 1.84409558, 2.32341484)

# DSE obtained with Rainer's code

DSE_Rainer <- c(0.11645117591608793,0.14636003747903648,0.18439151433041295,

                0.23251999827981656,0.2933257316240555,0.36916500651202966,0.4650399965596331,

                0.5860454935035602,0.738553273391974,0.9280422447589767,1.1705684285914486,

                1.4795320531800742,1.8636618038457486,2.3449457391257185)

# compute biovolume

get_biovolume <- function(DSE){

  r <- DSE/2

  biovolume <- (4*pi/3)*r**3

  return(biovolume)

}

# get biovolume for each DSE (equivalent to say for each size bin)

biovolumes <- map_dbl(DSE, get_biovolume)  # unit = m^3

# spectra at parking depth

uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |>

  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))

# convert abundance to biovolume

spectra_biovolume <- tibble(as.data.frame(t(t(uvp_spectra[,all_of(lpm_class)])*biovolumes)))

colnames(spectra_biovolume) <- colnames(spectra_biovolume)|> str_replace('Size', 'Biovolume')

uvp_spectra <- cbind(uvp_spectra, spectra_biovolume) |> mutate()

cflux <- cflux |> mutate(small_slope = (small_flux/633)**(1/0.77), big_slope = (big_flux/633)**(1/0.77))

## try model for small slopes

cflux2 <- cflux |> mutate(flux = big_slope)

data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)

# training data

tmp <- data_model |> select(flux, all_of(colnames(spectra_biovolume)))

set.seed(101)

boosting_rounds <- 3000

# model

model <- gbm(flux ~ ., data=tmp, distribution="gaussian",

cv.folds=5,

n.trees=boosting_rounds, shrinkage=0.005, interaction.depth=6,

bag.fraction=0.5,

n.cores=1

)

# get best model and thus best round number

best <- gbm.perf(model)

#print(best)

# get features importance

features_importance <- summary(model, n.trees=best, plotit = FALSE)

```

### SPECTRAL SLOPES

```{r, function compute slope}

# mid_DSE <- c(0.1147968,0.1446349,0.1822286,0.2295937,0.2892699,0.3644572,0.4591873,0.5785398,0.7289145,0.9183747,1.1570796,1.4578289,1.83674934,2.31415916)

# sizebin <- c(0.02640633,0.03326989,0.04191744,0.05281267,0.06653979,0.08383488,0.10562533,0.13307958,0.16766976,0.21125066,0.26615915,0.33533952,0.422501323,

#              0.532318310)

mid_DSE <- c(0.0911143073, 0.1147968,0.1446349,0.1822286,0.2295937,0.2892699,0.3644572,0.4591873,0.5785398,0.7289145,0.9183747,1.1570796,1.4578289,1.83674934,2.31415916)

sizebin <- c(0.0209587201, 0.02640633,0.03326989,0.04191744,0.05281267,0.06653979,0.08383488,0.10562533,0.13307958,0.16766976,0.21125066,0.26615915,0.33533952,0.422501323,

             0.532318310)

vertical_profiles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class)) |> drop_na(NP_Size_102) |> mutate(juld = as_date(juld)) |> select(-NP_Size_64)

drifting_profiles <- all_floats |> filter(PhaseName == 'PAR') |> select(juld, cycle, depth = pres, park_depth, wmo, all_of(lpm_class)) |> drop_na(NP_Size_102) |> mutate(juld = as_date(juld)) |> select(-NP_Size_64)

compute_slope <- function(i, data_spectra){

  spectrum <- data_spectra[i,]

  spectrum_norm <- spectrum/sizebin

  # prepare data for linear regression

  Y <- log(spectrum_norm)

  X <- log(mid_DSE)

  # check for finite value

  h <- is.finite(Y)

  Y <- Y[h]

  X <- X[h]

  data_slope <- tibble(X=X, Y=Y)

  model <- lm(formula = Y ~ X, data = data_slope)

  slope <- model$coefficients[2]

  return(slope)

}

# compute slope on drifting profiles

drifting_spectra <- as.matrix(data.frame(drifting_profiles[,6:20]))

index <- seq(from = 1, to = nrow(drifting_spectra), by = 1)

slopes <- map_dbl(index, compute_slope, data_spectra = drifting_spectra)

drifting_profiles$spectral_slope <- slopes

ggplot(drifting_profiles, aes(x = juld, y = spectral_slope, colour = factor(park_depth))) + geom_smooth() + facet_wrap(~wmo) + theme_bw() +

  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

  scale_color_brewer(palette = 'Dark2') + labs(x = 'Month', y = 'Spectral slope')

# compute slope on vertical profiles

vertical_spectra <- as.matrix(data.frame(vertical_profiles[,5:19]))

index <- seq(from = 1, to = nrow(vertical_spectra), by = 1)

slopes <- map_dbl(index, compute_slope, data_spectra = vertical_spectra)

vertical_profiles$spectral_slope <- slopes

```

```{r, add slopes}

# vertical_profiles$slope <- slopes

ggplot(vertical_profiles, aes(x = juld, y = depth, colour = spectral_slope)) + geom_point() + scale_colour_viridis(option = 'H') +

  scale_y_reverse(limits = c(2000,0)) +

  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

  labs(x = 'Time', y = TeX('slope')) + facet_wrap(~wmo)

```

### DRIFT DATA

```{r spectral slope + rati min/max flux}

tmp <- cflux |> mutate(ratioF = small_flux/(small_flux+big_flux))

# ggplot(tmp |> filter(big_flux>0), aes(x = min_time, y = ratioF, colour = park_depth)) + geom_point() + facet_wrap(~wmo)

#

# ggplot(tmp |> filter(big_flux>0), aes(x = min_time, y = ratioF)) + geom_point() + facet_wrap(~park_depth)

#

# ggplot(tmp |> filter(big_flux>0), aes(x = min_time, y = ratioF)) + geom_smooth() + facet_wrap(~park_depth)

ggplot(tmp |> filter(big_flux>0), aes(x = min_time, y = ratioF)) + geom_point() + geom_smooth() + facet_wrap(~park_depth)

ggplot(drifting_profiles, aes(x = juld, y = spectral_slope, colour = factor(park_depth))) + geom_smooth() + facet_wrap(~wmo) + theme_bw() +

  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +

  scale_color_brewer(palette = 'Dark2') + labs(x = 'Month', y = 'Spectral slope')

tmp2 <- drifting_profiles |> group_by(wmo, cycle, park_depth) |> summarize(median_spectral_slope = median(spectral_slope))

# merge data

check <- merge(tmp, tmp2)

check2 <- merge(tmp, tmp2) |> filter(ratioF < 1)

ggplot(check, aes(ratioF, median_spectral_slope)) + geom_point()

ggplot(check2, aes(ratioF, median_spectral_slope)) + geom_point()

# ggplot(tmp, aes(x = min_time, y = ratioF)) + geom_smooth() + facet_wrap(~park_depth)

```

## Discussion2

PARLER DU TRADEOFF BETWEEN SMALL FLUX AND BIG PARTICLE FLUX

The latter are however subject to a tradeoff between the likely underestimation of $F_{small}$ and the detection of false jumps (without great impact on the value of $F_{big}$ because false jumps have low $c_{p}$ values) if the threshold is too small and the likely overestimation of $F_{small}$ to the detriment of $F_{big}$ with missed jumps if the threshold is too high.

## Conclusion
