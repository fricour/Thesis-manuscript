# Case study in the Labrador Sea {#case-study-labrador-sea}

## Introduction

introduire la notion de cycle de flotteurs BGC-Argo etc (+ parking depth)

-   dire que le but du papier est de faire une comparaison OST UVP qui sont deux instruments qui permettent de mesurer la meme chose (le flux des particules, l'abondance de particules) et donc le flux de carbone de manière indirecte papier GRL Lacour de 2015

## Methods

### Deployment and floats configuration

Two couples of BGC-Argo floats were deployed in the Labrador Sea in late May 2022. The first couple (WMO 6904240 and 4903634) was deployed on the 22$^{nd}$ at coordinates (-49.3°E, 58.9°N) while the second (WMO 6904241 and 1902578) was deployed on the 30$^{th}$ at coordinates (-52.4°E, 56.8°N).

All 4 floats used in this study are PROVOR Jumbo floats (NKE Instrumentation, France) equipped with a CTD SBE41-CP (Sea-Bird Scientific, USA), a UVP6-LP (Hydroptic, France), a c-Rover 660 nm beam transmissometer (WETLabs, USA) and a ECO FLBBCD (WETLabs, USA) that includes a chlorophyll-a (Chla) fluorometer (excitation at 470 nm, emission at 695 nm), a colored dissolved organic matter (CDOM) fluorometer (excitation at 370 nm, emission at 460 nm) and a 700 nm particle backscatteringmeter ($b_{bp}$). All floats but one (1902578) were equipped with a OCR-504 Multispectral Radiometer to measure the photosynthetic available radiation.

All 4 floats have been configured to drift at 3 distinct parking depths (respectively, 200 m, 500 m and 1000 m for approximately 1, 3 and 5 days) to study the downward carbon flux. During these drift phases, measurements of particles abundance (14 size classes, between 0.1 mm and 2.5 mm) are taken every 20 minutes at 200 m and every 2h at 500 m and 1000 m whereas $c_{p}$ measurements are taken every 30 minutes at all depths. This configuration can be adjusted during the mission of the float via two-way Iridium communication.

```{r, load libraries}
library(akima)
library(broom)
library(tidyverse)
library(cmocean)
library(lubridate)
library(scales)
library(patchwork)
library(viridis)
library(ggridges)
library(ggvis)
library(pracma)
library(gbm)
library(MLmetrics)
library(purrr)
library(arrow)
library(plotly)
library(oce)
library(ggplot2)
library(latex2exp)
library(cowplot)
library(RColorBrewer)
library(vroom)
library(castr)
library(reactablefmtr)
library(gt)
library(gtExtras)
library(spatialrisk)
```

```{r, map of 4 floats}
#| label: fig-float-position
#| fig-cap: Route of each float since their deployment (black triangles). Dots represent ascending profiles. The profile that concludes the time series for each float is highlighted by a black contour.

# read floats data
float_6904240 <- vroom('/home/flo/dox/ArgoShine/6904240_FromNetCDF.csv')
float_6904241 <- vroom('/home/flo/dox/ArgoShine/6904241_FromNetCDF.csv')
float_4903634 <- vroom('/home/flo/dox/ArgoShine/4903634_FromNetCDF.csv')
float_1902578 <- vroom('/home/flo/dox/ArgoShine/1902578_FromNetCDF.csv')

# combine all data
all_floats <- rbind(float_6904240, float_6904241, float_4903634, float_1902578)

# add levels for some field
all_floats <- all_floats |> mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
                                   park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

# summarize localization of floats
argo_map <- all_floats |> select(WMO = wmo, cycle, lat, lon) |> drop_na(lat) |> dplyr::group_by(WMO, cycle) |> mutate(WMO = factor(WMO)) |>
  dplyr::summarise(lat = mean(lat), lon = mean(lon))

argo_map_first <- argo_map |> filter(cycle == 1)
argo_map_last <- argo_map |> filter(cycle == max(cycle))

# map positions of floats
world_data <- map_data("world")
w <- list(geom_polygon(aes(long, lat, group=group), data=world_data, fill="grey70"), coord_quickmap())

ggplot(argo_map) + w + geom_path(aes(lon, lat, colour = WMO), size = 1) +
  geom_point(aes(lon, lat, colour = WMO), size = 1.5) +
  scale_color_brewer(palette = 'Dark2') +
  theme_bw() +
  #scale_colour_manual(values = c('#a6cee3', '#33a02c', '#b2df8a', '#1f78b4')) +
  coord_cartesian(xlim = c(-55, -40), ylim = c(55,65)) + geom_point(data = argo_map_first, aes(lon, lat), shape = 17, size = 3) +
  geom_point(data = argo_map_last, aes(lon, lat, colour = WMO), size = 3) +
  geom_point(data = argo_map_last, aes(lon, lat, fill = WMO), size = 3, shape = 1) +
  labs(x = 'longitude', y = 'latitude') +
  #https://aosmith.rbind.io/2020/07/09/ggplot2-override-aes/#suppress-aesthetics-from-part-of-the-legend
  guides(color = guide_legend(override.aes = list(size = c(1.5,1.5,1.5,1.5),
                                                  shape = c(NA, NA, NA, NA))))
  
```

### Data processing

Data were downloaded from the Coriolis data center (biogeochemical data at ftp://ftp.ifremer.fr/ifremer/argo/dac/coriolis/ and UVP6 data at ftp://ftp.ifremer.fr/ifremer/argo/aux/coriolis/, last access: MONTH DAY, 2023). Chla data were quality controlled for Non-Photochemical Quenching (NPQ), a photoprotective mechanism that decreases the apparent fluorescence per unit of Chla following the method implemented by @Terrats2020 who modified the method of @Xing2018 for shallow-mixing cases. $c_{p}$ data were converted from counts to physical units (m$^{-1}$) using the following relationship

$$
c_{p}~[m^{-1}] = -\frac{1}{x}~log \left(~ \frac{c_{p}~[counts]-CSC_{dark}}{CSC_{cal}-CSC_{dark}} \right)
$$

where $x$, $CSC_{dark}$ and $CSC_{cal}$ are respectively the transmissometer pathlength (0.25 m), the sensor output when the beam is blocked (i.e. offset, 0 for each c-Rover of this study) and the sensor output with clean water in the path (12998, 13598, 13115 and 12855 for, respectively, float 4903634, 6904240, 6904241 and 1902578). The latter were obtained from the calibration sheets of each c-Rover.

UVP6 data and in particular the averaged number of particles per size class (i.e. particle size distribution hereafter referred to as PSD) was divided by the number of images at each depth and the water volume imaged by the UVP (0.7L) to obtain particle concentrations (units: #/L). Note that in an impending update by Coriolis, it will not be necessary to divide by the number of images anymore.

All data were divided into vertical profiles and drifting data using time stamps for drift. Finally, data were checked for instrumental anomalies. As a result, $c_{p}$ values from float 6904241 cannot be used from its 20$^{th}$ cycle onwards (\> November 17) due to a likely failure of the pump cleaning the optical window of the transmissometer.

### Derivation of carbon fluxes from OST

We applied a similar approach to @Estapa2013-gw and @Estapa2017-jr to derive the continuous component (i.e. continuous deposition of small particles on the upward-facing window of the transmissometer) and discontinuous (i.e. deposition of stochastic big particles) components of the downward POC flux, hereafter referred to as, respectively, $F_{small}$ and $F_{big}$.

First, the drifting $c_{p}$ signal was cleaned from spikes (hypothesized as transient particles not landing on the window or active swimmers [@Estapa2013-gw]). For this, we computed a 7-point moving median and a 7-point moving median absolute deviation (MAD). Then, we computed the anomaly to the moving median as the difference between the drifting $c_{p}$ signal and the moving median. We considered that a $c_{p}$ measurement was a spike when the anomaly was above $5.2 \times MAD$ following @Leys2013. Despiked $c_{p}$ data were then smoothed with a 7-point moving median filter.

Contrarily to previous works [@Estapa2013-gw; @Estapa2017-jr], we chose an empirical statistical approach instead of fixed thresholds to detect jumps. Therefore, we firstly computed the slope between each $c_{p}$ observation for each drifting phase and computed the z-score (i.e. standard score) of each of them. The detection of jumps therefore consisted in spotting outliers in the z-score distribution. For this, we computed the interquartile range ($IQR$) between the first ($Q_{1}$) and third ($Q_{3}$) quartiles. Positive and negative jumps were defined as, respectively, a z-score above $Q_{3} + 1.5 \times IQR$ and below $Q_{1} - 1.5 \times IQR$. We found that this method better detected outliers than the one used to despike the $c_{p}$ signal.

In order to compute $F_{small}$, we firstly computed the slope of each group of observations between jumps (positive and negative, see @fig-estapa-method_article). Undefined slopes (a single point between two jumps) were removed as well as any negative slope assuming that either the so-called fit failed (leading to undetected jumps) either the transmissometer window was perturbed by something else than the pump cleaning the optical window. Afterwards, we computed the weighted average slope ($slope_{w}$) using the previously computed slopes of each remaining group in order to derive the small particles flux using the relationship of @Estapa2022

$$
F_{small} = 633(0.25~slope_{w})^{0.77}
$$ {#eq-estapa}

where $slope_{w}$ is multiplied by the transmissometer pathlength due to the fact that @eq-estapa is an empirical POC-to-attenuance (ATN) relationship where ATN is the product of $c_{p}$ and the transmissometer pathlength.

We used the same relationship to compute $F_{big}$ by replacing $slope_{w}$ as the sum of positive jumps divided by the drifting time.

```{r, example of a cp signal analyzed for jumps (with an old method here)}
#| label: fig-estapa-method_article
#| fig-cap: Application of the method of @Estapa2017-jr on a drifting transmissometer to divide the $c_{p}$ signal into a continuous (dark blue) component and a discontinuous (positive jumps, red) component. Slopes for each group are depicted in light blue. A negative jump was also detected (yellow). Data from BGC-ARGO float WMO 6904240, drifting at 1000 m during its 21$^{st}$ cycle.
clean_cp_data <- function(data, moving_median_order){
  
  if(nrow(data) == 0){ # no cp data
    return(data)
  }else{
    
    # # Remove cp > 2 m-1 (don't really know why but eh)
    # if (any(data$cp > 2)){
    #   data <- data[-which(data$cp > 2),]
    # }
    
    # despike cp data
    data$cp <- oce::despike(data$cp, reference = 'median', k = 7, replace = 'NA')
    
    # median average
    data$cp <- pastecs::decmedian(data$cp, order = moving_median_order, ends = "fill") |> pastecs::extract(component = "filtered")
  }
  return(data)
  
}

tmp <- all_floats |> filter(wmo == 6904240, park_depth == '1000 m', cycle == 21, PhaseName == 'PAR') |> drop_na(cp) |> select(dates = juld, everything())

threshold <- 0.08

clean_data <- clean_cp_data(tmp, 3)

# save cleaned Cp signal from spikes to a temporary variable
tmp2 <- clean_data
tmp2$jump <- NA
# compute slope between two adjacent points (except first and last point of drifting time series)
for (i in 2:(nrow(tmp2)-1)){
  #print(i)
  delta_x <- as.numeric(difftime(tmp2$dates[i], tmp2$dates[i-1], units = 'days'))
  delta_y <- tmp2$cp[i]-tmp2$cp[i-1]
  tmp2$jump[i] <- delta_y/delta_x
}
  
# assign a colour for the plot
tmp2$colour <- 'base signal'
tmp2$colour[which(abs(tmp2$jump)>threshold)] <- 'jump' # abs is needed for 'negative jumps'
  
# add group to compute slope (for each subgroups of points, separated from a jump)
tmp2$group <- NA
  
# compute where the jumps are (index value of the array)
jump_index <- which(tmp2$colour == 'jump')
  
# assign group 
for (i in jump_index){
  for (j in 1:nrow(tmp2)){
    if ((j < i) & (is.na(tmp2$group[j]))){
      tmp2$group[j] <- paste0('group_',i)
    }
  }
}
  
# add last group (because it is AFTER the last index so it was not computed before)
tmp2$group[which(is.na(tmp2$group))] <- 'last_group'
  
# compute slope for each subgroups
slope_df <- tmp2 |> filter(colour == 'base signal', jump != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x)
  
# remove negative slope from the mean slope (no physical meaning)
slope_df <- slope_df |> filter(slope > 0)
  
# remove if only one point (cannot fit a slope with one point)
slope_df <- slope_df |> filter(nb_points > 1)
  
# compute mean slope 
#mean_slope <- mean(slope_df$slope)
mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
# estapa relationship (see POSTER)
#poc_flux <- 633*((mean_slope*0.25)**0.77) # *0.25 because ATN = cp*0.25
poc_flux <- 633*((mean_slope)**0.77) # /!\ slope for computed for ATN on y axis (delta_y *0.25 because ATN  = cp*0.25) -> should be OK
  
# build dataframe to plot each subgroup
part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
part_slope <- rbind(part1, part2)
  
# spot negative jump
tmp2$colour[which((tmp2$colour == 'jump') & (tmp2$jump < 0))]  <- 'negative jump'
  
# add big particles flux
rows_to_keep <- c(jump_index, jump_index-1) 
tmp3 <- tmp2[rows_to_keep,] |> select(dates, cp, jump, colour, group) |> dplyr::arrange(dates)
  
# remove negative jump, if any
check_colour <- unique(tmp3$colour)
if(length(check_colour) == 3){
  index_neg_jump <- which(tmp3$colour == 'negative jump')
  tmp3 <- tmp3[-c(index_neg_jump, index_neg_jump+1),]
  tmp3 <- tmp3 |> mutate(diff_jump = cp - lag(cp)) 
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else if(length(check_colour) == 2){
  tmp3 <- tmp3 |> mutate(diff_jump = cp - lag(cp)) 
  even_indexes <- seq(2,nrow(tmp3),2)
  tmp3 <- tmp3[even_indexes,]
}else{ # NO jump
  tmp3 <- NULL
  }
  
# big particles flux
if(is.null(tmp3)){ # no jumps
  big_part_poc_flux <- 0
}else{
  tmp4 <- tmp3 |> filter(diff_jump > 0)
  if(nrow(tmp4) == 0){ # no positive jumps
    big_part_poc_flux <- 0
  }else{
    delta_y <- sum(tmp4$diff_jump) *0.25 # to get ATN (= cp*0.25)
    max_time <- max(tmp2$dates)
    min_time <- min(tmp2$dates)
    delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
    slope_big_part <- delta_y/delta_x
    big_part_poc_flux <- 633*(slope_big_part**0.77)
  }
}
  
# compute total drifting time
max_time <- max(tmp2$dates)
min_time <- min(tmp2$dates)
drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))

check <- tmp2 |> select(dates, cp, colour)
check <- check |> filter(colour %in% c('jump', 'negative jump'))

# ggplot(clean_data, aes(x = dates, y = cp)) + geom_point(size= 3) + geom_point(data = tmp2, aes(x = dates, y = cp, colour = colour), size = 2) + 
#     scale_colour_manual(values = c('#003366','#E31B23', '#FFC325')) + 
#     scale_shape_manual(values = c(1,1,4)) +
#     geom_path(data = part_slope, aes(x = time, y = cp, group = group), colour = '#DCEEF3', size = 1) + theme_bw() + 
#     labs(x = 'Time', y = 'Cp (1/m)') +
#   geom_point(data = check, aes(x = dates, y = cp, colour = colour))

ggplot(tmp2, aes(x = dates, y = cp, colour = colour)) + geom_point(size = 3) +
    scale_colour_manual(values = c('#003366','#E31B23', '#FFC325')) + 
    #scale_shape_manual(values = c(1,1,4)) +
    geom_path(data = part_slope, aes(x = time, y = cp, group = group), colour = '#DCEEF3', size = 2) + theme_bw() + 
    labs(x = 'Time', y = TeX('$c_{p}~(m^{-1})$')) +
  geom_point(data = check, aes(x = dates, y = cp, colour = colour), size = 3) +
  geom_point(data = check, aes(x = dates, y = cp), shape = 1, size = 3, colour = 'black')  +
  #https://r-graph-gallery.com/239-custom-layout-legend-ggplot2.html
  # theme(legend.position = c(0.02,0.99), legend.justification = c('left', 'top'), 
  #       legend.title = element_blank())
    theme(legend.position = 'none')
```

## Results

### OST-derived carbon fluxes

```{r, function to detect and plot jumps}

plot_jumps <- function(data){
  
  # make sure that the cp signal is in chronological order
  tmp <- data |> arrange(dates)
  
  # despike cp data with a 7-point moving window
  tmp$cp <- despike(tmp$cp, k = 3)
  
  # smooth cp data with a 3-point moving median, n time(s)
  tmp$cp <- slide(tmp$cp, fun = median, k = 3, n = 1, na.rm=T)
  
  # compute slope between two adjacent points (except first point) # we could start after 1h to let the float stabilize
  delta_x <- as.numeric(tmp$dates - lag(tmp$dates), units = 'days')
  delta_y <- tmp$cp - lag(tmp$cp)
  tmp$slope <- delta_y/delta_x
  
  # compute a Z score (assuming a normal distribution of the slopes) on the slopes
  tmp <- tmp |> mutate(zscore = (slope - mean(slope, na.rm = T))/sd(slope, na.rm = T))
  
  # spot outliers on the Z score signal
  # interquartile range between Q25 and Q75 -> had to used that and not the despike function because slopes are often close (or equal) to 0 so it can miss clear jumps. Q25 and Q75 are more trustworthy in this case than the despike function of Jean-Olivier (see package castr on his github: https://github.com/jiho/castr)
  IQR <- quantile(tmp$zscore, probs = 0.75, na.rm=T) - quantile(tmp$zscore, probs = 0.25, na.rm=T)
  # outliers ('spikes' in the Z score signal)
  spikes_down <- tmp$zscore < quantile(tmp$zscore, 0.25, na.rm=T) - 1.5 *IQR
  spikes_up <-  tmp$zscore > quantile(tmp$zscore, 0.75, na.rm=T) + 1.5 *IQR
  spikes <- as.logical(spikes_down + spikes_up)
  
  # assign spikes
  tmp$spikes <- spikes
  
  # assign colour code to cp signal
  tmp$colour <- 'base signal' # base signal = smoothed despiked cp signal
  tmp[which(tmp$spikes == TRUE),]$colour <- 'jump'
  
  # add group to compute the slope of each group of points, separated by a jump
  tmp$group <- NA
  
  # index of jumps in the array
  jump_index <- which(tmp$colour == 'jump')
  
  # assign group identity to each group of points, separated by a jump (= subgroup)
  for (i in jump_index){
    for (j in 1:nrow(tmp)){
      if ((j < i) & (is.na(tmp$group[j]))){
        tmp$group[j] <- paste0('group_',i)
      }
    }
  }
  tmp$group[which(is.na(tmp$group))] <- 'last_group'
  
  # compute slope for each subgroup
  slope_df <- tmp |> filter(colour == 'base signal', slope != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), 
                                                                                                                  nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],
                                                                                                                  delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),
                                                                                                                  delta_y = (last_cp-first_cp)*0.25, slope = delta_y/delta_x) # *0.25 to convert cp to ATN
  
  # remove negative slope from the mean slope (no physical meaning)
  slope_df <- slope_df |> filter(slope > 0)
  
  # remove if only one point (cannot fit a slope with one point)
  slope_df <- slope_df |> filter(nb_points > 1)
  
  # compute weighted average slope (to take into account the fact that some subgroups might have 2 points and a high slope vs. large group of points with a small slope)
  mean_slope <- sum(slope_df$nb_points * slope_df$slope)/sum(slope_df$nb_points)
  
  # convert cp to POC using Estapa's relationship 
  poc_flux <- 633*(mean_slope**0.77) # /!\ slope computed for ATN on y axis (delta_y *0.25 because ATN = cp*0.25) -> should be OK
  
  # build dataframe to plot each subgroup
  part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
  part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
  part_slope <- rbind(part1, part2)
  
  # spot negative jump
  tmp$colour[which((tmp$colour == 'jump') & (tmp$slope < 0))]  <- 'negative jump'
  
  # add big particles flux to the party
  rows_to_keep <- c(jump_index, jump_index-1)
  tmp2 <- tmp[rows_to_keep,] |> select(dates, cp, slope, colour, group) |> arrange(dates)
  
  # remove negative jumps, if any
  check_colour <- unique(tmp2$colour)
  if(length(check_colour) == 3){ # there is at least a negative jump
    index_neg_jump <- which(tmp2$colour == 'negative jump')
    tmp2 <- tmp2[-c(index_neg_jump, index_neg_jump+1),]
    tmp2 <- tmp2 |> mutate(diff_jump = cp - lag(cp)) 
    even_indexes <- seq(2,nrow(tmp2),2)
    tmp2 <- tmp2[even_indexes,]
  }else if(length(check_colour) == 2){ # only positive jumps
    tmp2 <- tmp2 |> mutate(diff_jump = cp - lag(cp)) 
    even_indexes <- seq(2,nrow(tmp2),2)
    tmp2 <- tmp2[even_indexes,]
  }else{ # No jump
    tmp2 <- NULL
  }
  
  if(is.null(tmp2)){ # no jump
    big_part_poc_flux <- 0
    tmp3 <- NULL
  }else{
    tmp3 <- tmp2 |> filter(diff_jump > 0)
    if(nrow(tmp3) == 0){ # no positive jumps
      big_part_poc_flux <- 0
    }else{
      delta_y <- sum(tmp3$diff_jump) *0.25 # to get ATN (= cp*0.25)
      max_time <- max(tmp$dates)
      min_time <- min(tmp$dates)
      delta_x <- as.numeric(difftime(max_time, min_time, units = 'days'))
      slope_big_part <- delta_y/delta_x
      big_part_poc_flux <- 633*(slope_big_part**0.77)
    }
  }
  
  # compute total drifting time
  max_time <- max(tmp$dates)
  min_time <- min(tmp$dates)
  drifting_time <- as.numeric(difftime(max_time, min_time, units = 'days'))
  
  # to plot subgroups
  part_slope_tmp <- part_slope |> mutate(dates = time, colour = 'slope')
  
  # plot
  jump_plot <- plot_ly(tmp, x = ~dates, y = ~cp, type = 'scatter', mode = 'markers', color = ~colour, colors = c('#003366','#E31B23', '#FFC325')) |>
    add_lines(data= part_slope_tmp, x = ~dates, y = ~cp, split = ~group, color = I('#DCEEF3'), showlegend = F) |>
    layout(title= paste0('Drifting time: ', round(drifting_time,3), ' days\n',
                         'Mean ATN slope (light blue): ', round(mean_slope,3), ' day-1\n',
                         'POC flux (small particles): ', round(poc_flux,1), ' mg C m-2 day-1\n',
                         'POC flux (big particles): ', round(big_part_poc_flux,1), ' mg C m-2 day-1'), yaxis = list(title = 'Cp (1/m)'), xaxis = list(title = 'Time'))
  
  
  #return(jump_plot)
  #return(list('jump_plot' = jump_plot, 'jump_table' = tmp3))
  
    # adapt script to return big and small flux
  df <- tibble('max_time' = max_time, 'min_time' = min_time, 'small_flux' = poc_flux, 'big_flux' = big_part_poc_flux, park_depth = data$park_depth[1], wmo = data$wmo[1],
               cycle = data$cycle[1])
  
  return(df)
  
}
```

```{r, compute Fsmall and F big}

# remove bad data from float 6904241
float_6904241_slope <- float_6904241 |> filter(cycle < 20)
# create dataframe for all drifting data (for all floats)
all_floats_slope <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
all_floats_slope <- all_floats_slope |> filter(PhaseName == 'PAR') |> drop_na(cp) |> select(depth = pres, cp, dates = juld, park_depth, wmo, cycle) |>
   mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
          park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))

# start loop for all data
wmo <- c(4903634, 6904240, 6904241, 1902578)
park_depth <- c('200 m', '500 m', '1000 m')

res <- data.frame()
for (i in wmo){
  max_cycle <- as.numeric(all_floats_slope |> filter(wmo == i) |> dplyr::summarise(max_cycle = max(cycle)))
  for (j in park_depth){
    for (k in seq(1:max_cycle)){
      tmp <- all_floats_slope |> filter(wmo == i, park_depth == j, cycle == k)
      if(nrow(tmp) == 0){
        next
      }else if(nrow(tmp) < 3){ # case where there is not enough data
        next
      }else{
          output <- plot_jumps(tmp)
          res <- rbind(res, output)
      }
    }
  }
}

# keep the good fluxes
cflux <- res |> mutate(drifting_time = difftime(max_time, min_time, units = 'days'), WMO = factor(wmo))

cflux_info_table <- cflux |> dplyr::group_by(park_depth) |> summarize(min_smallf = min(small_flux, na.rm=T), max_smallf = max(small_flux, na.rm=T),
                                                                mean_smallf = mean(small_flux, na.rm=T), median_smallf = median(small_flux, na.rm=T),
                                                                std_smallf = sd(small_flux, na.rm=T),
                                                                min_bigf = min(big_flux, na.rm=T), max_bigf = max(big_flux, na.rm=T),
                                                                mean_bigf = mean(big_flux, na.rm=T), median_bigf = median(big_flux, na.rm=T),
                                                                std_bigf = sd(big_flux, na.rm=T))

cflux$date <- as_date(cflux$min_time)
```

$F_{small}$ and $F_{big}$ were computed at each drifting depth for all floats (@fig-OST-flux). At 200 m, $F_{small}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ (average $\pm$ standard deviation). At 500 m, $F_{small}$ is comprised between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_smallf, 1)` mg C m$^{2}$ day$^{-1}$. For both drifting depths, the trend shows a decrease of $F_{small}$ from June to January. At 1000 m, all floats measured an increase of $F_{small}$ from late May to mid-July followed by a steady decrease towards January, with $F_{small}$ ranging between `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$min_smallf, 1)` and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_smallf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_smallf, 1)` mg C m$^{-2}$ day$^{-1}$ on average.

```{r, plot Fsmall and Fbig}
#| label: fig-OST-flux
#| fig-cap: OST-derived small (top) and big (bottom) particles flux for all floats at each drifting depth. Black lines and shaded areas represent, respectively, the monthly mean flux and the 95% confidence interval around the mean. 

up <- cflux |> ggplot(aes(x = date, y = small_flux)) +
    geom_point(data = cflux, aes(x = date, y = small_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = cflux, aes(x = date, y = small_flux), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = '', y = TeX('$F_{small}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  scale_y_continuous(trans = 'log10') +
  theme_bw() 

down <- cflux |> ggplot(aes(x = date, y = big_flux)) +
  geom_point(data = cflux, aes(x = date, y = big_flux, colour = WMO, shape = WMO)) +
  geom_smooth(data = cflux, aes(x = date, y = big_flux), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  guides(fill=guide_legend(title="WMO")) +
  facet_wrap(~park_depth, scales = 'free') +
  labs(x = 'Month', y = TeX('$F_{big}$ (mg C m$^{-2}$ day$^{-1}$)')) +
  theme_bw() +
  scale_y_continuous(trans = 'log10') +
  theme(legend.position = 'none') 
  
up / down

```

$F_{big}$ was estimated at 200 m between 0 (no deposition of big particles on the transmissometer window) and `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '200 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$. High fluxes (\> 100 mg C m$^{-2}$ day$^{-1}$) were measured by 3 out of 4 floats at the end of May before a sharp decrease towards July. At 500 m, $F_{big}$ ranges between `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$min_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ and `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '500 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$, decreasing from September to January. At 1000 m, $F_{big}$ shows a similar pattern than the one observed for $F_{small}$ with an increase from June to mid-July, followed by a decrease towards January, with values ranging from 0 to `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$max_bigf, 1)` mg C m$^{-2}$ day$^{-1}$ with an average of `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$mean_bigf, 1)` $\pm$ `r round(cflux_info_table[cflux_info_table$park_depth == '1000 m',]$std_bigf, 1)` mg C m$^{-2}$ day$^{-1}$.

### UVP6 particle abundances

#### On drifting mode

```{r, uvp data}

lpm_class <- c("NP_Size_102", "NP_Size_128", "NP_Size_161",
                         "NP_Size_203", "NP_Size_256", "NP_Size_323",
                         "NP_Size_406", "NP_Size_512", "NP_Size_645",
                         "NP_Size_813", "NP_Size_1020", "NP_Size_1290",
                        "NP_Size_1630", "NP_Size_2050")

uvp_data <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date, cycle, wmo, park_depth, all_of(lpm_class)) |> 
  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))

uvp_data <- uvp_data |> pivot_longer(cols = lpm_class, names_to = 'size', values_to = 'conc') |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric())

# compute daily mean chla for each float at each cycle and at each drifting depth
mean_uvp_data <- uvp_data |> group_by(cycle, WMO, size, park_depth, juld) |> summarize(mean_conc = mean(conc, na.rm=T))
```

In this study, we focus on 14 size classes, from 102 $\mu$m to 2.5 mm. In @fig-uvp-200m, @fig-uvp-500m and @fig-uvp-1000m, we show the daily averaged concentrations of particles in each class size for each float at their respective drifting depth. At 200 m, it can be observed that particles concentrations in the 102 - 512 $\mu$m range are relatively uniform between June and October before decreasing sharply towards January. Between 512 $\mu$m and 1.29 mm, concentrations are steadily decreasing from June. Above 1.29 mm, the abundance of particles is already very low (\< 0.1 particle/L) in June and keeps decreasing over time.

```{r, uvp data at 200 m}
#| label: fig-uvp-200m
#| fig-cap: Daily averaged concentrations of particles abundance in 14 size classes measured by the UVP6 for each float at 200 m. Black lines and shaded areas represent, respectively, the daily mean particles abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particles abundance is maximum.

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp200m <- ggplot_build(p200m)

test <- tmp200m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> summarize(index_max = which.max(y), max_date = x[index_max]) |> mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p200m <- mean_uvp_data |> filter(park_depth == '200 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p200m
```

At 500 m, particles concentrations in the 102 - 645 $\mu$ range slowly decrease from July to September with a sharp decrease starting in October. Above 645 $\mu$m, the trend shows a maximum between the end of July and the first half of August though the concentrations of particles in those classes are very low (\< 0.5 particle/L) on the entire timeserie.

```{r, uvp data at 500 m}
#| label: fig-uvp-500m
#| fig-cap: Daily averaged concentrations of particles abundance in 14 size classes measured by the UVP6 for each float at 500 m. Black lines and shaded areas represent, respectively, the daily mean particles abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particles abundance is maximum.

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp500m <- ggplot_build(p500m)

test <- tmp500m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> summarize(index_max = which.max(y), max_date = x[index_max]) |> mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p500m <- mean_uvp_data |> filter(park_depth == '500 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p500m
```

At 1000 m, all 4 floats measured a clear maximum in particles abundance in the 102 - 645 $\mu$m in the second half of July. Above 645 $\mu$m, the same trend can still be observed with concentrations of less than 1 particle/L.

```{r, uvp data at 1000 m}
#| label: fig-uvp-1000m
#| fig-cap: Daily averaged concentrations of particles abundance in 14 size classes measured by the UVP6 for each float at 1000 m. Black lines and shaded areas represent, respectively, the daily mean particles abundance and the 95% confidence interval around the mean. Dashed vertical lines indicate the time at which the daily mean particles abundance is maximum.
#| fig-dpi: 96

#{r, uvp data at 1000 m, out.width="900px", out.height="800px"}

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') #+
  #geom_vline(data = test, aes(xintercept = date_max))

tmp1000m <- ggplot_build(p1000m)

test <- tmp1000m$data[[2]] |> filter(is.finite(y)) |> group_by(PANEL) |> summarize(index_max = which.max(y), max_date = x[index_max]) |> mutate(date_max = as_date(max_date, origin = "1970-01-01"))
test$size <- unique(uvp_data$size)

p1000m <- mean_uvp_data |> filter(park_depth == '1000 m') |> ggplot() + geom_point(aes(juld, mean_conc, colour = WMO, shape = WMO)) +
  geom_smooth(aes(juld, mean_conc), colour = 'black') +
  scale_color_brewer(palette = 'Dark2') + 
  scale_x_date(labels = date_format("%m"), date_breaks = '2 month') +
  theme_bw() + labs(x = 'Month', y = 'Particles abundance (#/L)') +
  scale_y_continuous(trans = 'log10') +
  facet_wrap(~size, scales = 'free_y') +
  geom_vline(data = test, aes(xintercept = date_max), linetype = 'dashed')

p1000m
```

TODO : It should be noted that all floats measure the same trend -\> logique si on parle de la dynamique du bruit de fond de la zone et pas de la dynamique évènementielle qui serait différentes en fonction des flotteurs vu qu'ils dérivent chacun dans le parcours respectifs. Ça devrait renforcer cet aspect.

## Discussion

### Comparing OST and UVP data

For the first time, BGC-Argo floats equipped with both a transmissometer and a UVP6 have been deployed to better characterize the downward carbon flux and the nature of sinking particles from a multi-instrument approach [@Claustre2021-tx]. The quantification of the POC flux using the OST method [@Bishop2004-at; @Estapa2013-gw; @Estapa2017-jr] is based on an empirical POC:ATN ratio \[REF estapa 2022\] whereas UVP6 PSD measurements are used to compute POC fluxes using @eq-guidi-article3:

$$
F = \sum_{i}^{N}C_{i}Ad_{i}^{b}
$$ {#eq-guidi-article3}

where $C_{i}$, A and b represent, respectively, the concentration of particles (number/L) for the $i^{th}$ class of mean diameter $d_{i}$ (in mm) and 2 constants with $b$ being related to the fractal dimension ($D$) of an aggregate by $D = (b +1)/2$ [@Guidi2008-vp].

The use of @eq-guidi-article3 to derive carbon fluxes from PSD was firstly implemented by @Guidi2008-vp who used a minimization procedure between sediment traps data and estimated fluxes using PSD from several previous versions of the UVP to find the optimal value of $A$ and $b$ (i.e. 12.5 and 3.81). @Iversen2010-an and @Fender2019-ou followed the same procedure and concluded that the couple of parameters found by [@Guidi2008-vp] could not be used globally at the expense of greatly over- or underestimating POC fluxes, depending on the sampling region. In addition, the formulation of such a model implies that $A$ and $b$ would be valid at global scale, at all depths, for all UVP models and across all size classes which is arguable [@Bisson2022-bt]. @eq-guidi-article3 also assumes that the flux is only size-dependent though the main driver for the settling of aggregates is still debated [@Iversen2020-kb; @Iversen2022-qj]. As a result, the direct comparison of carbon fluxes derived from both OST and UVP data with @eq-guidi-article3 should be focused on the trend rather than on absolute carbon flux values.

```{r, compute guidi fluxes at parking depth}
#| label: fig-comparison-ost-uvp
#| fig-cap: Comparison of total carbon fluxes derived from the OST (black) and the UVP6 (red) where the error bars represent the 25$^{th}$ and 75$^{th}$ percentiles around the median. 

compute_flux <- function(A, b, PSD){ # PSD = Particle Size Distribution
  # for classes sizes between 0.102 mm and 1.5 mm
  #mid_ESD <- c(0.1164512, 0.1463600, 0.1843915, 0.2325200, 0.2933257, 0.3691650, 0.4650400, 0.5860455, 0.7385533, 0.9280422, 1.1705684, 1.4795321) # With Rainer's code (paruvpy)
  # mid_ESD <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685,
  #              0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
  #              1.16170742, 1.46365963) # With Lionel's script
  # mid_ESD <- c(0.23051195, 0.29042685,
  #              0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
  #              1.16170742, 1.46365963) # With Lionel's script
  #mid_ESD <- c(0.287555212,0.362129811,0.455929819,0.57466512,0.724144323,0.91063714,1.147083258,1.450068964)
  mid_ESD <- c(0.29042685,
               0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
               1.16170742, 1.46365963) # With Lionel's script
  exp <- A * mid_ESD ** b
  estimated_flux <- rowSums(t(t(PSD)*exp))
  return(estimated_flux)
}

parking_data <- rbind(float_4903634, float_6904240, float_6904241_slope, float_1902578)
parking_data <- parking_data |> filter(PhaseName == 'PAR') |> drop_na(NP_Size_102) |> select(park_depth, all_of(lpm_class), dates = juld, park_depth, wmo, cycle) |> mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578)),
                                   park_depth = factor(park_depth, levels = c('200 m', '500 m', '1000 m')))
parking_spectra <- parking_data |> dplyr::select(NP_Size_256:NP_Size_1290)

parking_data <- parking_data |> mutate(guidi_flux = compute_flux(12.5, 3.81, parking_spectra)) |> group_by(park_depth, cycle, wmo) 

# plot it
comparison_flux_ost_uvp <- merge(parking_data, cflux) |> mutate(total_flux = small_flux + big_flux)

# https://stackoverflow.com/questions/41077199/how-to-use-r-ggplot-stat-summary-to-plot-median-and-quartiles
ggplot(comparison_flux_ost_uvp) + geom_line(aes(x = date, y = total_flux)) + facet_wrap(~wmo~park_depth, scales = 'free_y', nrow = 4) +
  geom_errorbar(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'red', width = 5) +
  geom_line(aes(x = date, y = guidi_flux),
                      stat = 'summary',
                      fun = median, colour = 'red') + scale_y_continuous(trans = 'log10') + theme_bw()

```

```{r, correlations between OST and UVP carbon fluxes}
#| label: tbl-correlation
#| tbl-cap: TODO
correlations <- comparison_flux_ost_uvp |> select(park_depth, wmo, cycle, guidi_flux, small_flux, big_flux, date, total_flux) |> group_by(wmo, park_depth, cycle) |> mutate(median_guidi_flux = median(guidi_flux)) |> group_by(park_depth, wmo) |> summarise(cor_small = round(cor(median_guidi_flux, small_flux),2), cor_big = round(cor(median_guidi_flux, big_flux),2), cor_total = round(cor(median_guidi_flux, total_flux),2))

correlations <- correlations |> pivot_wider(names_from = park_depth, values_from = c(cor_small, cor_big, cor_total))

gt(correlations) |> tab_spanner(label = "Small flux", columns = 2:4) |> tab_spanner(label = "Big flux", columns = 5:7) |> tab_spanner(label = 'Total flux', columns = 8:10) |>
  cols_label(wmo = 'WMO', `cor_small_200 m` = '200 m',
             `cor_small_500 m` = '500 m',
             `cor_small_1000 m` = '1000 m',
             `cor_big_200 m` = '200 m',
             `cor_big_500 m` = '500 m',
             `cor_big_1000 m` = '1000 m',
             `cor_total_200 m` = '200 m',
             `cor_total_500 m` = '500 m',
             `cor_total_1000 m` = '1000 m')

```

Figure @fig-comparison-ost-uvp shows a comparison of carbon fluxes estimated with the OST ($F_{total} = F_{small} + F_{big}$) and the UVP6 (median of all UVP measurements while the float was drifting) at all drifting depths. Overall, carbon fluxes estimated with the coefficients ($A$, $b$) of @Guidi2008-vp, are lower than the ones measured by the OST. Without additional measurements (e.g. co-deployed sediment traps), it is however impossible to decipher which measurements is closest to the truth. Nonetheless, the trends measured by both instruments are very well correlated at all drifting depths for floats 6904240 and 6904241 with correlations comprised between `r min(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,8:10])` and `r max(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,8:10])`. @tbl-correlation also shows that the correlation between UVP-derived carbon fluxes and $F_{small}$ and $F_{big}$ is `r mean(as.vector(as.matrix(correlations[correlations$wmo %in% c(6904240, 4903634),][1:2,2:7])))` on average. We calculated lower correlations with the other two floats, especially the float 1902578 at 200 m, where correlations between the UVP-derived carbon fluxes and all fluxes (small, big and total) are close to 0. 

Floats equipped with both a transmissometer and a UVP6 represent an opportunity to look at the influence of parts of the size spectrum on $F_{small}$ and $F_{big}$. For this, we decided to build a regression model using XGBoost, a gradient boosting method [REF], using biovolumes measured by the UVP to predict the slope of $F_{small}$ and $F_{big}$ measured by the OST.

```{r, model based on slope}

# compute biovolumes for mid size bin (ESD)
# DSE from 102 microns to 2.05 mm (this class to goes to 2.58 mm) (values obtained from Guidi's Matlab code converted on python)

# DSE <- c(0.11525597, 0.14521343, 0.18295745, 0.23051195, 0.29042685,
#               0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779,
#               1.16170742, 1.46365963, 1.84409558, 2.32341484)
# 
# # DSE obtained with Rainer's code
# DSE_Rainer <- c(0.11645117591608793,0.14636003747903648,0.18439151433041295,
#                 0.23251999827981656,0.2933257316240555,0.36916500651202966,0.4650399965596331,
#                 0.5860454935035602,0.738553273391974,0.9280422447589767,1.1705684285914486,
#                 1.4795320531800742,1.8636618038457486,2.3449457391257185)

DSE <- c(0.29042685,0.36591491, 0.46102389, 0.58085371, 0.73182981, 0.92204779, 1.16170742, 1.46365963)

# compute biovolume
get_biovolume <- function(DSE){
  r <- DSE/2
  biovolume <- (4*pi/3)*r**3
  return(biovolume)
}

# get biovolume for each DSE (equivalent to say for each size bin)
biovolumes <- map_dbl(DSE, get_biovolume)  # unit = m^3

# spectra at parking depth
uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |>
  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))

# convert abundance to biovolume
spectra_biovolume <- tibble(as.data.frame(t(t(uvp_spectra[,all_of(lpm_class)])*biovolumes)))
colnames(spectra_biovolume) <- colnames(spectra_biovolume)|> str_replace('Size', 'Biovolume')

# add data to initial abundance spectra
uvp_spectra <- cbind(uvp_spectra, spectra_biovolume)

# recompute slope based on Estapa's formula
cflux <- cflux |> mutate(small_slope = (small_flux/633)**(1/0.77), big_slope = (big_flux/633)**(1/0.77))

## build model
flux_model <- function(boosting_rounds, which_slope){
    
  # select which flux to build the model
  if(which_slope == 'small'){
    cflux2 <- cflux |> mutate(flux = log(small_slope)) |> filter(is.finite(flux))
  }else{
    cflux2 <- cflux |> mutate(flux = log(big_slope)) |> filter(is.finite(flux)) # because log(0) creates non finite value -> bug
  }
    
  # merge data for training
  data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)
    
  # training data
  tmp <- data_model |> select(flux, all_of(colnames(spectra_biovolume)))
  
  set.seed(101)
  # model
  model <- gbm(flux ~ ., data=tmp, distribution="gaussian",
  cv.folds=5,
  n.trees=boosting_rounds, shrinkage=0.005, interaction.depth=6,
  bag.fraction=0.5,
  n.cores=1
  )
   
  # get best model and thus best round number
  best <- gbm.perf(model)
  #print(best)
  
  # get features importance
  features_importance <- summary(model, n.trees=best, plotit = FALSE)
  
  return(list(features_importance = features_importance, model = model, best_tree = best))
}

# small_model <- flux_model(3000, 'small')
# big_model <- flux_model(3000, 'big')
# save(small_model, big_model, file = '/home/flo/dox/THESIS/DATA/MODEL/model_xgboost_data_slope')
  
load(file = 'DATA/MODEL/model_xgboost_data_slope')

# # get spectra
# uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |> 
#   drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))
# 
# flux_model <- function(boosting_rounds, which_flux){
#     
#     # select which flux to build the model
#     if(which_flux == 'small'){
#       cflux2 <- cflux |> mutate(flux = small_flux) 
#     }else if(which_flux == 'big'){
#       cflux2 <- cflux |> mutate(flux = big_flux)
#     }else{
#       cflux2 <- cflux |> mutate(flux = small_flux + big_flux)
#     }
#     
#     # merge data for training
#     data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)
#     
#   # training data
#   tmp <- data_model |> select(flux, all_of(lpm_class))
#   
#   set.seed(101)
#   # model
#   model <- gbm(flux ~ ., data=tmp, distribution="gaussian",
#   cv.folds=5,
#   n.trees=boosting_rounds, shrinkage=0.005, interaction.depth=6,
#   bag.fraction=0.5,
#   n.cores=1
#   )
#    
#   # get best model and thus best round number
#   best <- gbm.perf(model)
#   #print(best)
#   
#   # get features importance
#   features_importance <- summary(model, n.trees=best, plotit = FALSE)
#   
#   
#   return(list(features_importance = features_importance, model = model, best_tree = best))
#    
# }
#   
# # small_model <- flux_model(3000, 'small')
# # big_model <- flux_model(3000, 'big')
# # total_model <- flux_model(3000, 'total')
# # 
# # save(small_model, big_model, total_model, file = '/home/flo/dox/THESIS/DATA/MODEL/model_xgboost_data') 
#   
# load(file = 'DATA/MODEL/model_xgboost_data')

```

```{r, model predictions}

model_predict <- function(which_slope, mod, best_tree_number){

    # get spectra
    uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |>
  drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))

    # convert abundance to biovolume
    spectra_biovolume <- tibble(as.data.frame(t(t(uvp_spectra[,all_of(lpm_class)])*biovolumes)))
    colnames(spectra_biovolume) <- colnames(spectra_biovolume)|> str_replace('Size', 'Biovolume')

    # add data to initial abundance spectra
    uvp_spectra <- cbind(uvp_spectra, spectra_biovolume)

    # select which flux to build the model
    if(which_slope == 'small'){
      cflux2 <- cflux |> mutate(flux = small_slope)
    }else{
      cflux2 <- cflux |> mutate(flux = big_slope)
    }

    # merge data for training
    data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)

    # # training data
    # tmp <- data_model |> select(flux, all_of(colnames(spectra_biovolume)))

    # predictions on mean spectra values
    uvp_spectra_average <- data_model |> dplyr::group_by(wmo, cycle, depth) |> dplyr::summarize(depth = unique(depth), true_flux = log(unique(flux)),
                                                                                   NP_Biovolume_102 = mean(NP_Biovolume_102),
                                                                                   NP_Biovolume_128 = mean(NP_Biovolume_128),
                                                                                   NP_Biovolume_161 = mean(NP_Biovolume_161),
                                                                                   NP_Biovolume_203 = mean(NP_Biovolume_203),
                                                                                   NP_Biovolume_256 = mean(NP_Biovolume_256),
                                                                                   NP_Biovolume_323 = mean(NP_Biovolume_323),
                                                                                   NP_Biovolume_406 = mean(NP_Biovolume_406),
                                                                                   NP_Biovolume_512 = mean(NP_Biovolume_512),
                                                                                   NP_Biovolume_645 = mean(NP_Biovolume_645),
                                                                                   NP_Biovolume_813 = mean(NP_Biovolume_813),
                                                                                   NP_Biovolume_1020 = mean(NP_Biovolume_1020),
                                                                                   NP_Biovolume_1290 = mean(NP_Biovolume_1290),
                                                                                   NP_Biovolume_1630 = mean(NP_Biovolume_1630),
                                                                                   NP_Biovolume_2050 = mean(NP_Biovolume_2050)) |> ungroup()

  # prediction
  pred_flux <- predict(mod, newdata = uvp_spectra_average, n.trees = best_tree_number)

  uvp_spectra_average$pred_flux <- pred_flux

  return(uvp_spectra_average)
}

# predictions
# pred_small <- model_predict('small', small_model$model, small_model$best_tree) |> select(wmo, cycle, depth, true_small = true_flux, pred_small = pred_flux)
# pred_big <- model_predict('big', big_model$model, big_model$best_tree) |> select(true_big = true_flux, pred_big = pred_flux) 
# all_predictions <- cbind(pred_small, pred_big)
# vroom::vroom_write(all_predictions, file = '/home/flo/dox/THESIS/DATA/MODEL/all_predictions_slopes.csv')
all_predictions <- vroom('DATA/MODEL/all_predictions_slopes.csv') 

# model_predict <- function(which_flux, mod, best_tree_number){
# 
#     # get spectra
#     uvp_spectra <- all_floats |> filter(PhaseName == 'PAR') |> mutate(month = month(juld), week = week(juld), DOY = yday(juld), short_date = ymd(juld)) |> select(juld, month, week, DOY, short_date,  cycle, wmo, park_depth, all_of(lpm_class)) |>
#   drop_na(NP_Size_102) |> mutate(juld = as_date(juld), WMO = factor(wmo))
# 
#     # select which flux to build the model
#     if(which_flux == 'small'){
#       cflux2 <- cflux |> mutate(flux = small_flux)
#     }else if(which_flux == 'big'){
#       cflux2 <- cflux |> mutate(flux = big_flux)
#     }else{
#       cflux2 <- cflux |> mutate(flux = small_flux + big_flux)
#     }
# 
#     # merge data for training
#     data_model <- merge(cflux2, uvp_spectra) |> mutate(depth = if_else(park_depth == '1000 m', 1000, if_else(park_depth == '500 m', 500, 200))) |> select(-park_depth)
# 
#   # predictions on mean spectra values
#   uvp_spectra_average <- data_model |> dplyr::group_by(wmo, cycle, depth) |> dplyr::summarize(depth = unique(depth), true_flux = unique(flux),
#                                                                                    NP_Size_102 = mean(NP_Size_102),
#                                                                                    NP_Size_128 = mean(NP_Size_128),
#                                                                                    NP_Size_161 = mean(NP_Size_161),
#                                                                                    NP_Size_203 = mean(NP_Size_203),
#                                                                                    NP_Size_256 = mean(NP_Size_256),
#                                                                                    NP_Size_323 = mean(NP_Size_323),
#                                                                                    NP_Size_406 = mean(NP_Size_406),
#                                                                                    NP_Size_512 = mean(NP_Size_512),
#                                                                                    NP_Size_645 = mean(NP_Size_645),
#                                                                                    NP_Size_813 = mean(NP_Size_813),
#                                                                                    NP_Size_1020 = mean(NP_Size_1020),
#                                                                                    NP_Size_1290 = mean(NP_Size_1290),
#                                                                                    NP_Size_1630 = mean(NP_Size_1630),
#                                                                                    NP_Size_2050 = mean(NP_Size_2050)) |> ungroup()
# 
# 
#   # prediction at 1000 m
#   #spectra <- uvp_spectra_average |> select(true_flux, all_of(lpm_class[]))
# 
#   # prediction
#   pred_flux <- predict(mod, newdata = uvp_spectra_average, n.trees = best_tree_number)
# 
#   uvp_spectra_average$pred_flux <- pred_flux
# 
#   return(uvp_spectra_average)
# }
# 
# # pred_small <- model_predict('small', small_model$model, small_model$best_tree) |> select(wmo, cycle, depth, true_small = true_flux, pred_small = pred_flux)
# # pred_big <- model_predict('big', big_model$model, big_model$best_tree) |> select(true_big = true_flux, pred_big = pred_flux)
# # pred_total <- model_predict('total', total_model$model, total_model$best_tree) |> select(true_total = true_flux, pred_total = pred_flux)
# #
# # all_predictions <- cbind(pred_small, pred_big, pred_total) |> mutate(pred_total2 = pred_small + pred_big, anom = pred_total - pred_total2)
# #
# # vroom::vroom_write(all_predictions, file = '/home/flo/dox/THESIS/DATA/MODEL/all_predictions.csv')
# all_predictions <- vroom('DATA/MODEL/all_predictions.csv')

```

```{r, plot model predictions}

p_small <- ggplot(all_predictions) + geom_point(aes(true_small, pred_small)) + theme_bw() 
round(R2_Score(y_true = all_predictions$true_small, y_pred = all_predictions$pred_small), 2)

p_big <- ggplot(all_predictions |> filter(is.finite(true_big))) + geom_point(aes(true_big, pred_big)) + theme_bw()
all_predictions |> filter(is.finite(true_big)) |> summarize(round(R2_Score(true_big, pred_big)), 2)

p_small / p_big

```


TODO :

-   science: on voit le système latent, on a raté le bloom
-   rajouter les vitesses de sédimentations (n'importe quelle méthode fera l'affaire) -\> permet d'utiliser le graphique de l'UVP en vertical -\> hop une justification en plus
-   rajouter dans les méthodes les produits satellites qu'on utilise
-   différentes dynamiques post-bloom
-   phénologie du bloom printanier
- 

### Environmental conditions prior to bloom

-   aller chercher les produits satellitaires pour montrer que l'on a bien raté le bloom ou que les conditions du bloom n'étaient pas réunies

```{r, chla corrected for NPQ and MLD}

# fetch data
npq_data <- vroom('DATA/ARGO/npq_corrected_data.csv')  

# clean and smoothed chla data
d <- npq_data |> select(cycle, MLD, pres, juld, wmo, chla_npq) |> mutate(date = as_date(juld))
d <- d |> group_by(date, wmo, pres, cycle) |> summarize(mean_chla = mean(chla_npq, na.rm=T), mld = unique(MLD))
d <- d |> mutate(smoothed_chla = smooth(mean_chla)/2.15, smoothed_chla_up = smooth(mean_chla)/(2.15-0.37), smoothed_chla_down = smooth(mean_chla)/(2.15+0.37)) # 2.15 = Roesler factor in the Labrador Sea

#d$wmo <- factor(d$wmo, levels = c(6904240,4903634,6904241,1902578))

# plot it
ggplot(d) + geom_point(aes(x = date, y = pres, colour = log10(smoothed_chla))) +
  scale_y_reverse(limits = c(200,0)) +
  #scale_colour_viridis(option = 'A') +
  scale_colour_cmocean(name = 'algae', oob = scales::squish, limits = c(-1,1), na.value = 'grey') +
  scale_x_date(labels = date_format("%m"), date_breaks = '1 month') +
  labs(x = 'Time', y = 'NPQ-corrected Chla') + facet_wrap(~wmo) + theme_bw() +
  geom_line(aes(x = date, y = mld), colour = 'black', size = 1)

# compute chla integration in the MLD and in the 0-100 m zone
d_chla <- d |> group_by(wmo, date) |> summarize(chla_0_100m = integrate(smoothed_chla, pres, from=0, to=100),
                                       mld = unique(mld),
                                       chla_0_mld = integrate(smoothed_chla, pres, from = 0, to=mld),
                                       median_chla_0_20m = median(smoothed_chla[pres<=20], na.rm=T),
                                       median_chla_0_20m_up = median(smoothed_chla_up[pres<=20], na.rm=T),
                                       median_chla_0_20m_down = median(smoothed_chla_down[pres<=20], na.rm=T))
# 
# ggplot(d_chla) + geom_point(aes(date, chla_0_100m)) + facet_wrap(~wmo, scales = 'free') + theme_bw() + labs(x = 'Month', y = 'Integrated Chla 0-100 m (CHLA UNITS)')
#ggplot(d_chla) + geom_point(aes(date, chla_0_mld)) + facet_wrap(~wmo, scales = 'free')

# add couple info
d_chla <- d_chla |> mutate(group = if_else(wmo %in% c(6904240,4903634), '6904240 - 4903634', '6904241 - 1902578'))

```

```{r, chla and MLD from satellite}

#chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_with_clouds_sat.feather') |> select(time, lat, lon, chla = CHL)
chla_sat <- arrow::read_feather('DATA/SATELLITE/chla_without_clouds_sat.feather') |> select(time, lat, lon, chla = CHL)
mld_sat <- arrow::read_feather('DATA/SATELLITE/mld_sat.feather') |> mutate(longitude = longitude - 360) |> select(time, lat = latitude, lon = longitude, mld = mlotst)
  
# CHLA
chla_cover_deployment1 <- points_in_circle(chla_sat, lon_center = -49.2, lat_center = 58.9, radius = 100000) # 50 km around center
chla_cover_deployment1 <- chla_cover_deployment1 |> mutate(date = as_date(time), group = '6904240 - 4903634') |> mutate(deploy = as_date('2022-05-22'))

chla_cover_deployment2 <- points_in_circle(chla_sat, lon_center = -52.3, lat_center = 56.7, radius = 10000) # 50 km around center
chla_cover_deployment2 <- chla_cover_deployment2 |> mutate(date = as_date(time), group = '6904241 - 1902578') |> mutate(deploy = as_date('2022-05-30'))

# all deploy
chla_cover <- rbind(chla_cover_deployment1, chla_cover_deployment2)

chla_plot <- ggplot(chla_cover |> filter(date > '2022-04-01')) + geom_errorbar(aes(x = date, y = chla),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'black', width = .5) + theme_bw() +
  labs(x = 'Time', y = 'Chla')+ facet_wrap(~group) +
  geom_vline(data = chla_cover, aes(xintercept = deploy), colour = "black", linetype = 'dashed') +
  geom_pointrange(data = d_chla |> filter(date < '2022-08-01'), aes(x = date, ymin = median_chla_0_20m_down, ymax = median_chla_0_20m_up, y = median_chla_0_20m), colour = "red", size = .25)
                
# MLD
mld_cover_deployment1 <- points_in_circle(mld_sat, lon_center = -49.2, lat_center = 58.9, radius = 100000) # 50 km around center
mld_cover_deployment1 <- mld_cover_deployment1 |> mutate(date = as_date(time), group = '6904240 - 4903634') |> mutate(deploy = as_date('2022-05-22'))

mld_cover_deployment2 <- points_in_circle(mld_sat, lon_center = -52.3, lat_center = 56.7, radius = 100000) # 50 km around center
mld_cover_deployment2 <- mld_cover_deployment2 |> mutate(date = as_date(time), group = '6904241 - 1902578') |> mutate(deploy = as_date('2022-05-30'))

mld_cover <- rbind(mld_cover_deployment1, mld_cover_deployment2)

mld_plot <- ggplot(mld_cover |> filter(date > '2022-04-01')) + geom_errorbar(aes(x = date, y = mld),
                      stat = 'summary',
                      fun.min = function(z) {quantile(z, 0.25)},
                      fun.max = function(z) {quantile(z, 0.75)},
                      fun = median, colour = 'black', width = 5) +
  labs(x = 'Time', y = 'MLD (m)') + facet_wrap(~group) + theme_bw() +
  geom_vline(data = chla_cover, aes(xintercept = deploy), colour = "black", linetype = 'dashed') +
  geom_point(data = d_chla |> filter(date < '2022-08-01') |> mutate(week = week(date), DOY = yday(date)) |> group_by(week, group) |> summarize(median_mld = median(mld), mean_date = as_date(mean(DOY), origin = '2022-01-01')), aes(mean_date, median_mld), colour = "red")

chla_plot / mld_plot

```

with a radius of 100 km around both deployment positions

### Sinking speed

-   Montrer que les sinking speed observées ne sont pas compatibles avec des valeurs de bloom (càd rapides pour les grosses particules à minima)

```{r, sinking speed plot}

sinking_fit <- vroom('DATA/ARGO/gaussian_fit_sinking_speed_full.csv') |> mutate(size = size_class |> str_remove('NP_Size_') |> as.numeric(), date = as_date(date))

estimate_sinking_speed <- function(siz, data, threshold){
  
  # above 500 m
  tmp <- data |> filter(size == siz) |> filter(depth <= threshold)
  model <- lm(formula = depth ~ time_max_abundance, data = tmp)
  tmp$sinking_speed_above_threshold <- model$coefficients[2] # equivalent to slope of the fit
  tmp2 <- tmp
  
  # below 500 m
  tmp <- data |> filter(size == siz) |> filter(depth > threshold)
  model <- lm(formula = depth ~ time_max_abundance, data = tmp)
  tmp2$sinking_speed_below_threshold <- model$coefficients[2] # equivalent to slope of the fit
  
  return(tmp2)
}

# sinking speed for each float
sinking_6904240 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 6904240), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
sinking_6904241 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 6904241), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
sinking_1902578 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 1902578), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
sinking_4903634 <- map_dfr(unique(sinking_fit$size), estimate_sinking_speed, sinking_fit |> filter(wmo == 4903634), 400) |> select(wmo, sinking_speed_above_threshold, sinking_speed_below_threshold, size) |> distinct()
all_sinking_speed <- rbind(sinking_1902578, sinking_4903634, sinking_6904240, sinking_6904241)

```

```{r, vertical plots + sinking speed}

particles <- all_floats |> filter(PhaseName == 'NPAR') |> select(juld, cycle, depth = pres, wmo, all_of(lpm_class), MLD) |> drop_na(NP_Size_102) |> mutate(juld = as_date(juld))

particles_bis <- particles |> pivot_longer(cols = all_of(lpm_class), names_to = 'size', values_to = 'concentration') |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric())
#particles_bis$size <- factor(particles_bis$size, levels = lpm_class)

# rel_conc
particles_bis <- particles_bis |> group_by(wmo, size) |> mutate(rel_conc = (concentration-min(concentration))/(max(concentration) - min(concentration)))

up_part <- particles_bis |> filter(depth < 2000, size < 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)') +
  geom_pointrange(data = sinking_fit |> filter(size < 512), aes(xmin = as_date(xmin, origin = '2022-01-01'), xmax = as_date(xmax, origin = '2022-01-01'), x = as_date(DOY, origin = '2022-01-01'), y = depth), size = .2, colour = 'white') 

up_part

down_part <- particles_bis |> filter(depth < 2000, size > 512) |>
  #group_by(size) |>
  ggplot() + facet_grid(wmo~size) + theme_bw() +
  geom_point(aes(x=juld, y=depth, colour=log10(rel_conc)), size=1, shape=15) +
  scale_colour_viridis(option = 'A', limits = c(-2.5,-1), oob = scales::squish, na.value = 'grey', direction = -1) + 
  scale_y_reverse() +
  scale_x_date(labels = date_format("%m"), date_breaks = '3 month') +
  labs(colour = '#/L', x = 'Month', y = 'Depth (m)') +
  geom_pointrange(data = sinking_fit |> filter(size > 512), aes(xmin = as_date(xmin, origin = '2022-01-01'), xmax = as_date(xmax, origin = '2022-01-01'), x = as_date(DOY, origin = '2022-01-01'), y = depth), size = .2, colour = 'white') 

down_part

```
```{r, sinking speed values}

gt(all_sinking_speed)

```


### TS diagram

-   Pour le drift à 200, 500 et 1000 m

```{r, TS diagram}

#ts_data <- all_floats |> filter(PhaseName == 'NPAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth) |> drop_na(temp) |> filter(sigma > 26) |> mutate(date = as_date(date))

ts_data <- all_floats |> filter(PhaseName == 'PAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth) |> drop_na(temp) |> filter(sigma > 26, park_depth != '200 m') |> mutate(date = as_date(date))
#ts_data <- all_floats |> filter(PhaseName == 'PAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth) |> drop_na(temp) |> filter(sigma > 26) |> mutate(date = as_date(date))
ggplot(ts_data) + geom_point(aes(psal, temp, colour = factor(park_depth))) + theme_bw()

# ggplot(ts_data) + geom_point(aes(psal, temp, colour = wmo))

ts_data <- all_floats |> filter(PhaseName == 'NPAR') |> select(date = juld, depth = pres, temp, psal, wmo, sigma, park_depth) |> drop_na(temp) |> filter(sigma > 26) |> mutate(date = as_date(date))

# add contour
# breaks_sigma <- seq(min(round(ts_data$sigma*2)/2), max(round(ts_data$sigma*2)/2), by = 1)
ggplot(ts_data) + geom_point(aes(psal, temp, colour = depth)) + scale_color_viridis_c(option = 'A') + theme_bw()

```

### Processes at play

J'ai enlevé le truc du spectre inversé à 500 et 1000 m car difficile de conclure -> mais comme on est dans la meme masse d'eau, on pourrait le garder, why not

```{r, monthly anomaly between drifting at 500 and 1000 m}
#| label: fig-processes-at-play
#| fig-cap: Ratio of monthly mean particles concentrations during drifting (1000 m over 500 m) for size classes below 1.63 mm. 

# tmp <- uvp_data |> filter(park_depth != '200 m') |> group_by(wmo, size, month, park_depth) |> 
#   summarize(monthly_mean_conc = mean(conc), monthly_median_conc = median(conc)) |>
#   # compute concentration anomaly at 500 and 1000 m
#   mutate(diff_anomaly = monthly_mean_conc - lead(monthly_mean_conc), div_anomaly = monthly_mean_conc/lead(monthly_mean_conc)) |> drop_na(diff_anomaly) |> #filter(is.finite(div_anomaly)) |>
#   select(wmo, size, month, diff_anomaly, div_anomaly) |> mutate(month = month.abb[month]) |> 
#   mutate(size = as.character(size)) |>
#   mutate(rev_div_anomaly = 1/div_anomaly) |> 
#   #filter(is.finite(rev_div_anomaly)) |>
#   mutate(diff_anomaly = round(diff_anomaly, 1), rev_div_anomaly = round(rev_div_anomaly, 1), div_anomaly = round(div_anomaly, 1)) |>
#   ungroup()

# test en weekly
# tmp <- uvp_data |> filter(park_depth != '200 m', month != 5) |> group_by(wmo, size, week, park_depth) |> 
#   summarize(monthly_mean_conc = mean(conc), monthly_median_conc = median(conc)) |>
#   # compute concentration anomaly at 500 and 1000 m
#   mutate(diff_anomaly = monthly_mean_conc - lead(monthly_mean_conc), div_anomaly = monthly_mean_conc/lead(monthly_mean_conc)) |> drop_na(diff_anomaly) |> #filter(is.finite(div_anomaly)) |>
#   select(wmo, size, week, diff_anomaly, div_anomaly) |> 
#   mutate(size = as.character(size)) |>
#   mutate(rev_div_anomaly = 1/div_anomaly) |> 
#   #filter(is.finite(rev_div_anomaly)) |>
#   mutate(diff_anomaly = round(diff_anomaly, 1), rev_div_anomaly = round(rev_div_anomaly, 1)) |>
#   ungroup()
# 
# create_data_for_heatmap <- function(float){
#   y <- unique(tmp$size)[1:12] # above, data is way too low
#   d <- tmp |> filter(wmo == float, size %in% y, week > 5)
#   x <- unique(d$week)
#   data <- expand.grid(X=x, Y=y)
#   #data$Z <- tt$diff_anomaly
#   data$Z <- d$rev_div_anomaly
#   data$wmo <- float
#   return(data)
# }

# d1 <- create_data_for_heatmap(6904241)
# d2 <- create_data_for_heatmap(1902578)
# d3 <- create_data_for_heatmap(4903634)
# d4 <- create_data_for_heatmap(6904240)
# 
# dd <- rbind(d1, d2, d3, d4)

# ggplot(dd, aes(X, Y, fill= Z)) + 
#   geom_tile() + scale_fill_viridis() + facet_wrap(~wmo)

create_data_for_heatmap <- function(float, df){
  y <- unique(df$size)[1:12] # above, data is way too low
  d <- df |> filter(wmo == float, size %in% y)
  #x <- c('Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan')
  x <- unique(d$month)
  data <- expand.grid(X=x, Y=y)
  #data$Z <- tt$diff_anomaly
  data$Z <- d$rev_div_anomaly
  #data$Z <- d$div_anomaly
  data$wmo <- float
  return(data)
}

# for some reason, there is some bug with the quarto compilation .. so I save the tibble .. and reload it 
# d1 <- create_data_for_heatmap(6904241, tmp)
# d2 <- create_data_for_heatmap(1902578, tmp)
# d3 <- create_data_for_heatmap(4903634, tmp)
# d4 <- create_data_for_heatmap(6904240, tmp)
# dd <- rbind(d1, d2, d3, d4)
# vroom_write(dd, 'heatmap_data.csv')

# reload + redo some annoying stuff that were lost during the writing of the csv
dd <- vroom('heatmap_data.csv') |> mutate(Y = as.character(Y)) 
dd$X <- factor(dd$X, levels = c('May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan'))
dd$Y <- factor(dd$Y, levels = c('102', '128', '161', '203', '256', '323', '406', '512', '645', '813', '1020', '1290'))
dd$Y <- factor(dd$Y, levels = c('1290', '1020', '813', '645', '512', '406', '323', '256', '203', '161', '128', '102'))
# dd$wmo <- factor(dd$wmo, levels = c(6904240,4903634,6904241,1902578))

#dd <- dd |> mutate(Z_binned = cut(Z, breaks = c(0,1,1.5,2.5,max(Z))))
dd <- dd |> mutate(Z_binned = cut(Z, breaks = c(0,.5,1,1.5,2,max(Z))))

# ggplot(dd) +
#   geom_tile(aes(X, Y, fill = Z), alpha = 0.75) + scale_fill_viridis() + theme_bw() + labs(x = 'Month', y = 'Size class') +
#   facet_wrap(~wmo) +
#   geom_text(data = dd, aes(X, Y, label = Z))

ggplot(dd) + 
  geom_tile(aes(X, Y, fill = Z_binned), colour = 'black', alpha = 1) + theme_bw() + labs(fill = 'Ratio', x = 'Month', y = 'Size class') +
  facet_wrap(~wmo) + scale_fill_viridis_d(option = 'A')

```

```{r, test gt table}

monthly_mean <- uvp_data |> mutate(month = month.abb[month(juld)]) |> group_by(size, park_depth, month) |> summarize(mean_conc = round(mean(conc, na.rm=T), 2)) |> ungroup()

# pivot wider
monthly_mean <- monthly_mean |> pivot_wider(names_from = month, values_from = mean_conc) |> select(size, depth = park_depth, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, Jan, Feb) 

monthly_mean |> group_by(size) |> gt(rowname_col = 'depth')

#monthly_mean |> gt(rowname_col = c('size')) |> tab_stubhead(label = 'Class') 

```

#### Features importance

```{r, features importance}
# 
# features <- small_model$features_importance |> select(size = var, importance = rel.inf) |> mutate(model = 'small') |>
#   bind_rows(big_model$features_importance |> select(size = var, importance = rel.inf) |> mutate(model = 'big')) |>
#   bind_rows(total_model$features_importance |> select(size = var, importance = rel.inf) |> mutate(model = 'total'))
# 
# rownames(features) <- NULL
# 
# features <- features |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric())
# 
# ggplot(features) + geom_point(aes(x = as.factor(size), y = importance, colour = model), size = 5) + labs(x = 'Size', y = 'Features importance')

```

#### Predictions

```{r, plot predictions}

# p_small <- ggplot(all_predictions) + geom_point(aes(true_small, pred_small)) + xlim(c(0,25)) + ylim(c(0,25)) + theme_bw() +
# 
#   annotate("text", x = 3, y = 23, label = paste0('R² = ' , round(R2_Score(all_predictions$true_small, all_predictions$pred_small), 2)))
# 
# p_big <- ggplot(all_predictions) + geom_point(aes(true_big, pred_big)) + xlim(c(0,50)) + ylim(c(0,50)) + theme_bw() +
# 
#   annotate("text", x = 5, y = 45, label = paste0('R² = ' , round(R2_Score(all_predictions$true_big, all_predictions$pred_big), 2)))
# 
# p_total <- ggplot(all_predictions) + geom_point(aes(true_total, pred_total)) + xlim(c(0,100)) + ylim(c(0,100)) + theme_bw() +
# 
#   annotate("text", x = 3, y = 80, label = paste0('R² = ' , round(R2_Score(all_predictions$true_total, all_predictions$pred_total), 2)))
# 
# (p_small + p_big) / p_total

# plot(test$true_flux, y=test$pred_flux, asp=1)

# print(R2_Score(test$pred_flux, test$true_flux))

```

#### same plot mais pour les classes de tailles

```{r, TEST }

# depth_bin <- c(seq(0,500,50), seq(600, 1000, 100), seq(1250, 2000, 250))
# 
# part_distrib <- vertical_profiles |> filter(depth < 2000) |> select(juld, depth, wmo, NP_Size_256) |> mutate(depth_bin = cut(depth, breaks = depth_bin)) |>
# 
#   mutate(month = month.abb[month(juld)]) |> ungroup()
# 
# tmp <- part_distrib |> group_by(wmo, depth_bin, month) |> summarize(mean_part = log10(mean(NP_Size_256))) |>
# 
#   mutate(cut_part = cut(mean_part, breaks = c(seq(0,100,20),+Inf))) |> ungroup()
# 
# create_data_for_heatmap_bis <- function(float, df){
# 
#   df <- df |> filter(wmo == float) |> select(mean_part, depth_bin, month)
# 
#   y <- unique(df$depth_bin)
# 
#   df <- df|> pivot_wider(names_from = depth_bin, values_from = mean_part)
# 
#   x <- unique(df$month)
# 
#   data <- expand.grid(X=x, Y=y)
# 
#   data$Z <- as.vector(as.matrix(df |> select(-month)))
# 
#   data$wmo <- float
# 
#   return(data)
# 
# }
# 
# d1 <- create_data_for_heatmap_bis(6904241, tmp)
# 
# d2 <- create_data_for_heatmap_bis(1902578, tmp)
# 
# d3 <- create_data_for_heatmap_bis(4903634, tmp)
# 
# d4 <- create_data_for_heatmap_bis(6904240, tmp)
# 
# dd <- rbind(d1, d2, d3, d4) |> mutate(X = factor(X, levels = c('May','Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan'))) |>
# 
#   mutate(Y = factor(Y, levels = sort(unique(Y), decreasing = T))) |>
# 
#   mutate(wmo = factor(wmo, levels = c(6904240,4903634,6904241,1902578))) #|>
# 
#   #mutate(Z = factor(Z, levels = levels(tmp$cut_part)))
# 
# ggplot(dd) +
# 
#   geom_tile(aes(X, Y, fill = Z), colour = 'black', alpha = 1) + theme_bw() + labs(fill = 'Log10(abundance) #/L', x = 'Month', y = 'Depth bin', title = 'NP_Size_256') +
# 
#   facet_wrap(~wmo) + scale_fill_viridis(option = 'A')

```

#### same plots mais par mois


```{r, monthly heatmap}

# depth_bin <- c(seq(0,500,50), seq(600, 1000, 100), seq(1250, 2000, 250))
# 
# part_distrib <- vertical_profiles |> filter(depth < 2000) |> select(juld, depth, wmo, all_of(lpm_class)) |> mutate(depth_bin = cut(depth, breaks = depth_bin)) |> mutate(month = month.abb[month(juld)]) |> ungroup() |> pivot_longer(cols = all_of(lpm_class), names_to = 'size', values_to = 'concentration')
# 
# tmp <- part_distrib |> group_by(depth_bin, size, month) |> summarize(mean_part = mean(concentration)) |> mutate(rel_mean_conc = (mean_part-min(mean_part))/(max(mean_part) - min(mean_part))) |> ungroup()
# 
# plot_heat_month <- function(m){
# 
#   # transform data for heatmap
# 
#   #m <- 'May'
# 
#   tmp2 <- tmp |> filter(month == m) |> mutate(size = size |> str_remove('NP_Size_') |> as.numeric()) |> select(-c(month, mean_part))
# 
#   y <- unique(tmp2$depth_bin)
# 
#   x <- unique(tmp2$size)
# 
#   test <- tmp2 |> pivot_wider(names_from = depth_bin, values_from = rel_mean_conc)
# 
#   #test <- tmp2 |> pivot_wider(names_from = depth_bin, values_from = mean_part)
# 
#   data <- expand.grid(X=x, Y=y)
# 
#   data$Z <- as.vector(as.matrix(test |> select(-size)))
# 
#   data$month <- m
# 
#   return(data)
# 
# }
# 
# m_data <- map_dfr(c('May','Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan'), plot_heat_month)
# 
# m_data <- m_data |> mutate(Y = factor(Y, levels = sort(unique(Y), decreasing = T)),
# 
#                    month = factor(month, levels = c('May','Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan')))
# 
# ggplot(m_data |> mutate(Y = factor(Y, levels = sort(unique(Y), decreasing = T)))) +
# 
#   geom_tile(aes(as.factor(X), Y, fill = Z), colour = 'black', alpha = 1) + theme_bw() + scale_fill_viridis(option = 'A') +
# 
#   facet_wrap(~month)

```


## Discussion2

PARLER DU TRADEOFF BETWEEN SMALL FLUX AND BIG PARTICLE FLUX

The latter are however subject to a tradeoff between the likely underestimation of $F_{small}$ and the detection of false jumps (without great impact on the value of $F_{big}$ because false jumps have low $c_{p}$ values) if the threshold is too small and the likely overestimation of $F_{small}$ to the detriment of $F_{big}$ with missed jumps if the threshold is too high.

## Conclusion
