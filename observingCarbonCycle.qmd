# Modeling and observing the BCP {#sec-modelling-observing-BCP}

## Biogeochemical models {#sec-biogeochem-models}

The study of the BCP is of paramount importance to answer the question of how much carbon is exported and how much of the exported carbon will be sequestered. In order to achieve this goal, ocean modelers have developed different kinds of biogeochemical models to better quantify the different export pathways of the BCP.

Because measuring directly the POC export flux would be challenging, it is common to estimate the fraction of the total NPP that is exported (i.e. export efficiency) [@Henson2011-mt]. This fraction is usually referred to as the $e-ratio$ (see Information Box 2 and @fig-export-ratios).

![$e-ratio$ estimations obtained with different types of models: empirical models (a, b), food-web models (c, d) and data-assimilated models (e, f). Note that the export value computed by @Laws2011-nw (9 Pg C y$^{-1}$) was recomputed by @Siegel2022-wd (6.9 Pg C y$^{-1}$), therefore the $e-ratio$ map presented here might not be representative of the one we would have got with with the initial export value. Figure taken from @Siegel2022-wd.](image/ObservingCarbonCycle/export_ratio.jpeg){#fig-export-ratios}

::: {.blackbox data-latex=""}
::: {.flushleft data-latex=""}
**Information Box 2.** **Net primary production and export ratio**
:::

Total net primary production includes new production (NP, mainly fueled from the upwelling of nitrogen from deeper water to the EZ) and regenerated production (RP, fueled with nitrogen recycled through the food web within the EZ) [@Dugdale1967-ut]. The ratio of new production over total production is referred to as the $f-ratio$ [@Eppley1979-pj].

$$
f-ratio = \frac{NP}{NP+RP}
$$

In a steady state system, new production equals export production therefore the $f-ratio$ should be equal to the ratio of export production (EP) to total NPP, referred to as the $e-ratio$ by @Laws2000-jl.

$$
e-ratio = \frac{EP}{NPP}
$$
:::

I will hereafter briefly describe a non exhaustive list of biogeochemical models to illustrate how the export production can be estimated using the $e-ratio$.

@Henson2011-mt and @Laws2011-nw (panels a and b in @fig-export-ratios) are both empirical models. @Henson2011-mt parameterized the $e-ratio$ as a decreasing function of SST (derived from satellite observations, see @fig-bgc-models) tuned on a compilation of $^{234}Th$-derived export measurement (see @sec-historical-measurements) for a total export estimated at 5 Pg C y$^{-1}$. @Laws2011-nw modelled the $e-ratio$ as a decreasing linear function of SST and a power-law function of NPP whose parameters were determined using particle export estimates based on estimates of new production (nitrogen-based) and $^{234}Th$ and sediment trap export flux data compiled by @Dunne2005-an. They estimated a total export of 9 Pg C y$^{-1}$.

@Siegel2014-wg and @Archibald2019-dj (panels c and d in @fig-export-ratios) are both food-web models (see Information Box 3). @Siegel2014-wg used a mechanistic approach using satellite observations (NPP, PSS and phytoplankton carbon concentrations, see also @fig-bgc-models) to estimate the export of the BGP (6 Pg C y$^{-1}$). @Archibald2019-dj added a new module to the model of @Siegel2014-wg in order to incorporate the diel vertical migration of zooplankton for a total export of 6.5 Pg C y$^{-1}$.

::: {.blackbox data-latex=""}
::: {.flushleft data-latex=""}
**Information Box 3.** **Food-web model**
:::

A simple food-web model is the NPZD model, which consists of four state variables: Nutrient (DIN), Phytoplankton (PHYTO), Zooplankton (ZOO) and DETRITUS. The arrows represent the flows (or interactions) between the state variables (e.g. f1 represents the DIN uptake by PHYTO, f2 represents the grazing of PHYTO by ZOO, etc.), describing the sources and sinks of a given state variable [@Soetaert2008-er]. Forcing functions (or external variables) are external factors (i.e. not computed by the model) imposed (i.e. forced) to the model because they drive or regulate the system. Examples of forcing functions (among many others) in ecological models are light intensity (represented here by the solar radiation), temperature or wind. Finally, a model usually contains output variables (represented here by the Chlorophyll) that allow the comparison of the model to reality @Soetaert2008-er.

::: {.center data-latex=""}
![](image/ObservingCarbonCycle/npzd_no_background.png)
:::

Figure taken from [@Soetaert2008-er].
:::

@DeVries2017-wu and @Nowicki2022-no (panels e and f in @fig-export-ratios) are both data-assimilated models. These models operate by nudging model solutions to the observed fields (e.g. satellite and/or in situ data) or by optimizing model parameters to the observed fields [@Siegel2022-wd]. @DeVries2017-wu combined both satellite-derived data (NPP and PSS) and in situ observations (POC fluxes, DOC concentrations and dissolved $O_{2}$ distributions) to assess the carbon export and its transfer in the mesopelagic zone using a steady state ocean circulation inverse model (OCIM, the circulation component of their model). Their carbon export estimate is between 7.3 and 8.6 Pg C y$^{-1}$. @Nowicki2022-no added a two-phytoplankton/two-zooplankton food-web model with a migration zooplankton component to the model of @DeVries2017-wu in order to estimate the magnitude of each export pathway of the BCP. In all, @Nowicki2022-no estimated a carbon export of 10 Pg C y$^{-1}$.

### Model uncertainties {#sec-model-uncertainties}

When we compare the $e-ratio$ maps (@fig-export-ratios) or the carbon export estimations of just six biogeochemical models (see @sec-biogeochem-models), we already observe strong differences. In my opinion, it means at least two things. On the one hand, we are seeing an increase in model complexity from empirical models to data-assimilated inverse models. At the same time, we also gathered more observational data as well as a deeper understanding of the BCP (@sec-a-new-paradigm-for-the-bcp) that allow us to better represent its underlying processes. On the other hand, each degree of complexity propagates more and more uncertainty (@fig-bgc-models) that we need to account for in the results.

![Top of the atmosphere (TOA) radiance is converted into remote sensing reflectance $R_{rs}(\lambda)$, which is the ratio of the solar radiation reflected from the surface of the ocean that penetrates the sea surface at a discrete wavelength $\lambda$. Using TOA and $R_{rs}(\lambda)$, primary products (yellow circles) can be derived (e.g. POC, inherent optical properties (IOP), chlorophyll (Chl) or sea surface temperature (SST). Those primary products can be used in numerical models (e.g. empirical relationships) to compute secondary products (orange and red circles) such as the phytoplankton commmunity composition (PCC), particle size spectra (PSS), phytoplankton carbon concentrations (C$_{phyto})$ and the NPP. Models (light blue circles) therefore incorporate primary and secondary products as well as ancillary data (red squares) either from satellite (e.g. SST), in situ data or model-based data (ocean circulation) to estimate carbon export and sequestration (dark blue circles). Figure taken from @Siegel2022-wd.](image/ObservingCarbonCycle/carbon_models.jpeg){#fig-bgc-models width="750" height="580"}

## Observation tools {#sec-observation-tools}

### Historical measurements {#sec-historical-measurements}

In the 20th century, the downward POC flux was traditionally measured with sediment traps. Those instruments are composed of one or several collectors of sinking material whose design depends on its use and position in the water column (e.g. moored traps, surface tethered traps, drifting traps) [@Honjo2008-kn]. They were extensively used [@Honjo2008-kn] though their use has been steadily decreasing since 2004, reaching less than 500 measurements per year since 2010 (@fig-sediment-trap-thorium-yearly-distribution).

```{r, poc measurements per year, dpi = 300}
#| label: fig-sediment-trap-thorium-yearly-distribution
#| fig-cap: Yearly distibution of POC flux measurements by sediment traps or derived from Thorium 234 deficits. Compilation based on @tbl-poc-from-sediment-traps and @tbl-poc-from-thorium.

library(ggplot2)
library(viridis)
trap <- vroom::vroom('DATA/Trap_Thorium/TRAP_DATABASE.csv')
thorium <- vroom::vroom('DATA/Trap_Thorium/THORIUM_DATABASE.csv')
trap$year <- lubridate::year(trap$start_date)
thorium$year <- lubridate::year(thorium$start_date)
sub_trap <- trap |> dplyr::select(year, instrument_type)
sub_trap$instrument_type <- 'Sediment trap'
sub_thorium <- thorium |> dplyr::select(year, instrument_type)
sub_thorium$instrument_type <- 'Thorium 234'
combined <- rbind(sub_trap, sub_thorium)
plot <- ggplot(combined, aes(x = year, fill = instrument_type)) + geom_bar(colour = 'black') + scale_fill_manual(values = c('#003366', '#DCEEF3')) + # #57cc99 #80ed99
    #geom_vline(aes(xintercept = median(year)), color = "#d53e4f", linetype = "dashed") +
    labs(x = "Year", y = "N records") + 
    theme_bw() + 
    #theme(legend.position = c(0.12, 0.85), legend.background = element_rect(fill = "white", color = "white")) + 
    theme(legend.position = 'top') +
    theme(legend.title=element_blank())

# plot <- ggplot(combined, aes(x = year, fill = instrument_type)) + geom_bar() + scale_fill_manual(values = c('#E85D04', '#FFBA08')) + # #57cc99 #80ed99
#     #geom_vline(aes(xintercept = median(year)), color = "#d53e4f", linetype = "dashed") +
#     labs(x = "Year", y = "N records") + theme_bw() + guides(fill=guide_legend(title="Method"))

plot
```

To estimate the downward POC flux, one therefore needs to quantify the amount of POC collected through a given area over a known time period at a given depth. However, the apparent sinking flux measured with sediment traps can be biased due to hydrodynamic issues that could result in over- or under-collection of particles (especially at the surface), swimmers (zooplankton, small fish) that could enter sediment traps and affect the collected material and solubilization of collected material (e.g. by bacteria) [@Buesseler2007-ql].

Given some potential bias in the carbon fluxes estimated with sediment traps, independent approaches using radionuclide disequilibria have been used to assess the collection efficiency of the latter [@Buesseler1991-vo]. This method is based on the fact that $^{238}U$ is a conservative element in seawater (chemically inert with a half-life of 4.5 billion years) while its daughter nuclide $^{234}Th$ (half-life of 24.1 days) can be adsorbed onto particle surfaces and therefore be scavenged by sinking particles [@Savoye2006-gb]. Consequently, the deficiency of $^{234}Th$ relative to $^{238}U$ is used as a measure of removal of surface $^{234}Th$ through particle scavenging [@Buesseler1991-vo] that is then translated into a flux of exported $^{234}Th$ and subsequently a flux of exported POC by using a known (e.g. estimated from the material collected in sediment traps) POC/$^{234}Th$ ratio [@Buesseler2007-ql].

While this method seems to cope with some of the issues observed in sediment traps, it also has its caveats, particularly a POC/$^{234}Th$ ratio that varies with particle size and the fact that it only works well in the upper 100-200 m [@Buesseler2007-ql].

In all, POC fluxes derived from sediment traps and $^{234}Th$ have their own uncertainties and errors thus their estimations of downward POC fluxes should not be seen as ground truth. In addition, they are labor-intensive and require ship support. Their relative contribution to the estimation of POC fluxes is thus expected to keep decreasing (@fig-sediment-trap-thorium-yearly-distribution) with the advent of alternative new methods promoted by an increasing use of automated underwater vehicles (AUVs).

## The potential of autonomous platforms {#sec-potential-autonomous-platforms}

It is admitted that ship-based measurements are potentially the most precise. However, they lack spatiotemporal coverage and they are usually not cost-effective. Consequently, oceanographers have turned to new platforms and have developed new sensors to study the marine carbon cycle and the BCP in particular.

Remote sensing data acquired with ocean color satellites have been used for a long time [@Gordon1980-dm] to study key EZ variables related to carbon export [@Claustre2021-tx] such as phytoplankton biomass [@Robinson2004-cj], phytoplankton communities [@Uitz2015-hd] and particle size distribution [@Kostadinov2009-gf; @Kostadinov2010-bu] to name a few (see also @sec-biogeochem-models). While having a wide spatial coverage, these platforms are limited to the ocean's surface and their measures can be affected by several factors such as the cloud cover or low solar elevation (e.g. @Marshall1994-vk).

Instrumented moorings have supplemented ship-based and remote sensing measurements by collecting high-resolution data at fixed locations [@Chai2020-uq]. Moorings can be equipped with many sensors (e.g. sediment traps or imaging systems to study the type of sinking particles (@sec-BGC-sensors)) to study carbon-related variables (see Table 1 in @Claustre2021-tx). They are therefore ideal platforms for long-term time series (e.g. Bermuda Atlantic Time-series Study (BATS) since 1988) with the disadvantage of limited spatial coverage and high maintenance costs [@Chai2020-uq].

In order to fill the scarcity of data from research vessels and moorings as well as coping with surface-constrained satellite measurements, AUVs have been developed to take in situ measurements in all weathers and at all times, only being dependent on their battery life and ocean currents (for drifting AUVs).

Biogeochemical Argo (BGC-Argo) profiling floats (@fig-bgc-float) are probably the most well known to address the BCP. They are cost-effective, able to monitor biogeochemical processes at a global scale throughout the year and under all weather conditions [@Chai2020-uq]. The goal of the BGC-Argo community is to develop an array of 1000 floats that measure 6 key biogeochemical and bio-optical variables: Chlorophyll a (Chla), particle backscatter ($b_{bp}$), oxygen, nitrate, pH and downwelling irradiance in addition to temperature and salinity [@Roemmich2019-pn]. Besides this objective, the BGC-Argo community is testing new sensors (see @sec-BGC-sensors) to explore specific topics related to the BCP [@Claustre2020-jo].

Gliders are also effective AUVs to study the BCP [e.g. @Omand2015-mf]. In comparison with BGC-Argo floats (@fig-BGC-Argo-cycle), gliders mostly operate in shallow waters [@Chai2020-uq] and are able to take measurements along a predefined transect due to their ability to change their internal weight distribution [@Johnson2009-kb].

![Illustration of a typical BGC-Argo 10-day cycle. From the beginning of the cycle (1), the profiling float descends to its parking depth (2) then drifts for \~ 9 days (3) to dive at 2000 m (4) before the ascent where data are acquired (5) and transmitted to satellite while at the surface (6). Note that the profiling float can be programmed (and reprogrammed during its mission via Iridium transmission) with different configurations, depending on its utilization. For the study of the BCP, some floats could be configured to drift at multiple parking depths to study the BGP or to shorter cycle duration to study the VMP. Image taken from @Claustre2020-jo.](image/ObservingCarbonCycle/argo_float_profiling.jpeg){#fig-BGC-Argo-cycle}

In the future, new types of platforms (e.g. unmanned surface vehicles such as saildrones or wave gliders), new sensors (see @sec-BGC-sensors) and new technologies (e.g. lidar) will be continuously deployed, improved and developed in the hope of better understanding the BCP and its components [@Chai2020-uq; @Claustre2021-tx].

## Specific sensors on autonomous platforms for the study of the BCP {#sec-BGC-sensors}

The BGC-Argo array (@sec-potential-autonomous-platforms) has already demonstrated its ability to understand some aspects of the BCP [e.g. @Lacour2017-ro; @Lacour2019-id; @Briggs2020-gz; @Yang2021-vg; @Llort2018-pa; @Wang2022-hn]. However, the current array may lack some key data to further our understanding of the BCP. Therefore, other types of sensors could be added to help fill this gap.

In this work, I will focus on two sensors: a transmissometer used as an optical sediment trap (OST) and the Underwater Vision Profiler 6 (UVP6) [@Picheral2021-sx], a miniaturized version of the UVP5 (@fig-bgc-float).

![Illustration of an augmented BGC-Argo float. From top to bottom: GPS & Iridium (two-way communication) antenna, Oxygen optode, CTD (salinity, temperature and depth), pH sensor, radiometer (downwelling photosynthetic available radiation and downwelling irradiance at 380, 412 and 490 nm), fluorometer (Chla and colored dissolved organic matter) and scattering meter ($b_{bp}$), UVP6 (particle size spectra and images), transmissometer (particle beam attenuation) and nitrate sensor. \@Thomas Boniface.](image/ObservingCarbonCycle/float_picture1.png){#fig-bgc-float width="400"}

A transmissometer measures the transmittance ($T_{r}$) of a light beam at a given wavelength. Given $T_{r}$, the beam attenuation coefficient ($c$) is computed as

$$
c = \frac{ln(T_{r})}{r}
$$ {#eq-bean-attenuation}

where $r$ is the transmissometer pathlength.

The value of $c$ is the result of absorption and scattering by particles, water and dissolved substances [@Zaneveld1994-ez]. The transmissometer uses a light beam at 660 nm whose attenuation coefficient can be splitted into three components: the absorption and scattering of light by particles ($c_{p}$), the absorption by water ($c_{w}$) and the absorption by dissolved organic matter ($c_{DOM}$) [@Jerlov1976-hr]. At 660 nm, $c_{w}$ is assumed constant at 0.364 m$^{-1}$ and $c_{DOM}$ is considered negligible [@Jerlov1976-hr] hence variations of $c$ essentially represent variations of $c_{p}$.

The use of transmissometers to characterize POC concentrations was shown by @Bishop1999-es who found that POC concentrations were well correlated with $c_{p}$, independently of ocean environment, season or depth. Subsequently, the idea of using a transmissometer as an OST was firstly applied by @Bishop2004-at where the magnitude of carbon export was quantified based on the particles accumulation on the upward-facing window of vertically oriented transmissometers on autonomous floats while they were drifting (i.e. parked at a specific depth). This method was later improved by [@Estapa2013-gw] who divided the attenuation signal into a continuous component, that is the regular accumulation of small particles on the transmissometer window and a discontinuous component resulting in positive jumps likely attributed to the deposition of (rarer) large particles (see @fig-estapa-method and @sec-case-study-labrador-sea). @Estapa2017-jr further calibrated the OST carbon flux proxy using codeployed sediment traps and drifting transmissometers thus allowing the conversion of $c_{p}$ attenuation fluxes (units: m$^{-1}$d$^{-1}$) into carbon fluxes (units: mg C m$^{-2}$d$^{-1}$) using empirical POC-to-attenuance (ATN) ratio were ATN (unitless) is the product of $c_{p}$ and the transmissometer pathlength.

```{r, OST on BGC-Argo example with jump}
#| label: fig-estapa-method
#| fig-cap: Application of the method of @Estapa2017-jr on a drifting transmissometer to divide the $c_{p}$ signal into a continuous (dark blue) component, cleaned from spikes and negative jumps (assumed as not related to carbon flux) and a discontinuous (positive jumps, light blue) component. Data from BGC-ARGO float WMO 4903634, drifting at 1000 m during its 9$^{th}$ cycle.
library(tidyverse)
library(ggplot2)
library(vroom)
#library(castr)
library(plyr)
library(scales)
library(lubridate)
library(latex2exp)

# From Louis Terrats
clean_cp_data <- function(file, moving_median_order, threshold) {
  
  # Store raw data 
  raw_file <- file
  
  #print(head(file))
  
  # Remove cp > 2 m-1 
  if (any(file$cp > 2)) {
    file <- file[-which(file$cp > 2),]
  }
  
  # Store relevant variables in vectors (for speeding up the process)
  cp <- file$cp
  diff_cp <- diff(cp)
  dates <- file$dates
  
  # Spike removal
  # Spike is identified as a significant increase (threshold = 0.01 m-1 h-1) followed by a return to normal values within the next 3 hours.
  tmp <- {diff(cp) / as.numeric(diff(dates), units = "hours")} %>% abs()
  spikes <- which(tmp > threshold)+1 # why +1?
  if (length(spikes) > 0) {
    # For each spike
    for (j in 1 : length(spikes)) {
      
      # Get the value measured 3 hours after the spike (= post_spike)
      spike <- spikes[j]
      time_diff <- difftime(dates, dates[spike], unit = "hours") 
      post_spike <- which(time_diff >= 3)[1] 
      
      # If the difference between the spike and the post_spike cp is superior to the threshold : values get back to normal ones
      if (is.na(cp[spike])) {next 
      } else {
        if ({!is.na(post_spike)} && {(cp[spike]-cp[post_spike]) > threshold}) { 
          back_to_normal <- which({(cp - cp[post_spike]) < threshold} & {dates > dates[spike]})[1] # Locate the date of back to normal values 
          cp[(spike):(back_to_normal-1)] <- NA # And remove the spike zone
        } else { 
          if (is.na(post_spike)) { # If post spike is Na, that means spike is near the end of the segment
            cp[spike:length(cp)] <- NA # Remove the end zone (which contains a spike)
          }
        }
      }
    }
  }
  
  # Store the sum of spikes 
  spikes <- spikes[which(spikes %in% which(is.na(cp)))]
  spikes <- diff_cp[spikes-1] %>% abs() %>% sum(na.rm = TRUE)  
  
  # Remove rows with NA (= remove spike zones)
  if (any(is.na(cp))) {
    file <- file[-which(is.na(cp)),]
  }
  
  # Moving median
  if (nrow(file) > 0) {
    file$cp <- pastecs::decmedian(file$cp, order = moving_median_order, ends = "fill") %>% pastecs::extract(component = "filtered")
  } else {return(list("data" = file, "plot" = NA, "spikes" = spikes))}
  
  # Plot showing results and previous data
  plot <- ggplot() + 
    geom_point(data = raw_file, aes(x=dates, y=cp, color = "black"), alpha = 0.5, size = 4) + 
    geom_point(data = file, aes(x=dates, y=cp, color = "blue"), size = 4) +
    scale_x_datetime(labels = date_format("%d-%m@%Hh")) + 
    labs(y = expression(c[p]~"["*m^-1*"]"), x = "Datetime") + 
    scale_colour_manual(name = '', values =c('black'='black','blue'='blue'), labels = c('Raw data','Filtered signal')) +
    # Theme
    theme_bw() +
    theme(text = element_text(size=16), 
          legend.text=element_text(size=16), 
          legend.position = 'right',
          axis.text = element_text(color="black", size = 16),
          plot.title = element_text(face="bold", hjust = 0.5))
  
  return(list("data" = file, "plot" = plot, "spikes" = spikes)) 
}

# From me
float_4903634 <- vroom::vroom('/home/flo/dox/THESIS/DATA/ARGO/4903634_OMTAB_VERSION.csv')
profile <- 'lovuse017c_016_01'
drift_depth <- '1000 m'

check <- float_4903634 |> select(dates = juld, everything()) |> filter(cycle == profile, park_depth == drift_depth) |> drop_na(cp)
despiked_data <- clean_cp_data(check, 3, 0.01)

# save cleaned Cp signal from spikes to a temporary variable
tmp2 <- despiked_data$data
# init jump values to NA
tmp2$jump <- NA
# compute slope between two adjacent points (except first and last point of drifting time series)
for (i in 2:(nrow(tmp2)-1)){
  #print(i)
  delta_x <- as.numeric(difftime(tmp2$dates[i], tmp2$dates[i-1], units = 'days'))
  delta_y <- tmp2$cp[i]-tmp2$cp[i-1]
  tmp2$jump[i] <- delta_y/delta_x
}

# assign a slope threshold depending on the drifting depth (this was estimated based on all 4 floats, could be refined later but works pretty well)
if(drift_depth == '200 m'){
  threshold <- 0.4
}else if(drift_depth == '500 m'){
  threshold <- 0.22
}else{
  threshold <- 0.177
}

# assign a colour for the plot
tmp2$colour <- 'cleaned signal'
tmp2$colour[which(abs(tmp2$jump)>threshold)] <- 'jump' # abs is needed for 'negative jumps'

# add group to compute slope (for each subgroups of points, separated from a jump)
tmp2$group <- NA

# compute where the jumps are (index value of the array)
jump_index <- which(tmp2$colour == 'jump')

# assign group 
for (i in jump_index){
  for (j in 1:nrow(tmp2)){
    if ((j < i) & (is.na(tmp2$group[j]))){
      tmp2$group[j] <- paste0('group_',i)
    }
  }
}

# add last group (because it is AFTER the last index so it was not computed before)
tmp2$group[which(is.na(tmp2$group))] <- 'last_group'

# compute slope for each subgroups
slope_df <- tmp2 |> filter(colour == 'cleaned signal', jump != 'NA') |> dplyr::group_by(group) |> dplyr::summarise(min_time = min(dates), max_time = max(dates), 
                                                                                                                nb_points = n(), first_cp = cp[1], last_cp = cp[nb_points],
                                                                                                                delta_x = as.numeric(difftime(max_time, min_time, units = 'days')),
                                                                                                                delta_y = last_cp-first_cp, slope = delta_y/delta_x)

# remove if only one point (cannot fit a slope with one point)
slope_df <- slope_df |> filter(nb_points > 1)

# compute mean slope
mean_slope <- mean(slope_df$slope)

# estapa relationship (see POSTER)
poc_flux <- 633*(mean_slope**0.77)

# build dataframe to plot each subgroup
part1 <- slope_df |> select(group, time = min_time, cp = first_cp)
part2 <- slope_df |> select(group, time = max_time, cp = last_cp)
part_slope <- rbind(part1, part2)

# spot negative jump
tmp2$colour[which((tmp2$colour == 'jump') & (tmp2$jump < 0))]  <- 'negative jump'

# change names
tmp2 <- tmp2 |> mutate(colour = if_else(colour == 'cleaned signal', 'Small particle deposition', 'Big particle deposition (jump)'))

jump <- filter(tmp2, colour == 'Big particle deposition (jump)')

jump_plot <- ggplot(tmp2, aes(x = dates, y = cp, fill = colour)) + geom_point(size= 3) +
  geom_point(data = jump, aes(x = dates, y = cp), size = 3, shape = 21) +
  #scale_fill_manual(values = c('#003366','#DCEEF3')) +
  scale_fill_manual(values = c('#DCEEF3', '#003366')) +
  theme_bw() +
  ylab(TeX('$c_{p}~(m^{-1})$')) +
  xlab('Datetime') +
  theme(legend.position = 'top', legend.title = element_blank()) 

jump_plot

```

The UVP6 (@fig-UVP6) is a miniaturized version of the UVP5 [@Picheral2021-sx], an imaging system developed to characterize particles (e.g. size, gray level, taxonomic group). Two versions of the UVP6 are available: the UVP6-HF (high frequency) and the UVP6-LP (low power) which has been specifically designed for AUVs that have strict energy requirements (e.g. autonomous floats).

The UVP6-LP (hereafter referred to as UVP6) is thus both a particle counter (0.1 mm - 2 mm) and a particle classifier (\> 0.6 mm). The former counts particles in different class sizes in order to compute the PSD while the latter extracts pixel-related and geometric features from each image to classify them (i.e. assign them a taxonomic group) directly on board using artificial intelligence (@sec-uvp-ia) with the aim of estimating zooplankton biomass [@Claustre2020-jo; @Claustre2021-tx].

![Illustration of the basic functioning of the UVP6. A light beam illuminates a region in front of the camera (imaged volume: 0.7L) which takes pictures at a given temporal resolution and subsequently characterizes and classifies relevant objects. \@Thomas Boniface.](image/ObservingCarbonCycle/UVP6_b.png){#fig-UVP6}

The deployment of autonomous platforms equipped with a UVP6 and the zooplankton recognition algorithm for the study of the BCP is just beginning (first deployment in May 2022 in the Labrador Sea, see @sec-case-study-labrador-sea). On the contrary, the use of PSD in carbon flux studies has already been thoroughly explored [@Guidi2008-vp; @Iversen2010-an; @Jouandet2011-bt; @McDonnell2012-ic; @Roullier2014-wf; @Guidi2015-sj; @Guidi2016-nj; @Ramondenc2016-nu; @Kiko2017-qy; @Fender2019-ou; @Giering2020-mc; @Cael2021-qn; @Bisson2022-bt].

Using the PSD, one can compute the number spectrum $n_{i}$ [@Stemmann2004-jv] which can be defined as the normalized concentration of particles in a given size bin $i$. Knowing the mass ($m$) and sinking speed ($w$, see @eq-sinking-speed) of aggregates, one can derive the total mass flux ($F$)

$$
F = \int_{0}^{\infty} n(d) m(d) w(d) dd
$$ {#eq-carbon-flux}

where $d$ is the aggregate diameter.

Assuming spherical aggregates, $m$ can be written as follow

$$
m(d) = \frac{\pi\rho d^{3}}{6}
$$ {#eq-mass-aggregate}

where $\rho$ is the average density of aggregates.

As explained in @sec-sinking-speed, aggregates have fractal geometries ($D$) hence @eq-sinking-speed and @eq-mass-aggregate can be rewritten as follow

$$
m(d) = \alpha d^{D}
$$ {#eq-mass-aggregate-fractal}

$$
w(d) = \beta d^{D-1}
$$ {#eq-sinking-speed-fractal}

where $\alpha$ and $\beta$ are constant.

Aggregate mass and sinking speed are often described using a power law relationship [@Alldredge1988-lx; @Guidi2008-vp] therefore it can be derived that

$$
mw = Ad^{B}
$$

and therefore from @eq-mass-aggregate-fractal and @eq-sinking-speed-fractal that $A= \alpha\beta$ and $B = 2D-1$.

Due to a finite number ($N$) of size bins, @eq-carbon-flux is rewritten as

$$
F = \sum_{i=1}^{N}n_{i}Ad_{i}^{B}\Delta d_{i}
$$ {#eq-carbon-flux-finite}

where $d_{i}$ is the mid-point of bin $i$.

@Guidi2008-vp used POC fluxes derived from sediment traps and UVP PSD data to estimate the $A$ and $b$ values of @eq-carbon-flux-finite by minimizing the following cost function

$$
Cost = \sum_{j = 1}^{n} (log(F_{UVP,j})-log(F_{ST},j))^{2}
$$ {#eq-cost-function}

where $F_{UVP,j}$, $F_{ST,j}$ and $n$ are, respectively, the flux estimate for the $j^{th}$ sediment trap measurement, the flux measured in sediment trap $j$ and the number of paired measurements (i.e. match-ups) for trap-UVP comparisons.

@Guidi2008-vp found an optimal couple of parameters $(A, B) = (12.5, 3.81)$ while @Iversen2010-an followed the same procedure and found $(A, B) = (273, 4.27)$ whereas @Fender2019-ou found $(A, B) = (15.4, 1.05)$. @Iversen2010-an assumed that the difference in the optimal couple of parameters was due to the study site used for the minimization (productive upwelling system versus open ocean in @Guidi2008-vp) while @Fender2019-ou suggested that a value of $B < 3$ means that either the density or sinking speed of particles is inversely correlated with particles diameter thus they suspected that the flux was dominated by particles in the lower size detection limit of the UVP they were using. Other values for $A$ and $B$ are being used in the literature [e.g. @Kiko2017-qy] based on a mechanistic approach developed by @Kriest2002-oc to investigate the formation of marine aggregates.

In all, it seems that the choice for the values of $A$ and $B$ is critical for deriving carbon fluxes derived from PSD [@Iversen2010-an; @Fender2019-ou]. Three main limitations highlighted by @Bisson2022-bt for UVP optically derived estimates of flux would be: (1) assuming that $A$ and $B$ are globally valid at all depths, for all UVP models and across all size classes, (2) the size to sinking rate uncertainty and (3) the size to carbon mass uncertainty..

The assumption that all particles of a given size class have the same carbon content and settling rate questions the formulation of the model itself (@eq-carbon-flux-finite). It has been shown that the pool of sinking particles is varied (e.g. compact fecal pellets, porous aggregates) and that the main driver of the sinking speed is currently subject to debate [@sec-sinking-speed]. Therefore, the size-dependent model of [@Guidi2008-vp] could be improved by taking into account the composition and nature of particles and by increasing the number of paired measurements between UVP PSD-derived flux estimations and other direct POC flux estimations (e.g. sediment traps, $^{234}Th$, OST).

## Research questions and objectives

In the context of the pressing issue of climate change, better understanding the BCP is of paramount importance [@Martin2020-wa]. Although the BCP is not responsible for the absorption of anthropogenic $CO_{2}$, any change in its strength or efficiency could have a significant impact on atmospheric $CO_{2}$ concentrations [@Kwon2009-ir]. Therefore, there is a need to better understand the biological and physical drivers of each component of the BCP to reduce uncertainties in the amount of carbon exported and its sequestration potential.

To this end, a new observational framework has been proposed by @Claustre2021-tx to combine measurements from several platforms (i.e. research vessels, satellites, moorings and AUVs) to investigate the BCP. Within this framework, AUVs are being deployed with newly developed sensors to characterize the type of sinking particles (e.g. zooplankton, fecal pellets, aggregates), their size and the transformation mechanisms affecting them (e.g. fragmentation, aggregation).

This framework will hopefully address the questions raised so far, i.e., better understand the underlying drivers of each component of the BCP, decouple the contribution of each pump to the observed downward carbon flux lengthily attributed to the BGP and estimates of carbon sequestration \[@Boyd2019-ez; @Passow2012-zh\] as well as close the ocean carbon budget \[@Burd2010-hc\].

This work will therefore address three mains topics related to the study of the BCP:

1.  BGC-Argo floats equipped with a UVP6 and the zooplankton recognition algorithm are able to classify particles larger than 0.6 mm. However, these AUVs are limited in terms of their battery life. They are typically deployed for several years, which requires special attention to the energy consumption of each sensor. In @sec-uvp-ia I will therefore describe the methodology used to develop the zooplankton classification model for the UVP6 accounting for energy limitations.
2.  4 BGC-Argo floats equipped with both a transmissometer and a UVP6 were deployed for the first time in the Labrador Sea in late May 2022 to investigate the post-spring bloom and its associated POC fluxes. In @sec-case-study-labrador-sea, I will show how transmissometers used as OSTs compare with PSDs measured by UVP6s as well as investigate the link between small and big particle fluxes derived by both instruments.
3.  Many studies have assumed that carbon sequestration only occurs in the deep ocean (typically 1000 m). In @sec-to-do, I reassess the global sequestration ($\geq$ 100 years) of the BCP and its components by using a continuous approach to carbon storage as opposed to the common view of long-term sequestration below a fixed reference depth.
