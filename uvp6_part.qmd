# UVP6 embedded classification algorithm {#sec-uvp6}

Introduire le chapitre et dire que le papier est en cours somehow ...

## Embedded recognition of zooplankton using the Underwater Vision Profiler 6 for use on autonomous platforms

Florian Ricour, Marc Picheral, Camille Catalano, Edouard Leymarie, Hervé Claustre, Léo Lacour, Lionel Guidi, Jean-Olivier Irisson

### Abstract

Millions of images of oceanic particles have been acquired with ship-based imaging instruments. The task of labeling images is time consuming but necessary to train classification models using artificial intelligence algorithms. Autonomous Underwater Vehicles (AUVs) such as Biogeochemical-Argo floats (BGC-Argo) have revolutionized our understanding of oceanic systems but they are flawed by their battery life and by the time they can spend at the ocean surface to transmit their data. In this paper, we show how we developed the first zooplankton classification model for AUVs using the Underwater Vision Profiler 6 (UVP6). To this end, we used a features-based classifier called eXtreme Gradient Boosting (XGBoost) adapted to specific energy constraints imposed by profiling floats and we compared its performance with a state-of-the-art convolutional neural network as if energy was not a limiting factor. In addition, we show how we dealt with a highly imbalanced data set towards marine snow (\> 80% of detritus) and what were the compromises to improve the classification scores of biological classes. Finally, we show the classification report (precision and recall for each label) of the final classification model and use it to analyse some preliminary results from BGC-Argo floats deployed in the Labrador Sea.

### Introduction

Inside the ocean, a cohort of diverse organisms known as zooplankton thrive. Those tiny beasts are so important for our oceans [@Steinberg2017-ri] that we decided to take pictures of them [@Picheral2010-to; @Gorsky2010-ca; @Cowen2008-wq; @Benfield2007-lt; @Lombard2019-ld]. Nonetheless, the amount of images taken by those imaging instruments has become so huge that it is now a massive work to classify them by hand. Fortunately, powerful machine learning techniques have been implemented to tackle the issue of automatic plankton image classification [@Luo2018-oo].

However, it remains a fierce challenge. Within a single taxon, one can find organisms with very different sizes and shapes, organisms with significant morphological differences acquired during their ontogenetic development, partial organisms (e.g. siphonophores) or organisms with various orientations, hindering the automatic classification process [@Benfield2007-lt]. In addition to this global heterogeneity, the great majority of images are not composed of zooplankton. Instead, non-living particles (mostly marine snow) steal the show, producing a highly imbalanced data set towards the latter.

The discussion around automatic plankton classification is a key topic in oceanography and the development of convolutional neural networks (CNNs) within that frame has already lead to promising results [@Luo2018-oo; @Cui2018-nt; @Dai2017-ex; @Campbell2020-cq; @Schroder2019-um; @Bochinski2019-af; @Lee2016-jl; @Cheng2019-ar; @Guo2021-xi; @Ellen2019-nh; @Orenstein2017-ya]. Nevertheless, real-time image processing done at sea is far less mentioned to our knowledge. Usually, images are brought back to the lab and classified with CNNs which, in comparison with features-based classifiers, that is machine learning algorithms using a set of features extracted from the image (e.g. area, eccentricity, grey levels) have outperformed the latter for plankton recognition [@Ellen2019-nh]. Though, it comes at a cost. CNNs are energy consuming hence their implementation for embedded applications can be limited by power consumption [@Biswas2018-uw; @Khabbazan2019-oi; @Oh2017-kj].

This is of particular importance for autonomous underwater vehicles (AUVs) and especially for a new version of Biogeochemical-Argo (BGC-Argo) floats that will be equipped with the Underwater Vision Profiler 6 (UVP6), the portable version of the UVP5 [@Picheral2010-to] to better understand the carbon cycle [@Xing2018-zo]. The lifetime of floats is limited by their batteries hence by the power consumed by its sensors. Therefore, using an embedded CNN to classify plankton images or sending images through iridium transmission to be classified in the lab would not be possible for economical (cost of iridium transmission) and technical reasons. Typically, a BGC-Argo float samples the water column between the surface and 1000 m, it would therefore have to stay at the surface for a time long enough to transmit all images collected after each profile, consuming energy proportionally to the size of the image set while being subject to biofouling or unpredictable hazards. For all these reasons, it was decided to use a features-based classifier for the development of the embedded plankton recognition algorithm on board the UVP6.

In the literature, such algorithms (e.g. Decision Tree (DT), Random Forest (RF), Support Vector Machine (SVM)) have already been used for plankton recognition (see Table 1 of @Gonzalez2017-tr for an exhaustive list). For instance, @Blaschko2005-ep obtained an accuracy of 71% using a SVM on a set of 982 images among 13 classes using a 10-fold cross-validation (CV), @Sosik2007-cj obtained a global accuracy of 88% using a SVM on a test set of 3300 images covering 22 classes, @Bell2008-yo obtained 82% of accuracy using a 10-fold CV RF algorithm on 53 classes with 10 to 30 images per class. However, it should be noted that this accuracy dropped to 63% after the removal of non-biological classes such as detritus and fiber. For bigger training sets, which will be the case in this study, @Orenstein2017-ya trained a RF model on approximately 650.000 images across 70 classes for an accuracy of 91% on the test set (2.75 million images). Though, one class in that test set contained 1.6 million images and if the classifier had put every object in that class, the accuracy would have been around 60% so caution must be applied when interpreting performance metrics. @Faillettaz2016-wr trained a RF model on 6000 images across 14 classes, reaching an accuracy of 56% on a test set of 1.5 million objects, discarding those below a certain probability threshold. This allowed to improve the global accuracy by 16% but resulted in the loss of 72% of objects in the predicted data set.

It is quite difficult to compare all methods due to the diversity of imaging instruments hence the variety of image formats (e.g. resolution), the number of classes (trade off between the model performance and the number of classes as illustrated in @Fernandes2009-hw, the training set (e.g. number of images per class, diversity, balance), the scientific objective (e.g. precision-recall trade off), the type and number of features and the algorithm used. In many cases, RF seemed to be the best classifier [@Gorsky2010-ca; @Faillettaz2016-wr] but in embedded applications, energy constraints need to be taken into account. Therefore, the model needs to be small and the classification should be easy and fast. Gradient Tree Boosting (GTB) seems to offer this compromise when compared to RF (model size) and SVM (computation complexity). For these reasons, we chose XGBoost [@Chen2016-xt], an optimized implementation of GTB, also known to be one of the best assets to win Kaggle's machine learning competitions.

In this paper, we present the results of an embedded zooplankton recognition algorithm using XGBoost on UVP5 data. We show how we built the embedded recognition (ER) model under strict technical constraints, what were our compromises and what techniques we used to improve the classification. We also compare the performances of our features-based algorithm with a state-of-the-art neural network as if energy was not a limiting factor. Finally, we apply the method and follow the recommendations learnt from the preliminary UVP5 model to the UVP6 data set to build the ER model that will be included in the UVP6 for AUVs.

### Material and method

#### Instruments

The UVP6 is the miniaturized version of the UVP5 [@Picheral2021-sx]. It is smaller, it uses less energy and it has been specifically developed to be mounted on AUVs such as BGC-Argo floats with the aim of providing data of similar quality than from the UVP5 (@fig-uvp-pipeline). There are 2 versions: the LP (low power) and the HF (high frequency). The former uses less energy (lower image acquisition rate) and will be deployed on AUVs while the latter is designed to be used on CTD rosettes and cruising AUVs [@Picheral2021-sx].

![Scheme of the UVP6 pipeline with the processing done in the lab to create the ER model and the processing time (in ms) of the main steps when the UVP6 is deployed on board an AUV. kpx means kilo pixel.](image/uvp6/fig_pipeline_UVP6.jpg){#fig-uvp-pipeline}

UVP5 Standard Definition (SD) profiles have been collected since 2008 at a global scale (@fig-map-of-profiles) whereas the UVP6 has been deployed very recently in specific locations and thus lacks spatial coverage to be fully representative. For this reason, UVP5 data will be used to create a preliminary ER model and explore ways of improvement. Ultimately, we will train the ER model for the UVP6 by applying observations made on the preliminary UVP5 model.

![UVP5 and UVP6 profile coverage used in this study. UVP5 profiles used in the test set are depicted by green crosses.](image/uvp6/map_of_profiles.png){#fig-map-of-profiles}

#### Data preparation

The full UVP5 SD data set is composed of 3.578.901 validated images whereas the full UVP6 LP data set has 634.459 validated images. Each image was segmented to find the main object (i.e. the biggest connected region in pixels). From it, 55 features were extracted to describe this object in terms of shape (e.g. pixel area, eccentricity), gray level and mostly statistical moments (see @tbl-features). For technical reasons, all UVP6 images with a pixel area below 79 pixels were removed (7% of images). 

```{r, features, dpi = 300}
#| label: tbl-features
#| tbl-cap: List of 55 features (abbreviations used in our python package UVPec (https://github.com/ecotaxa/uvpec) and their descriptions) extracted from each object based on the object geometrical characteristics, the pixel gray values histogram and statistical moments.

# library(gt)
# library(gtExtras)
#library(flextable)
library(kableExtra)
library(tibble)

# object_attributes <- tibble(attr = c('area', 'width', 'height', 'x','y','xm','ym','major','minor', 'angle', 'eccentricity', 'esd', 'bbox_area', 'extent'), comment = c('number of pixels', 'bounding box width', 'bounding box height', 'local centroid (x direction)', 'local centroid (y direction)', 'weighted local centroid (x direction)', 'weighted local centroid (y direction)', 'major axis', 'minor axis', 'orientation angle','-', 'equivalent spherical diameter', 'bounding box area', 'pixel ratio between object and bounding box'))
# 
# hist_attributes <- tibble(attr = c('mean', 'stddev', 'mode', 'min', 'max', 'intden', 'median','histcum1', 'histcum3', 'vrange', 'meanpos', 'cv', 'sr'), comment = c('-', 'standard deviation', '-', '-', '-', 'sum of all pixel values', '-', 'first quartile', 'third quartile', 'vmax - vmin', '(vmax - vmin)/range', '100*(stddev/mean)', '100*(stddev/vrange)'))
# 
# stat_moment <- tibble(attr = c('hu_moment-i (i: 1 to 7)', 'gray_hu_moment-i (i: 1 to 7)',
#                                'central_moment-2-0',
#                                'central_moment-1-1',
#                                'central_moment-0-2',
#                                'central_moment-3-0',
#                                'central_moment-2-1',
#                                'central_moment-1-2',
#                                'central_moment-0-3',
#                                'gray_central_moment-2-0',
#                                'gray_central_moment-1-1',
#                                'gray_central_moment-0-2',
#                                'gray_central_moment-3-0',
#                                'gray_central_moment-2-1',
#                                'gray_central_moment-1-2',
#                                'gray_central_moment-0-3'),
#                       comment = c('Hu moments (translation, scale and rotation invariant)', 'weighted Hu moments', 'central moments (translation invariant)', '-', '-', '-', '-', '-', '-', 'weighted central moments', '-', '-', '-', '-', '-', '-'))

object_attributes <- tibble(attr = c('area', 'width', 'height','major','minor', 'angle', 'eccentricity', 'esd', 'bbox_area', 'extent'), comment = c('number of pixels', 'bounding box width', 'bounding box height', 'major axis', 'minor axis', 'orientation angle','eccentricity', 'equivalent spherical diameter', 'bounding box area', 'pixel ratio between object and bounding box'))

hist_attributes <- tibble(attr = c('mean', 'stddev', 'mode', 'min', 'max', 'x','y','xm','ym','intden', 'median','histcum1', 'histcum3', 'vrange', 'meanpos', 'cv', 'sr'), comment = c('mean pixel intensity', 'standard deviation of pixel intensity', 'pixel intensity mode', 'min pixel intensity', 'max pixel intensity', 'local centroid (x direction)', 'local centroid (y direction)', 'weighted local centroid (x direction)', 'weighted local centroid (y direction)', 'sum of all pixel values', 'median pixel intensity', 'pixel intensity first quartile', 'pixel intensity third quartile', 'vmax - vmin', '(vmax - vmin)/range', '100*(stddev/mean)', '100*(stddev/vrange)'))

stat_moment <- tibble(attr = c('hu_moment-i (i: 1 to 7)', 'gray_hu_moment-i (i: 1 to 7)',
                               'central_moment-i-j (2-0;1-1;0-2;3-0;2-1;1-2;0-3)',
                               'gray_central_moment-i-j (2-0;1-1;0-2;3-0;2-1;1-2;0-3)'),
                      comment = c('Hu moment', 'weighted Hu moment', 'central moment', 'weighted central moment'))

all_features <- rbind(object_attributes, hist_attributes, stat_moment)
colnames(all_features) <- c('Features name', 'Description')

# gt(all_features) |> gt_highlight_rows(rows = 1:13,
#                                       fill = 'lightgrey')
#all_features |> flextable() |> color(i = 1:13, color = 'lightgrey')
# kable(head(iris, 5), align = 'c', booktabs = TRUE) %>%
#   row_spec(1, bold = TRUE, italic = TRUE) %>% 
#   row_spec(2:3, color = 'white', background = 'black') %>%
#   row_spec(4, underline = TRUE, monospace = TRUE) %>% 
#   row_spec(5, angle = 45) %>% 
#   column_spec(5, strikeout = TRUE)

# kable(all_features) |> row_spec(1:14, background = '#F2F2F2') |> 
#   row_spec(28:43, background = "#F2F2F2") |> row_spec(0, bold=T, align = 'c')

kable(all_features) |> row_spec(0, bold=T, align = 'c')

```

Validated objects were manually classified into specific taxa belonging to well known biological groups such as Rhizaria, Copepoda or Cnidaria to name a few. However, the maximum number of classes (also referred to as labels or taxa) for the ER was fixed to 40 in order to limit the size of the ER model and the volume of data that needs to be transmitted. We chose to aggregate classes iteratively to find the best number of classes for the ER. The first level (i.e. all labels (\> 40) are retained) served as a baseline for a prior model, without making any hypotheses. The next level was used as a first aggregation step based on the number of objects in each class and the purity of the class. Below a given threshold, rare classes were merged with their most look-alike taxa and parents on the phylogenetic tree were removed when their children were abundant (i.e. parents could not be classified to a deeper taxonomic level, likely causing confusion with their children during training). The second level of aggregation was based on statistical metrics post training (see hereafter how a model is trained), meaning that we looked at the confusion matrix and the classification report (@fig-confusion-matrix-uvp5 and @tbl-classification-report-uvp5-xgboost-cnn, respectively) to find which classes did not perform well such that they had to be merged to improve the classification score. This last step was repeated until an agreement was found. Thus, classes were merged only if they were confused amongst the same biological group on the phylogenetic tree or if they had a similar ecological role. This involves that some classes end up with a variety of organisms that do not necessarily show similar morphological traits (e.g. other\<living). It is definitely not ideal but we were bound to keep as many biological classes as possible to counterbalance non-living classes.

<!-- ![Confusion matrix (normalized on true labels) of the best UVP5 XGBoost model.](image/uvp6/confusion_matrix_best_model_uvp5.jpg){#fig-confusion-matrix-uvp5-xgboost} -->

```{r, confusion matrix CNN UVP5, fig.height = 6}
#| label: fig-confusion-matrix-uvp5
#| fig-cap: Confusion matrix (normalized on true labels) for the best UVP5 model trained with XGBoost and a CNN.

library(viridis)
library(gridExtra)

# define x and y axes
x <- c('Acantharia', 'Aulacanthidae', 'Aulosphaeridae', 'Copepoda', 'Trichodesmium', 
            'artefact', 'bubble', 'darksphere', 'detritus', 'fiber', 'other<living')

y <- factor(x, levels = rev(x))

# create grid
data <- expand.grid(X=x, Y=y, stringsAsFactors = T)

# gray levels
gray_cnn_uvp5 <- c(38,252,254,254,242,254,255,255,251,253,250,
           249,61,252,255,241,253,255,255,247,255,251,
           251,250,17,255,255,253,255,255,250,255,252,
           255,255,255,61,255,255,255,255,214,253,252,
           254,255,255,255,0,255,255,255,245,253,255,
           255,255,255,255,255,92,255,255,187,254,254,
           255,255,255,255,253,253,0,255,246,255,255,
           251,248,253,255,242,254,252,109,222,255,252,
           255,255,255,253,251,248,255,255,17,251,254,
           255,255,255,255,248,253,255,255,189,108,254,
           250,252,255,247,253,247,255,255,230,249,99)

# reverse grayscale so that 0 becomes 255 and 255 becomes 0 and normalize to 1
gray_cnn_uvp5 <- abs(gray_cnn_uvp5 - 255)/255

# add gray values in the grid
data$gray_level <- gray_cnn_uvp5

# add group for facet plot
data <- data |> mutate(group = 'CNN')

right <- ggplot(data, aes(X, Y, fill= gray_level)) + geom_tile() + scale_fill_viridis(option = "F", direction = -1) +
  labs(x = "Predicted labels", y = "True labels") + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

# do the same for the xgboost confusion matrix
gray_xgboost_uvp5 <- c(66,252,249,255,250,255,255,255,233,249,250,
                        253,62,248,255,248,244,255,255,244,255,249,
                        254,252,4,255,255,255,255,255,243,255,251,
                        255,255,255,126,255,253,255,255,147,253,252,
                        254,255,255,255,35,255,255,253,226,248,255,
                        255,255,255,255,255,35,255,255,213,255,255,
                        255,255,255,255,253,250,17,255,235,255,255,
                        254,253,253,255,251,254,255,65,215,255,252,
                        255,255,255,253,253,246,255,255,0,250,253,
                        255,255,255,254,250,252,255,255,154,120,255,
                        250,252,253,246,252,241,255,253,221,245,135)

gray_xgboost_uvp5 <- abs(gray_xgboost_uvp5 - 255)/255
data2 <- expand.grid(X=x, Y=y, stringsAsFactors = T)
data2 <- data2 |> mutate(gray_level = gray_xgboost_uvp5, group = 'XGBoost')

left <- ggplot(data2, aes(X, Y, fill= gray_level)) + geom_tile() + scale_fill_viridis(option = "F", direction = -1) +
  labs(x = "Predicted labels", y = "True labels") + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

# left / right

# plot facet
data <- rbind(data, data2)
data <- data |> mutate(group = factor(group, levels = c('XGBoost', 'CNN'))) 
ggplot(data, aes(X, Y, fill= gray_level)) + geom_tile() + scale_fill_viridis(option = "F", direction = -1) +
  labs(x = "Predicted labels", y = "True labels", fill = '') + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + facet_wrap(~group, nrow = 2)


```

```{r, classification report XGBoost UVP5}
#| label: tbl-classification-report-uvp5-xgboost-cnn
#| tbl-cap: Precision, recall and f1-score for each class and globally (lightgray background) in the UVP5 model (XGBoost and CNN). avg average. avg stands for average.

labels <- c('Acantharea', 'Aulacanthidae', 'Aulosphaeridae', 'Copepoda', 'Trichodesmium', 
            'artefact', 'bubble', 'darsphere', 'detritus', 'fiber', 'other<living', 'accuracy',
            'macro avg', 'weighted avg', 'living macro avg', 'living weighted avg')

precision <- c(0.68,0.63,0.80,0.54,0.68,0.6,0.77,0.57,0.90,0.50,0.36,0.81,0.64,0.82,0.61,0.60)
recall <- c(0.68,0.69,0.84,0.50,0.75,0.75,0.81,0.68,0.86,0.52,0.48,0.81,0.69,0.81,0.66,0.65)
f1_score <- c(0.68,0.66,0.82,0.52,0.71,0.67,0.79,0.62,0.88,0.51,0.41,0.81,0.66,0.82,0.63,0.62)

precision_cnn <- c(0.60,0.61,0.89,0.56,0.56,0.63,0.76,.59,0.91,0.56,0.52,0.82,0.65,0.83,0.62,0.59)
recall_cnn <- c(0.79,0.73,0.87,0.72,0.89,0.63,0.89,0.58,0.87,0.58,0.61,0.82,0.74,0.82,0.74,0.78)
f1_score_cnn <- c(0.68,0.66,0.88,0.63,0.69,0.63,0.82,0.58,0.89,0.57,0.56,0.82,0.69,0.82,0.67,0.67)

cf_uvp5_xgboost_cnn <- tibble(labels, precision, recall, f1_score, precision_cnn, recall_cnn, f1_score_cnn)
colnames(cf_uvp5_xgboost_cnn) <- c('Labels', 'Precision', 'Recall', 'F1-score', 'Precision', 'Recall', 'F1-score')

kable(cf_uvp5_xgboost_cnn) |> row_spec(0, bold=T, align = 'c') |> #row_spec(1:16, align = 'c') |>
  row_spec(row = 12:16, background = '#F2F2F2') |> add_header_above(c(" " = 1, "XGBoost" = 3, "CNN" = 3))
  #column_spec(2, background = spec_color(cf_uvp5_xgboost$precision))
  #row_spec(12:16, background = '#F2F2F2')
```

In machine learning tasks, it is common to split the data into a training set, a validation set and a test set. The validation set is usually used to assess the quality of a model by tuning hyperparameters while avoiding overfitting (stopping the training before the model is too acquainted with the data). Ideally, the test set is not to be used during any step except at the very end where it is used to assess the global quality of the final model. In our case, both data sets (UVP5 and UVP6) are very imbalanced towards few non-biological classes (mostly detritus). As a result, in order for the model to see as much biological data as possible during training, we did not use a validation set but instead we did a threefold cross-validation.

One objective of the ER on board the UVP6 is to be used on AUVs such as BGC-Argo floats to study the vertical migration pump [@Claustre2021-tx]. Hence, we built the test set with entire UVP5 profiles going at least to 1000 m depth, the typical parking depth of BGC-Argo floats. Those profiles were also chosen to be representative of the global ocean (@fig-map-of-profiles). For UVP6 data, we did not select entire profiles because we were limited by the number of images in comparison with the UVP5 to build the biggest training set possible (80% of detritus in the UVP6 data set). Therefore, we splitted the data set in a 10-90 % stratified test-train fashion regardless of specific profiles.

Following the previous steps, the UVP5 test and training sets are composed of, respectively, 315.277 and 3.023.506 images while the UVP6 has 59.054 and 531.483 images for its test and training sets.

### Gradient boosting and model training

GTB generates a sequence of trees, each one trying to cope with the errors made by their predecessors. XGBoost is designed to be an optimized and regularized version of GTB. To learn from the data and derive a model, XGBoost first minimizes a regularized objective function $L$

$$
L = \sum_{i}l(\hat{y}_{i},y_{i}) + \sum_{k}\Omega(f_{k})
$$ {#eq-objective-function-xgboost}

with

$$
\Omega(f) = \gamma T + \frac{1}{2}\lambda||w||^{2} 
$$

where $l$ is a loss function that measures the difference between the prediction $\hat{y}_{i}$ and the true value $y_{i}$ and $\Omega$ is the regularization term, for each $k$ decision tree $f_{k}$. $\Omega$ is then splitted into two terms where $\gamma T$ penalizes the complexity of a tree proportionally to the number of leaves ($T$) and where the L2 norm penalizes the weights ($w$) of the leaves to avoid overfitting (both $\gamma$ and $\lambda$ are hyperparameters that can be tuned by the user). By doing so, XGBoost encourages both a simple and predictive model. From @eq-objective-function-xgboost, a scoring function that measures the quality of a tree structure can be derived (see Eq. 6 in @Chen2016-xt for details). However, in practice, it is not possible to test all suitable trees because there are infinite possibilities. Instead, the algorithm starts from a single leaf and iteratively adds branches to the tree, searching for the best split options.

To build the ER model, the first step was a threefold cross-validation using XGBoost parameterized with a set of given hyperparameters (i.e. learning rate ($\eta$), maximum depth of a tree ($D_{max}$) and weight sensitivity ($\omega$)). The second step consisted in the search of the optimal number of boosting rounds (i.e. $k$) before the model overfits. For this, we searched for the minimum value of the multi class log loss(also known as logistic loss or cross-entropy loss) evaluated at each boosting round. Using that optimal $k$ value, we then trained the ER model with the entire training set. Therefore, finding the best model consisted in finding the couple ($\eta$, $D_{max}$) that respected the energy constraints and outputed the best global metrics. The latters were determined using a customary grid search (@fig-grid-search) while $\omega$ was chosen empirically (see hereafter).

```{r, figure for grid search, fig.width=10, fig.height=6}
#| label: fig-grid-search
#| fig-cap: Grid search results with several classes of detritus (1, 3 or 5 classes). D$_{max}$ is depicted by the colour of the border of each shape (red or black). 
  
library(tidyverse)
library(ggplot2)
library(stringr)
library(gghighlight)

# read all data
d <- bind_rows(
  read_csv('DATA/UVP6/results_grid_search_UVP5.csv') %>%
    filter(str_detect(folder_name, "_1000")) %>%
    mutate(learning_rate = learning_rate/10) %>%
    filter(learning_rate < 0.5, max_depth < 7, n_detritus < 6)
) %>%
  # remove metrics with less meaning
  #select(-ends_with("bio"), -mylogloss) %>%
  # convert to tall format
  gather(key="metric", val="val", accuracy:living_recall_w) %>%
  # reformat hyperparameters
  rename(n_det=n_detritus) %>%
  mutate(
    eta=factor(learning_rate),
    Dmax=factor(max_depth),
    n_det=factor(n_det),
  )

#https://stackoverflow.com/questions/64271350/changing-column-names-in-a-scatterplot-with-facet-wrap?noredirect=1&lq=1
panel_names <- c(`accuracy`="global accuracy",
                    `living_precision_w`="weighted living precision",
                    `living_recall_w`="weighted living recall",
                    `precision_macro`="macro precision",
                    `recall_macro`="macro recall")

d %>%
  ggplot() +
  geom_point(aes(x=loading_time, y=val,
                 shape=eta, fill=eta, colour=Dmax, size=n_det)) +
  facet_wrap(~metric, scales="free_y", labeller=as_labeller(panel_names), nrow = 3) +
  scale_shape_manual(values=c(21:25), guide=guide_legend(expression(eta))) +
  scale_fill_manual(values=c('#ffffcc','#a1dab4','#41b6c4','#225ea8'), guide=guide_legend(expression(eta))) + 
  #scale_fill_manual(values=c('#ffffb2', '#fecc5c', '#fd8d3c', '#e31a1c'), guide=guide_legend(expression(eta))) + 
  scale_size_manual(values=c(3:5), guide=guide_legend("detritus")) +
  scale_colour_manual(values=c("red", "black"), guide=guide_legend(expression(D[max]))) +
  xlab('Model loading time (ms)') +
  ylab('Metrics') + theme_bw()
```

To cope with our unbalanced data set (more than 70% of detritus in the UVP5 data set), several options were available. We could artificially increase the number of biological data with the well known SMOTE oversampling technique [@Chawla2002-kg; @Fernandez2018-ft], decrease the number of detritus (random undersampling, keep only detritus close to unsupervised cluster centroids, etc.) or add a weight to each label during training to nudge the model towards under-represented classes (i.e. biological classes). In order to compare a features-based classifier with a state-of-the-art CNN, we chose the latter and we defined the weight of a class as follows:

$$
class~weight = \left(\frac{size~max}{class~size}\right)^{\omega}
$$ {#eq-class-weight}

where $size~max$ and $class~size$ are, respectively, the size of the biggest class (i.e. detritus) and the size of each class. Hence, the weight of detritus is 1 and all other classes have a weight \> 1.

We decided to select the best $\omega$ among 0 (no weights), 0.25, 0.50 and 0.75 (model very biased towards biological classes). In order to decipher between all options, we compared the predicted concentrations of classes for each $\omega$ with the real concentration of each class in different regions of the global ocean. As always in machine learning, we encountered a trade off between precision and recall. However, given that the UVP6 is more focused on carbon flux estimation (high precision) than on the detection of rare objects (high recall with low precision), a weight sensitivity of 0.25 seemed to be the best compromise.

In order to compare our features-based model with a state-of-the-art neural network not limited by energy constraints, we used a MobileNetV2 (depth multiplier 1.40) pre-trained on ImageNet. This is known as transfer learning and it was already applied with success for zooplankton classification in @Orenstein2017-ya. Input images have a size of \[224, 224, 3\], the size of the features extractor vector is 1792 followed by a fully connected layer (600 nodes) with a dropout of 0.4 to avoid over-fitting. The classification layer at the end of the network also has a dropout of 0.2. The CNN was trained with a decaying learning rate, a categorical cross entropy loss function and weights were defined with the same weight sensitivity of 0.25. The CNN was trained with a sufficient number of epochs such that its training reached a validation plateau (i.e. the model was not learning much with additional epochs).

### Results

It is well known that the task of labeling images in supervised machine learning tasks is time consuming and most probably painful when you have millions of images to label. It is also very frustrating to see that one class (largely) dominates when you are interested in the small ones and that some of them are so scarce that you need to merge them with their siblings (at the very best) on the phylogenetic tree. All in all, we found - after multiple trials and errors - 11 classes fulfilling a first compromise between classification performances and ecological importance: 7 biological classes (*Acantharea*, *Aulacanthidae*, *Aulosphaeridae*, *Copepoda*, *Trichodesmium*, darksphere and other\<living) and 4 non-living classes (detritus, bubble, fiber and artefact).

Once the number of classes is known, hyperparameters can be tuned to find the optimal couple for the classification results (i.e. grid search results shown in @fig-grid-search). In our case, the optimal model hyperparameters were 0.2 for $\eta$ and 5 for D$_{max}$ (also with only 1 class of detritus, see Discussion) to have both a good weighted living (or biological) precision and recall and a loading time (i.e. loading of the ER model in memory) below 700 ms for energetic reasons. In that configuration, the UVP5 model had an accuracy of 81%, a macro (i.e. label imbalance not taken into account) precision and recall of, respectively, 64% and 69% and a weighted living precision and recall of, respectively, 60% and 65%. CV found an optimal number of 850 boosting rounds before over-fitting and the expected loading time is 382 ms.

The classification report (CR) gives the precision, the recall, the f1-score (harmonic mean of the precision and recall) and global metrics on the test set. As it can be seen in @tbl-classification-report-uvp5-xgboost-cnn, detritus have a recall of 86% thus because detritus are abundant (more than 230.000 in the test set), it results in more than 30.000 detritus wrongly labelled and therefore disseminated in all other classes. The confusion matrix (CM) helps to see where those wrongly labelled images are. @fig-confusion-matrix-uvp5 shows that a lot of Copepoda are confused with detritus as well as fiber. The worst class in the CR is other\<living, which is not surprising because this class is composed of organisms that are not all morphologically and ecologically related. The best biological class is Aulosphaeridae with a f1-score of 82%.

In comparison with a CNN trained on UVP5 images, the CR (@tbl-classification-report-uvp5-xgboost-cnn) shows that the global accuracy is 1% higher (82%) with a macro precision and recall of, respectively, 65% and 74% and a weighted living precision and recall of, respectively, 59% and 78%. *Copepoda* are better detected (higher recall) but are still often (56% precision) misclassified. The *Aulosphaeridae* class has improved its f1-score to 88%. The class other\<living has also greatly improved even though this class cannot be interpreted further in terms of ecology. Overall, the CNN performs slightly better on biological (living) classes than XGBoost if we consider the f1-score but this 4% increase is mostly due to an increase in global recall and not in precision. For instance, the 14% increase in recall for *Trichodesmium* is followed by a 12% decrease in precision so there is still a lot of confusion at this stage for the UVP5 for some classes, even with a CNN that is not limited by any energy constraints.

<!-- ![Confusion matrix (normalized on true labels) of the UVP5 CNN model.](image/uvp6/confusion_matrix_CNN_uvp5.jpeg){#fig-confusion-matrix-uvp5-cnn} -->

### Discussion

#### Ways to improve the classification

The on board classification of plankton is definitely not an easy task. In order to improve the classification scores of biological classes, we tried several adjustments such as quantifying planktonic samples, splitting the detritus class into several subgroups and assigning weights to classes proportional to their size, which was the only option that enhanced the recognition of scarce classes (see Methods).

Quantification is an interesting concept that focuses on the sample instead of the individual. It was already shown in @Solow2001-jc, using plankton images from the Video Plankton Recorder, that the estimation of taxonomic abundances could be improved by computing the confusion matrix $\pmb{P}$ where $p_{i,j}$ is the probability that a taxon $i$ is classified as taxon $j$. If the classifier is perfect, $p_{i,j} = 1$ where $i = j$, the quantification resumes to a simple Classify and Count method (CC, see @Forman2005-eu). However, a perfect (especially in plankton ecology) classifier does not exist. Therefore, quantification is a tool to better estimate the abundance of planktonic groups while perfectly knowing that the classifier is biased. This quantification method was successively applied in the paper of @Sosik2007-cj on a 2 month data set although it is very likely that P needs to be adapted to the sampling conditions. This reevaluation of $\pmb{P}$ is due to what is defined as a data set shift where part of the unseen data follows a different distribution than the one from the training data [@Orenstein2020-jl; @Moreno-Torres2012-oq]). It is especially true for ER systems deployed on AUVs where planktonic communities change throughout the year. Thus, we tested different quantification methods (CC, adjusted count (AC), threshold count (TC), probabilistic adjusted count (PAC) and scaled probability adjusted count (SPAC), see @Gonzalez2017-tr for a review on those quantification methods) assuming that the estimation of $\pmb{P}$ was accurate enough (\> 3M images in our training set) and that the global coverage of UVP5 profiles in the training set was sufficient to cover various planktonic distributions from the test set.

We compared the abundance estimations of all quantification methods on the entire test set, all profiles included (\> 300k images). The SPAC showed the best estimations whereas TC drastically underestimated them (85% of data were lost using a probability threshold of 0.9). However, the aim of the ER is to be deployed on AUVs sampling vertical profiles in the water column therefore we applied the SPAC on 10.000 profile-like subsets and we summed all taxonomic abundances. In that configuration, the SPAC overestimated the real total abundance by 1 to 15% depending on the projected (high or low) number of objects in each profile. These percentages were literally exploding for small classes in both projections (also with PAC) therefore quantification was discarded for use on autonomous profilers at this stage.

Increasing the number of detritus classes was also considered in the hope of clustering them in the features space. Detritus display a variety of size, shape and grey levels so it must be possible to cluster them and therefore improve the classification score. For this, we applied a classic K-Means algorithm [@Hartigan1979-oz] on detritus cleaned from likely outliers using an Isolation Forest Algorithm [@Liu2012-xz] and we executed a grid search as described earlier. @fig-grid-search clearly shows that using more than 1 class of detritus drastically decreases the global accuracy of the model, decreases the weighted living recall and only increases by 1% the weighted living precision. In addition, the confusion matrix (@fig-confusion-matrix-uvp6-ndetritus) shows that it does not improve the ER model because there is a high confusion between subgroups of detritus. Essentially, it seems that detritus form a continuum in the features space and that more research should be done on the choice of features, clustering algorithms and related methods to better differentiate them in separate entities, which is not the purpose of this study.

<!-- ![Confusion matrix (normalized on true labels) of the a UVP5 XGBoost model with 5 classes of detritus.](image/uvp6/Muvpec_367314a7_confusion_matrix.jpeg){#fig-confusion-matrix-uvp6-ndetritus} -->
```{r, confusion matrix N detritus}
#| label: fig-confusion-matrix-uvp6-ndetritus
#| fig-cap: Confusion matrix (normalized on true labels) of a UVP5 XGBoost model with 5 classes of detritus.

x <- c('Acantharia', 'Aulacanthidae', 'Aulosphaeridae', 'Copepoda', 'Trichodesmium', 
            'artefact', 'bubble', 'darksphere', 'detritus_0', 'detritus_1', 'detritus_2', 'detritus_3', 'detritus_4', 'fiber', 'other<living')

y <- factor(x, levels = rev(x))

data <- expand.grid(X=x, Y=y, stringsAsFactors = T)

gray_levels_ndetritus <- c(64,252,249,255,250,255,255,255,255,240,253,247,254,249,251,
                           253,55,247,255,247,246,255,255,250,253,255,252,251,255,249,
                           254,252,0,255,255,255,255,255,250,250,255,250,254,255,250,
                           255,255,255,137,254,254,255,255,247,231,253,229,230,255,251,
                           254,255,255,255,33,255,255,253,254,251,255,238,246,249,255,
                           255,255,255,255,255,51,255,255,252,245,253,219,255,255,255,
                           255,255,255,255,253,250,11,255,253,251,255,247,245,255,255,
                           254,252,253,255,250,254,254,76,253,254,255,252,214,255,251,
                           255,255,255,255,255,181,255,255,253,192,247,239,255,226,251,
                           255,255,255,255,254,193,255,255,248,189,249,212,255,246,251,
                           255,255,255,254,255,212,255,255,253,194,221,242,255,229,251,
                           255,255,255,255,255,181,255,255,250,161,251,229,255,247,252,
                           255,255,255,253,253,249,255,255,246,239,255,173,159,252,255,
                           255,255,255,254,250,253,255,255,253,229,249,213,243,138,255,
                           251,252,253,246,252,242,254,253,247,246,253,247,249,245,134)

gray_levels_ndetritus <- abs(gray_levels_ndetritus - 255)/255

data$gray_xgboost_ndetritus <- gray_levels_ndetritus

ggplot(data, aes(X, Y, fill= gray_xgboost_ndetritus)) + geom_tile() + scale_fill_viridis(option = "F", direction = -1) +
  labs(x = "Predicted labels", y = "True labels", fill = '') + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
#### Application to the UVP6 data set

The aim of building an ER model was intended towards its implementation on the UVP6 for future deployments on BGC-Argo floats (or other profiling AUVs). Following the work done with the UVP5 data set, we applied the same method for UVP6 data. Thus, we started with the data aggregation step (choices of classes) where, in contrast with UVP5 data, some classes were kept (see @tbl-classification-report-uvp6-xgboost-cnn) because of their ecological role (e.g. small\<*Cnidaria* or tuff, an elongated cyanobacteria belonging to *Trichodesmium*) even though their classification metrics (precision and recall) were not particularly convincing. In the end, we reached an agreement on 20 classes. Among them, there are 16 biological classes (*Acantharia*, *Actinopterygii*, *Appendicularia*, *Aulacanthidae*, *Calanoida*, *Chaetognatha*, *Collodaria*, *Creseis*, *Foraminifera*, *Rhizaria*, *Salpida*, other\<living, tuff and puff \[another *Trichodesmium*\], small\<*Cnidaria*, solitaryglobule \[part of the *Rhizaria* group\] and 4 non-living classes (detritus, fiber, artefact, crystal). One will notice that we have the *Rhizaria* class as well as *Acantharia*, *Collodaria* and solitaryglobule. Those children of *Rhizaria* on the phylogenetic tree showed satisfying classification metrics compared to other *Rhizaria* subgroups (e.g. *Aulosphaeridae*, *Phaeodaria*, *Coelodendridae*, etc) hence we decided to separate them from their parent. Therefore, we assume that those classes are well recognized by human classifiers so that their parent *Rhizaria* is virtually free from them. Afterwards, we executed a grid search to find the optimal couple $\eta$ and $D_{max}$ ($\omega$ and the number of classes of detritus were fixed based on UVP5 results) and trained the final ER model whose CM and CR are reported in @fig-confusion-matrix-final-model and @tbl-classification-report-uvp6-xgboost-cnn.

```{r, confusion matrix N detritus}
#| label: fig-confusion-matrix-final-model
#| fig-cap: Confusion matrix (normalized on true labels) of the final UVP6 model trained with XGBoost.

library(RcppCNPy)
gray_levels_uvp6 <- array(t(npyLoad('DATA/UVP6/cm_uvp6.npy')))

x <- c('Acantharia', 'Actinopterygii', 'Appendicularia', 'Aulacanthidae',
       'Calanoida', 'Chaetognatha', 'Collodaria', 'Creseis',
       'Foraminifera', 'Rhizaria', 'Salpida', 'artefact', 'crystal',
       'detritus', 'fiber', 'other<living', 'puff', 'small<Cnidaria',
       'solitaryglobule', 'tuff')

y <- factor(x, levels = rev(x))
data <- expand.grid(X=x, Y=y, stringsAsFactors = T)
data$gray_levels <- gray_levels_uvp6

ggplot(data, aes(X, Y, fill= gray_levels)) + geom_tile() + scale_fill_viridis(option = "F", direction = -1) +
  labs(x = "Predicted labels", y = "True labels", fill = '') + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The specifications of the final UVP6 ER model are the following: $\eta$: 0.2, $D_{max}$: 3, $\omega$: 0.25, number of boosting rounds: 400 for a total loading time of 73 ms. Note that the CV initially found an optimal number of boosting rounds of 555 for a loading time of 101 ms). However, we observed that the log loss curve had already reached a plateau hence we were able to decrease the size of the model without losing much (\< 1%) in biological precision or recall, decreasing a bit more the energy consumption of the UVP6 on board AUVs.

```{r, classification report XGBoost and CNN uvp6}
#| label: tbl-classification-report-uvp6-xgboost-cnn
#| tbl-cap: Precision, recall and f1-score for each class and globally (lightgray background) of the best UVP5 model trained with XGBoost and a CNN. The abbreviation avg stands for average.

labels <- c('Acantharia', 'Actinopterygii', 'Appendicularia', 'Aulacanthidae', 'Calanoida', 'Chaetognatha', 'Collodaria', 'Creseis', 'Foraminifera', 'Rhizaria', 'Salpida', 'artefact', 'crystal', 'detritus', 'fiber','other<living', 'puff', 'small<Cnidaria', 'solitaryglobule', 'tuff', 'accuracy','macro avg', 'weighted avg', 'living macro avg', 'living weighted avg')

precision <- c(0.89,1,0.89,0.82,0.66,0.80,.73,0.81,0.71,0.69,0.63,0.91,0.89,0.95,0.73,0.44,0.73,0.55,0.83,0.55,0.91,0.76,0.91,0.73,0.61)
recall <- c(0.69,0.40,0.44,0.89,0.69,0.62,0.62,1,0.48,0.75,0.61,0.92,0.89,0.95,0.83,0.24,0.78,0.47,0.71,0.63,0.91,0.68,0.91,0.63,0.56)
f1_score <- c(0.78,0.57,0.59,0.85,0.68,0.70,0.67,0.89,0.57,0.72,0.62,0.91,0.89,0.95,0.78,0.31,0.75,0.51,0.77,0.59,0.91,0.71,0.91,0.66,0.57)

precision_cnn <- c(0.98,0.47,0.38,0.86,0.82,0.52,0.74,0.42,0.76,0.71,0.74,0.94,0.69,0.98,0.77,0.55,0.84,0.72,0.81,0.68,0.94,0.72,0.94,0.69,0.71)
recall_cnn <- c(1,0.73,0.64,0.86,0.89,0.88,0.88,1,0.89,0.92,0.89,0.92,0.97,0.96,0.85,0.62,0.94,0.86,0.88,0.93,0.94,0.88,0.94,0.86,0.82)
f1_score_cnn <- c(0.99,0.57,0.47,0.86,0.86,0.65,0.81,0.59,0.82,0.80,0.81,0.93,0.81,0.97,0.80,0.59,0.89,0.79,0.84,0.79,0.94,0.78,0.94,0.76,0.76)

cf_uvp6_xgboost_cnn <- tibble(labels, precision, recall, f1_score, precision_cnn, recall_cnn, f1_score_cnn)
colnames(cf_uvp6_xgboost_cnn) <- c('Labels', 'Precision', 'Recall', 'F1-score', 'Precision', 'Recall', 'F1-score')

kable(cf_uvp6_xgboost_cnn) |> row_spec(0, bold=T, align = 'c') |> #row_spec(1:16, align = 'c') |>
  row_spec(row = 21:25, background = '#F2F2F2') |> add_header_above(c(" " = 1, "XGBoost" = 3, "CNN" = 3))
```

#### How to use the CR in real applications?

4 BGC-Argo floats equipped with the ER were deployed in late May 2022 in the southern part of the Labrador Sea at the time of the spring bloom. As it can be seen in @fig-bgc-application-uvp6, *Calanoida* are more abundant around the deep chlorophyll maximum (DCM) and they would be observed in very low concentrations even below 1000 m. The CR for *Calanoida* shows that the model has a precision of 66% and a recall of 69% for that class. It implies that caution must be applied when interpreting the presence or absence of such copepods. In that case, we theoretically miss 31% of them (i.e. 31% of *Calanoida* are not detected as such, they are classified in another class) and out of the 69% that we detect as *Calanoida*, we are wrong 34% of the time. As a result, even though *Calanoida* have been shown to overwinter in deep waters (\> 1000 m) [@Krumhansl2018-dr], an isolated signal at depth does not mean a copepod was really there. Similarly, *Trichodesmium* should not be observed outside the tropical and subtropical regions @Westberry2006-bt therefore depending on the sampling region, some results can directly be rejected. Here, tuff are predicted in higher concentrations than puff which are only predicted at one depth. Again, if one looks at the CR for these 2 classes, one can see that the precision and recall for tuff are, respectively, 18% and 15% below those of puff. Finally, the interpretation of the detritus class is more straightforward because they have excellent metrics in the CR. For instance, it can be seen that detritus nicely follow the DCM curve. In contrast with biological classes where we are looking for patterns of presence or absence, we know that detritus are ubiquitous but their successive abundances could be interesting for flux studies.

```{r, uvp6 BGC application}
#| label: fig-bgc-application-uvp6
#| fig-cap : Concentrations of 4 classes (3 biological and detritus) estimated with data acquired from the UVP6 on board the BGC-Argo float (WMO 4903634) on the 15$^{th}$ of June 2022 in the Labrador Sea at latitude 59.27° N and longitude -50.56° E. Red points represent positive concentrations. Dotted lines represent the depth of the deep chlorophyll maximum.

library(ggplot2)
library(tidyverse)
library(gridExtra)
library(cowplot)
library(patchwork)

# extract bgc-argo data
#data <- read.csv('bgc_data/lovuse017c_010_01.csv', sep = '\t')
load("DATA/UVP6/lovuse017c_015_01.RData")

# for the discussion of the uvp paper
data <- dataprofile$data$uvp6_txo
ecodata <- dataprofile$data$eco

# plot chlorophyll for DCM
ecodata <- ecodata %>% select(pres = Pressure_dbar, chla = `chlorophyll-a_ug/l`)
tmp <- ecodata %>% group_by(pres) %>% summarize(n_sample = n(), total_conc = sum(chla), mean_conc = total_conc/n_sample) %>% filter(mean_conc > 0)
chla <- ggplot(tmp, aes(x = pres, y = mean_conc)) + geom_point() + scale_x_reverse() + coord_flip() +
  xlab('Pressure (dbar)') + ylab(expression('Chlorophyll a (mu/L)')) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) 
DCM <- tmp$pres[which.max(tmp$mean_conc)]

# object number 5 (Calanoida)
test <- data %>% select(Pressure_dbar, Nimages, ObjectNbr5)
test$ObjectConc5 <- test$ObjectNbr5/(test$Nimages*0.6)
test <- test %>% group_by(Pressure_dbar) %>% summarize(n_sample = n(), total_conc = sum(ObjectConc5), mean_conc = total_conc/n_sample)
cal <- ggplot(test, aes(x = Pressure_dbar, y = mean_conc)) + geom_point() + scale_x_reverse() + coord_flip()+
  xlab('Pressure (dbar)') + ylab('Calanoida (#/L)') + geom_point(data = test[test$mean_conc>0,], 
                                                                  aes(x = Pressure_dbar, y = mean_conc), colour = "red") +
  geom_vline(xintercept = DCM, lty = 'dotted') + theme_bw()

# object number 14 (detritus)
test <- data %>% select(Pressure_dbar, Nimages, ObjectNbr14)
test$ObjectConc14 <- test$ObjectNbr14/(test$Nimages*0.6)
test <- test %>% group_by(Pressure_dbar) %>% summarize(n_sample = n(), total_conc = sum(ObjectConc14), mean_conc = total_conc/n_sample)
det <- ggplot(test, aes(x = Pressure_dbar, y = mean_conc)) + geom_point() + scale_x_reverse() + coord_flip() +
  geom_point(data = test[test$mean_conc>0,], 
             aes(x = Pressure_dbar, y = mean_conc), colour = "red") +
  xlab('Pressure (dbar)') + ylab('detritus (#/L)') + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
  geom_vline(xintercept = DCM, lty = 'dotted') + theme_bw()

test <- data %>% select(Pressure_dbar, Nimages, ObjectNbr17)
test$ObjectConc17 <- test$ObjectNbr17/(test$Nimages*0.6)
test <- test %>% group_by(Pressure_dbar) %>% summarize(n_sample = n(), total_conc = sum(ObjectConc17), mean_conc = total_conc/n_sample)
puf <- ggplot(test, aes(x = Pressure_dbar, y = mean_conc)) + geom_point() + scale_x_reverse() + coord_flip()+
  xlab('Pressure (dbar)') + ylab('puff (#/L)') + geom_point(data = test[test$mean_conc>0,], 
                                                                          aes(x = Pressure_dbar, y = mean_conc), colour = "red") +
  geom_vline(xintercept = DCM, lty = 'dotted') + theme_bw()

test <- data %>% select(Pressure_dbar, Nimages, ObjectNbr20)
test$ObjectConc20 <- test$ObjectNbr20/(test$Nimages*0.6)
test <- test %>% group_by(Pressure_dbar) %>% summarize(n_sample = n(), total_conc = sum(ObjectConc20), mean_conc = total_conc/n_sample)
tuf <- ggplot(test, aes(x = Pressure_dbar, y = mean_conc)) + geom_point() + scale_x_reverse() + coord_flip()+
  xlab('Pressure (dbar)') + ylab('tuff (#/L)') + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
  geom_point(data = test[test$mean_conc>0,], 
             aes(x = Pressure_dbar, y = mean_conc), colour = "red") +
  geom_vline(xintercept = DCM, lty = 'dotted') + theme_bw()

#grid.arrange(cal, det, puf, tuf)
plot_grid(cal, det, puf, tuf, labels = "", nrow = 2, ncol = 2)



```

#### Prospects

The ER of zooplankton on board AUVs is just at its beginning. As expected, the CNN trained on UVP6 data (see @tbl-classification-report-uvp6-xgboost-cnn) outperforms the results of our features-based classifier. However, it should be noted that not all biological classes are better classified (e.g. *Actinopeterygii*, *Appendicularia*, *Chaetognatha*, *Creseis*). One can also observe that the difference between XGBoost and the CNN trained on UVP5 data on biological classes is less significant than for UVP6 data. This is likely explained by the fact that we were able to keep more (small) classes for the UVP6. Nevertheless, previous tests on UVP5 data with more classes increased the gap between XGBoost and CNN weighted results therefore the latter recognizes better small classes. If energy and hardware constraints limit the performances of our current ER model, it is not forbidden to think about light-weight CNNs for future applications on AUVs to better understand the dynamics and the behavior of zooplankton.

### Conclusion

The objective of this paper was to develop a zooplankton classification model under strict energy constraints for the UVP6 to be deployed on BGC-Argo floats. It was clear at the beginning that the use of powerful energy consuming CNNs had to be discarded and that we needed to build a satisfactory features-based model instead. To this aim, we firstly used XGBoost on UVP5 data, a more spatially and temporally representative data set than the recent UVP6 data set to create the data processing pipeline (classes aggregation, hyperparameter tuning, model training) and investigate ways to improve classification scores (quantification, detritus subgroups, label weights). In order to see how the model could be improved, we compared the XGBoost model with results from a CNN not limited by energy. Afterwards, we applied the data processing pipeline and the recommendations from the UVP5 data set analysis on the UVP6 data set to create the final ER model. This model contains 20 labels, 16 biological and 4 non-living classes, with a total loading time of 73 ms. The initial upper limit for the loading time was 700 ms therefore we are saving precious energy to extend the lifetime of BGC-Argo floats. Finally, we showed the CM and CR of the final model and how to use it to interpret some UVP6 taxonomic profiles observed by the float WMO 4903634 during the spring bloom in the southern part of the Labrador Sea.
